<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>hnakamur&#39;s blog at github</title>
    <link>https://hnakamur.github.io/blog/index.xml</link>
    <description>Recent content on hnakamur&#39;s blog at github</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja-JP</language>
    <lastBuildDate>Sun, 01 Jan 2017 09:40:35 +0900</lastBuildDate>
    <atom:link href="https://hnakamur.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>minikubeとVirtualBoxでNFSのpersistent volumeを試してみた</title>
      <link>https://hnakamur.github.io/blog/2017/01/01/use-nfs-persistent-volume-on-minikube-virtualbox/</link>
      <pubDate>Sun, 01 Jan 2017 09:40:35 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2017/01/01/use-nfs-persistent-volume-on-minikube-virtualbox/</guid>
      <description>

&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://kubernetes.io/docs/tutorials/&#34;&gt;Tutorials - Kubernetes&lt;/a&gt;のStateful Applicationsを試そうと思って少し読んだ所、 persistent volume というものを用意する必要があることがわかりました。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://kubernetes.io/docs/user-guide/persistent-volumes/#types-of-persistent-volumes&#34;&gt;Types of Persistent Volumes&lt;/a&gt; を見るとさまざまなタイプの persistent volume がありますが、Mac上での開発環境としてkubernetesを使うならNFSが手軽そうなので、これを試してみることにしました。&lt;/p&gt;

&lt;p&gt;このページを見てもよくわからなかったので、検索して見つけた以下の情報を参考にして試行錯誤して、とりあえず動くようになったのでメモです。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/minikube/issues/2#issuecomment-233629375&#34;&gt;Support mounting host directories into pods · Issue #2 · kubernetes/minikube&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/TheNewNormal/kube-solo-osx/blob/master/examples/pv/nfs-pv-mount-on-pod.md&#34;&gt;kube-solo-osx/nfs-pv-mount-on-pod.md at master · TheNewNormal/kube-solo-osx&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/master/examples/volumes/nfs&#34;&gt;kubernetes/examples/volumes/nfs at master · kubernetes/kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;minikubeからmacのディスクをnfsマウントする&#34;&gt;minikubeからmacのディスクをNFSマウントする&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/minikube/issues/2#issuecomment-233629375&#34;&gt;Support mounting host directories into pods · Issue #2 · kubernetes/minikube&lt;/a&gt;のコメントに従って以下のコマンドをmacで実行しました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo &amp;quot;/Users -network 192.168.99.0 -mask 255.255.255.0 -alldirs -maproot=root:wheel&amp;quot; | sudo tee -a /etc/exports
sudo nfsd restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;IPアドレスは &lt;code&gt;minikube ip&lt;/code&gt; の結果に合わせて調整します。私の環境では 192.168.99.100 だったので、それにあわせて &lt;code&gt;-network&lt;/code&gt; は 192.168.99.0、 &lt;code&gt;-mask&lt;/code&gt; は 255.255.255.0 としています。&lt;/p&gt;

&lt;p&gt;以下の手順で、手動で一度マウントしてみました。 &lt;code&gt;minikube start&lt;/code&gt; は既に起動済みなら不要です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;minikube start
minikube ssh -- sudo umount /Users
minikube ssh -- sudo /usr/local/etc/init.d/nfs-client start
minikube ssh -- sudo mount 192.168.99.1:/Users /Users -o rw,async,noatime,rsize=32768,wsize=32768,proto=tcp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;IPアドレスは &lt;code&gt;minikube ip&lt;/code&gt; の結果に合わせて調整します。私の環境では 192.168.99.100 だったので、minikubeからmacへは 192.168.99.1 で参照できるということでmountの引数にはこのアドレスを指定しています。&lt;/p&gt;

&lt;p&gt;マウントポイントの /Users は適宜変更変更します。&lt;/p&gt;

&lt;p&gt;無事マウントできたら &lt;code&gt;minikube ssh&lt;/code&gt; でssh接続して &lt;code&gt;df -h&lt;/code&gt; などでマウントされたことを確認し、minikube内からとmac側からファイルを作ったり削除して相互に見えることを確認しました。&lt;/p&gt;

&lt;p&gt;一通り確認したらminikube内から &lt;code&gt;sudo umount /Users&lt;/code&gt; でアンマウントしておきます。&lt;/p&gt;

&lt;h2 id=&#34;podからnfsのpersistent-volumeを使ってみる&#34;&gt;PodからNFSのpersistent volumeを使ってみる&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/TheNewNormal/kube-solo-osx/blob/master/examples/pv/nfs-pv-mount-on-pod.md&#34;&gt;kube-solo-osx/nfs-pv-mount-on-pod.md at master · TheNewNormal/kube-solo-osx&lt;/a&gt;と&lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/master/examples/volumes/nfs&#34;&gt;kubernetes/examples/volumes/nfs at master · kubernetes/kubernetes&lt;/a&gt;を参考にして試行錯誤しました。&lt;/p&gt;

&lt;p&gt;後者の &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/f5d9c430e9168cf5c41197b8a4e457981cb031df/examples/volumes/nfs/nfs-web-rc.yaml&#34;&gt;nfs-web-rc.yaml&lt;/a&gt;では ReplicationController というものを作っているのですが、&lt;a href=&#34;http://stackoverflow.com/questions/37423117/replication-controller-vs-deployment-in-kubernetes/37423281#37423281&#34;&gt;google compute engine - Replication Controller VS Deployment in Kubernetes - Stack Overflow&lt;/a&gt;というコメントによると、ReplicatioControllerはDeploymentsにとって変わられるものだそうです。ただし、 &lt;a href=&#34;http://stackoverflow.com/questions/37423117/replication-controller-vs-deployment-in-kubernetes/37423217#37423217&#34;&gt;google compute engine - Replication Controller VS Deployment in Kubernetes - Stack Overflow&lt;/a&gt;によるとDeploymentはまだベータです。&lt;/p&gt;

&lt;h2 id=&#34;サービス公開用の設定ファイル&#34;&gt;サービス公開用の設定ファイル&lt;/h2&gt;

&lt;p&gt;試行錯誤した結果の設定ファイルは以下の通りです。&lt;/p&gt;

&lt;p&gt;persistent-volume-nfs.ymlのspec.nfs.pathに対応するディレクトリはmacで &lt;code&gt;mkdir -p /Users/hnakamur/kube-data&lt;/code&gt; で作成しておきます。spec.nfs.serverはminikubeから見たmacのIPアドレスを指定します。&lt;/p&gt;

&lt;p&gt;spec.persistentVolumeReclaimPolicyは&lt;a href=&#34;https://github.com/TheNewNormal/kube-solo-osx/blob/252b46b4837efc41e7c85c7c3171518e23520866/examples/pv/nfs-pv-mount-on-pod.md&#34;&gt;kube-solo-osx/nfs-pv-mount-on-pod.md at 252b46b4837efc41e7c85c7c3171518e23520866 · TheNewNormal/kube-solo-osx&lt;/a&gt;ではRetainedとなっていたのですが、動かしてみるとエラーメッセージが出たのでそこに書いてあった選択肢の1つのRetainに変えました。&lt;/p&gt;

&lt;p&gt;persistemt volumeとpersistent volume claimについては&lt;a href=&#34;http://kubernetes.io/docs/user-guide/persistent-volumes/&#34;&gt;Persistent Volumes - Kubernetes&lt;/a&gt;に説明があります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat persistent-volume-nfs.yml
kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv-nfs
  labels:
    type: nfs
spec:
  capacity:
    storage: 30Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  nfs:
    # TODO: modify path and server appropriately
    path: /Users/hnakamur/kube-data
    server: 192.168.99.1
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;$ cat persistent-volume-claim.yml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 15Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Deploymentsについては&lt;a href=&#34;http://kubernetes.io/docs/user-guide/deployments/&#34;&gt;Deployments - Kubernetes&lt;/a&gt;を参照してください。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat nginx-deployment.yml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:1.11.8
          ports:
            - containerPort: 80
          volumeMounts:
            - mountPath: &amp;quot;/usr/share/nginx/html&amp;quot;
              name: nginx-data
      volumes:
        - name: nginx-data
          persistentVolumeClaim:
            claimName: my-pvc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Servicesについては&lt;a href=&#34;http://kubernetes.io/docs/user-guide/services/#type-nodeport&#34;&gt;Services - Kubernetes&lt;/a&gt;を参照してください。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat nginx-service.yml
kind: Service
apiVersion: v1
metadata:
  name: nginx
spec:
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: NodePort
  selector:
    app: nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;サービス作成と公開&#34;&gt;サービス作成と公開&lt;/h2&gt;

&lt;p&gt;上記の設定ファイルを用意しておけば、サービス作成と公開は以下のように実行するだけです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f persistent-volume-nfs.yml
kubectl create -f persistent-volume-claim.yml
kubectl create -f nginx-deployment.yml
kubectl create -f nginx-service.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;mac上で以下のコマンドでnginxで表示するHTMLファイルを作成します。HTMLファイルと言いつつ手抜きで単なるテキストです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo &#39;Hello Kubernetes NFS volume!&#39; &amp;gt; ~/kube-data/index.html
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;macからcurlでサービスのnginxにアクセスしてみる&#34;&gt;macからcurlでサービスのnginxにアクセスしてみる&lt;/h2&gt;

&lt;p&gt;ノードのIPとポートを取得して環境変数に設定。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export NODE_IP=$(minikube ip)
$ echo NODE_IP=$NODE_IP
NODE_IP=192.168.99.100
$ export NODE_PORT=$(kubectl get services/nginx -o go-template=&#39;{{(index .spec.ports 0).nodePort}}&#39;)
$ echo NODE_PORT=$NODE_PORT
NODE_PORT=32252
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;curlでアクセスすると、上記で作成したファイルの内容が表示されることを確認できました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes NFS volume!
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;サービス公開停止と削除&#34;&gt;サービス公開停止と削除&lt;/h2&gt;

&lt;p&gt;作成時とは逆の順番に &lt;code&gt;kubectl delete -f&lt;/code&gt; で設定ファイルを指定して削除すればOKでした。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl delete -f nginx-service.yml
kubectl delete -f nginx-deployment.yml
kubectl delete -f persistent-volume-claim.yml
kubectl delete -f persistent-volume-nfs.yml
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>minikubeでKubernetesのチュートリアルをやってみた</title>
      <link>https://hnakamur.github.io/blog/2016/12/31/tried-kubernetes-tutorial-with-minikube/</link>
      <pubDate>Sat, 31 Dec 2016 16:24:33 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2016/12/31/tried-kubernetes-tutorial-with-minikube/</guid>
      <description>

&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;

&lt;p&gt;検索してたら &lt;a href=&#34;https://news.ycombinator.com/item?id=12462261&#34;&gt;Why Kubernetes is winning the container war | Hacker News&lt;/a&gt; というHacker Newsのスレッドを見つけました。&lt;/p&gt;

&lt;p&gt;実際に勝つどうかはともかく、実際に使っている人やMesosphereやRed Hatの人のコメントがあり、非常に参考になりそうです。このブログ記事を書くまで私は Kubernetes はろくに触ったことが無かったので内容はよくわからないですが、後日また見直してみたいところです。&lt;/p&gt;

&lt;p&gt;上記のHacker Newsのコメントで以下の2つのチュートリアルが紹介されていました。このブログ記事はこのうち1つめのほうを試してみたメモです。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetesbootcamp.github.io/kubernetes-bootcamp/&#34;&gt;Kubernetes Bootcamp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kelseyhightower/kubernetes-the-hard-way&#34;&gt;kelseyhightower/kubernetes-the-hard-way: Bootstrap Kubernetes the hard way on Google Cloud Platform or Amazon EC2. No scripts.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;試してから気づいたのですが、全く同じ内容が Kubernetes の公式ドキュメントの &lt;a href=&#34;http://kubernetes.io/docs/tutorials/kubernetes-basics/&#34;&gt;Kubernetes Basics&lt;/a&gt; にありました。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://kubernetes.io/docs/tutorials/kubernetes-basics/&#34;&gt;Kubernetes Basics&lt;/a&gt; はいくつかの章（このチュートリアルでは Module と呼ばれています）に分かれていて、まず図解付きのわかりやすい概念説明があり、その後ブラウザ上のターミナルでコマンドを入力すると結果が表示されるというインタラクティブなチュートリアルになっています。&lt;/p&gt;

&lt;p&gt;各章末にクイズがあり、概念を理解したか確認できるのも良い感じです。&lt;/p&gt;

&lt;p&gt;ターミナルの左に説明文があり、入力する各コマンドをマウスでクリックすると、右側のターミナルに入力してくれるので手軽に試せます。&lt;/p&gt;

&lt;p&gt;とはいえ、手元の環境でも試してみたかったので、macOS上に環境構築してブラウザのインタラクティブチュートリアルとともに試してみました。&lt;/p&gt;

&lt;h2 id=&#34;macos-sierraでの事前準備&#34;&gt;macOS Sierraでの事前準備&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;minikube start --help&lt;/code&gt; の &lt;code&gt;--vm-driver&lt;/code&gt; の説明によると仮想マシンドライバは virtualbox xhyve vmwarefusion のいずれかでデフォルトは virtualbox です。
ということでVirtualBoxをインストールしておきます。私の環境ではバージョンは 5.1.12 でした。&lt;/p&gt;

&lt;p&gt;minikubeとKubernetesはGitHubのプロジェクトにリリースページがあってそこからバイナリをダウンロードできます。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/minikube/releases/tag/v0.14.0&#34;&gt;Release v0.14.0 · kubernetes/minikube&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/kubernetes/releases/tag/v1.5.1&#34;&gt;Release v1.5.1 · kubernetes/kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;が、Homebrewでパッケージが用意されていてバージョンも上記と同じで最新だったのでHomebrewでインストールしました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;brew install Caskroom/cask/minikube
brew install kubectl
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;module-1-kubernetsクラスタを作成する&#34;&gt;Module 1: Kubernetsクラスタを作成する&lt;/h2&gt;

&lt;p&gt;概念の説明は &lt;a href=&#34;https://kubernetesbootcamp.github.io/kubernetes-bootcamp/1-1.html&#34;&gt;Introduction to Kubernetes cluster&lt;/a&gt; を参照してください。&lt;/p&gt;

&lt;h3 id=&#34;minikubeのバージョン確認&#34;&gt;minikubeのバージョン確認&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ minikube version       
minikube version: v0.14.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;minikube起動&#34;&gt;minikube起動&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ minikube start
Starting local Kubernetes cluster...
Kubectl is now configured to use the cluster.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;kubectlのバージョン確認&#34;&gt;kubectlのバージョン確認&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl version
Client Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;5&amp;quot;, GitVersion:&amp;quot;v1.5.1&amp;quot;, GitCommit:&amp;quot;82450d03cb057bab0950214ef122b67c83fb11df&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;2016-12-22T13:56:59Z&amp;quot;, GoVersion:&amp;quot;go1.7.4&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;darwin/amd64&amp;quot;}
Server Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;5&amp;quot;, GitVersion:&amp;quot;v1.5.1&amp;quot;, GitCommit:&amp;quot;82450d03cb057bab0950214ef122b67c83fb11df&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;1970-01-01T00:00:00Z&amp;quot;, GoVersion:&amp;quot;go1.7.1&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;linux/amd64&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;クラスタの情報表示&#34;&gt;クラスタの情報表示&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl cluster-info
Kubernetes master is running at https://192.168.99.100:8443
kubernetes-dashboard is running at https://192.168.99.100:8443/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard

To further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;ノード一覧&#34;&gt;ノード一覧&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get nodes
NAME       STATUS    AGE
minikube   Ready     11h
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;module-2-アプリをデプロイ&#34;&gt;Module 2: アプリをデプロイ&lt;/h2&gt;

&lt;p&gt;概念の説明は &lt;a href=&#34;https://kubernetesbootcamp.github.io/kubernetes-bootcamp/2-1.html&#34;&gt;Your first application deployment&lt;/a&gt; を参照してください。&lt;/p&gt;

&lt;h3 id=&#34;kubernetes-bootcampアプリをデプロイ&#34;&gt;kubernetes-bootcampアプリをデプロイ&lt;/h3&gt;

&lt;p&gt;チュートリアルのために用意されたkubernetes-bootcampアプリをデプロイしてみます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl run kubernetes-bootcamp --image=docker.io/jocatalin/kubernetes-bootcamp:v1 --port=8080
deployment &amp;quot;kubernetes-bootcamp&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;デプロイされたアプリ一覧&#34;&gt;デプロイされたアプリ一覧&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get deployments
NAME                  DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kubernetes-bootcamp   1         1         1            1           57s
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;プロキシ起動&#34;&gt;プロキシ起動&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl proxy
Starting to serve on 127.0.0.1:8001
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;プロキシを起動したらプロンプトには戻ってこないので、以降のコマンドは別のターミナルで実行します。&lt;/p&gt;

&lt;h3 id=&#34;podの名前を取得&#34;&gt;Podの名前を取得&lt;/h3&gt;

&lt;p&gt;この後参照するため、Podの名前を取得して環境変数POD_NAMEに設定しておきます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export POD_NAME=$(kubectl get pods -o go-template --template &#39;{{range .items}}{{.metadata.name}}{{&amp;quot;\n&amp;quot;}}{{end}}&#39;)
$ echo Name of the Pod: $POD_NAME
Name of the Pod: kubernetes-bootcamp-390780338-6j8fn
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;見た感じ &lt;code&gt;--template&lt;/code&gt; の引数の書式はGo言語の &lt;a href=&#34;https://golang.org/pkg/text/template/&#34;&gt;text/template&lt;/a&gt;パッケージのテンプレート言語をそのまま使っているようです。&lt;/p&gt;

&lt;p&gt;Pod名の &lt;code&gt;390780338-6j8fn&lt;/code&gt; の部分はデプロイの度に生成されるランダムな文字列となっています。&lt;/p&gt;

&lt;h3 id=&#34;プロキシ経由でアプリにアクセス&#34;&gt;プロキシ経由でアプリにアクセス&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ curl http://localhost:8001/api/v1/proxy/namespaces/default/pods/$POD_NAME/
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-6j8fn | v=1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;module-3-デプロイしたアプリを詳しく見てみる&#34;&gt;Module 3: デプロイしたアプリを詳しく見てみる&lt;/h2&gt;

&lt;p&gt;概念の説明は &lt;a href=&#34;https://kubernetesbootcamp.github.io/kubernetes-bootcamp/3-1.html&#34;&gt;Pods and Nodes&lt;/a&gt; を参照してください。&lt;/p&gt;

&lt;h3 id=&#34;pod一覧表示&#34;&gt;Pod一覧表示&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods
NAME                                  READY     STATUS    RESTARTS   AGE
kubernetes-bootcamp-390780338-6j8fn   1/1       Running   0          13m
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;pod詳細表示&#34;&gt;Pod詳細表示&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe pods
Name:           kubernetes-bootcamp-390780338-6j8fn
Namespace:      default
Node:           minikube/192.168.99.100
Start Time:     Sat, 31 Dec 2016 17:15:41 +0900
Labels:         pod-template-hash=390780338
                run=kubernetes-bootcamp
Status:         Running
IP:             172.17.0.4
Controllers:    ReplicaSet/kubernetes-bootcamp-390780338
Containers:
  kubernetes-bootcamp:
    Container ID:       docker://f3d04d91e8f27b2b537c20d82253376993483f9bb9c0d1196ba50ecc3a69ff7c
    Image:              docker.io/jocatalin/kubernetes-bootcamp:v1
    Image ID:           docker://sha256:8fafd8af70e9aa7c3ab40222ca4fd58050cf3e49cb14a4e7c0f460cd4f78e9fe
    Port:               8080/TCP
    State:              Running
      Started:          Sat, 31 Dec 2016 17:15:42 +0900
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qqsb7 (ro)
    Environment Variables:      &amp;lt;none&amp;gt;
Conditions:
  Type          Status
  Initialized   True 
  Ready         True 
  PodScheduled  True 
Volumes:
  default-token-qqsb7:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-qqsb7
QoS Class:      BestEffort
Tolerations:    &amp;lt;none&amp;gt;
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath                           Type            Reason          Message
  ---------     --------        -----   ----                    -------------                           --------        ------          -------
  15m           15m             1       {default-scheduler }                                            Normal          Scheduled       Successfully assigned kubernetes-bootcamp-390780338-6j8fn to minikube
  15m           15m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Pulled          Container image &amp;quot;docker.io/jocatalin/kubernetes-bootcamp:v1&amp;quot; already present on machine
  15m           15m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Created         Created container with docker id f3d04d91e8f2; Security:[seccomp=unconfined]
  15m           15m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Started         Started container with docker id f3d04d91e8f2
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;podのログ表示&#34;&gt;Podのログ表示&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl logs $POD_NAME
Kubernetes Bootcamp App Started At: 2016-12-31T08:15:42.728Z | Running On:  kubernetes-bootcamp-390780338-6j8fn 

Running On: kubernetes-bootcamp-390780338-6j8fn | Total Requests: 1 | App Uptime: 580.532 seconds | Log Time: 2016-12-31T08:25:23.260Z
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;pod内でコマンド実行&#34;&gt;Pod内でコマンド実行&lt;/h3&gt;

&lt;p&gt;envコマンドを実行してみます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl exec $POD_NAME env
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=kubernetes-bootcamp-390780338-6j8fn
KUBERNETES_SERVICE_HOST=10.0.0.1
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBERNETES_PORT=tcp://10.0.0.1:443
KUBERNETES_PORT_443_TCP=tcp://10.0.0.1:443
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_PORT_443_TCP_PORT=443
KUBERNETES_PORT_443_TCP_ADDR=10.0.0.1
NPM_CONFIG_LOGLEVEL=info
NODE_VERSION=6.3.1
HOME=/root
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;hostname&lt;/code&gt; コマンドを実行してみます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl exec $POD_NAME hostname
kubernetes-bootcamp-390780338-6j8fn
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;ip a&lt;/code&gt; を実行してみます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl exec $POD_NAME ip a
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
11: eth0@if12: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default 
    link/ether 02:42:ac:11:00:04 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.4/16 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::42:acff:fe11:4/64 scope link tentative dadfailed 
       valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;pod内でbash実行&#34;&gt;Pod内でbash実行&lt;/h3&gt;

&lt;p&gt;以下のコマンドでbashを実行するとプロンプトが表示されます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl exec -ti $POD_NAME bash
root@kubernetes-bootcamp-390780338-6j8fn:/# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;チュートリアルのために用意されたkubernetes-bootcampアプリに含まれるファイル &lt;code&gt;server.js&lt;/code&gt; の内容を表示してみます。このアプリは Node.js で書かれていることがわかります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@kubernetes-bootcamp-390780338-6j8fn:/# cat server.js
var http = require(&#39;http&#39;);
var requests=0;
var podname= process.env.HOSTNAME;
var startTime;
var host;
var handleRequest = function(request, response) {
  response.setHeader(&#39;Content-Type&#39;, &#39;text/plain&#39;);
  response.writeHead(200);
  response.write(&amp;quot;Hello Kubernetes bootcamp! | Running on: &amp;quot;);
  response.write(host);
  response.end(&amp;quot; | v=1\n&amp;quot;);
  console.log(&amp;quot;Running On:&amp;quot; ,host, &amp;quot;| Total Requests:&amp;quot;, ++requests,&amp;quot;| App Uptime:&amp;quot;, (new Date() - startTime)/1000 , &amp;quot;seconds&amp;quot;, &amp;quot;| Log Time:&amp;quot;,new Date());
}
var www = http.createServer(handleRequest);
www.listen(8080,function () {
    startTime = new Date();;
    host = process.env.HOSTNAME;
    console.log (&amp;quot;Kubernetes Bootcamp App Started At:&amp;quot;,startTime, &amp;quot;| Running On: &amp;quot; ,host, &amp;quot;\n&amp;quot; );
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pod内からcurlで直接アプリにアクセスしてみます。 Node.js コンテナ内でbashを実行しているのでホスト名には localhost を指定しています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@kubernetes-bootcamp-390780338-6j8fn:/# curl localhost:8080
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-6j8fn | v=1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;exit&lt;/code&gt; を入力してPod内のbashを抜けます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@kubernetes-bootcamp-390780338-6j8fn:/# exit
exit
$ 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;module-4-アプリをkubernetes外に公開する&#34;&gt;Module 4: アプリをKubernetes外に公開する&lt;/h2&gt;

&lt;p&gt;概念の説明は &lt;a href=&#34;https://kubernetesbootcamp.github.io/kubernetes-bootcamp/4-1.html&#34;&gt;Services&lt;/a&gt; を参照してください。&lt;/p&gt;

&lt;p&gt;Module 1では &lt;code&gt;minikube proxy&lt;/code&gt; を実行してMacの8001番ポートでリッスンしておいて、Macから localhost:8001 でアクセスしました。&lt;/p&gt;

&lt;p&gt;ここではKubernetesのノード上のポートでリッスンして、Macからminikubeのproxyを経由せずに直接アクセスします。&lt;/p&gt;

&lt;h3 id=&#34;サービス一覧表示&#34;&gt;サービス一覧表示&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get services
NAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   10.0.0.1     &amp;lt;none&amp;gt;        443/TCP   12h
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;kubernetes-bootcampアプリを公開&#34;&gt;kubernetes-bootcampアプリを公開&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl expose deployment/kubernetes-bootcamp --type=&amp;quot;NodePort&amp;quot; --port 8080
service &amp;quot;kubernetes-bootcamp&amp;quot; exposed
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;再度サービス一覧表示&#34;&gt;再度サービス一覧表示&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get services
NAME                  CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE
kubernetes            10.0.0.1     &amp;lt;none&amp;gt;        443/TCP          12h
kubernetes-bootcamp   10.0.0.228   &amp;lt;nodes&amp;gt;       8080:31123/TCP   40s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上で &lt;code&gt;kubectl expose&lt;/code&gt; コマンドでサービスを公開したので、一覧にkubernetes-bootcampが含まれるようになりました。&lt;/p&gt;

&lt;h3 id=&#34;サービス詳細表示&#34;&gt;サービス詳細表示&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe services/kubernetes-bootcamp
Name:                   kubernetes-bootcamp
Namespace:              default
Labels:                 run=kubernetes-bootcamp
Selector:               run=kubernetes-bootcamp
Type:                   NodePort
IP:                     10.0.0.228
Port:                   &amp;lt;unset&amp;gt; 8080/TCP
NodePort:               &amp;lt;unset&amp;gt; 31123/TCP
Endpoints:              172.17.0.4:8080
Session Affinity:       None
No events.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;ノードのポート取得&#34;&gt;ノードのポート取得&lt;/h3&gt;

&lt;p&gt;この後参照するため、ノードのポートを取得して環境変数 NODE_PORT に設定しておきます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template=&#39;{{(index .spec.ports 0).nodePort}}&#39;)
$ echo NODE_PORT=$NODE_PORT
NODE_PORT=31123
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetesbootcamp.github.io/kubernetes-bootcamp/4-2.html&#34;&gt;Module 4のインタラクティブチュートリアル&lt;/a&gt;ではこの後 &lt;code&gt;curl host01:$NODE_PORT&lt;/code&gt; でアクセスしているのですが、手元の環境では &lt;code&gt;host01&lt;/code&gt; というホスト名ではアクセスできません。&lt;/p&gt;

&lt;p&gt;そこで、以下のコマンドを実行してノードのIPアドレスを取得し、環境変数 NODE_IP に設定します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export NODE_IP=$(minikube ip)
$ echo NODE_IP=$NODE_IP
NODE_IP=192.168.99.100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下のコマンドでMacからKubernetesのノードに直接アクセスします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-6j8fn | v=1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Module 4のStep 2でラベルを付けて、Step 3でサービス削除するのですが、この記事を書く時は飛ばしてしまったので、Module 6の後に行います。&lt;/p&gt;

&lt;h2 id=&#34;module-5-アプリをスケールアップする&#34;&gt;Module 5: アプリをスケールアップする&lt;/h2&gt;

&lt;p&gt;概念の説明は &lt;a href=&#34;https://kubernetesbootcamp.github.io/kubernetes-bootcamp/5-1.html&#34;&gt;Running multiple instances of an app&lt;/a&gt; を参照してください。&lt;/p&gt;

&lt;h3 id=&#34;デプロイされたアプリのスケールアップ前のレプリカ数を確認&#34;&gt;デプロイされたアプリのスケールアップ前のレプリカ数を確認&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;kubectl get deployments&lt;/code&gt; の結果にはデプロイごとにアプリのレプリカ（複製）の数が表示されます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get deployments
NAME                  DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kubernetes-bootcamp   1         1         1            1           46m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;この結果ではPodの数は1です。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;DESIRED: デプロイ時に指定したレプリカ数。desireの意味は「切望する」なので、デプロイ時に希望した数ということでしょう。&lt;/li&gt;
&lt;li&gt;CURRENT: 現在実行中のレプリカ数。&lt;/li&gt;
&lt;li&gt;UP-TO-DATE: 指定した状態に更新されたレプリカ数。&lt;/li&gt;
&lt;li&gt;AVAILABLE: ユーザが利用可能な（＝ユーザに実際にサービスが提供されている）レプリカ数。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;スケールアップ&#34;&gt;スケールアップ&lt;/h3&gt;

&lt;p&gt;このデプロイのレプリカ数を4に増やしてみます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl scale deployments/kubernetes-bootcamp --replicas=4
deployment &amp;quot;kubernetes-bootcamp&amp;quot; scaled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;デプロイ一覧で再度確認するとレプリカ数が4に増えていました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get deployments
NAME                  DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kubernetes-bootcamp   4         4         4            4           55m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pod一覧を &lt;code&gt;-o wide&lt;/code&gt; を指定して表示するとIPアドレスとノードを確認できます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods -o wide
NAME                                  READY     STATUS    RESTARTS   AGE       IP           NODE
kubernetes-bootcamp-390780338-6j8fn   1/1       Running   0          55m       172.17.0.4   minikube
kubernetes-bootcamp-390780338-jw7cn   1/1       Running   0          7s        172.17.0.5   minikube
kubernetes-bootcamp-390780338-p8jbb   1/1       Running   0          7s        172.17.0.6   minikube
kubernetes-bootcamp-390780338-vq3kx   1/1       Running   0          7s        172.17.0.7   minikube
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ちなみに &lt;code&gt;-o wide&lt;/code&gt; 無しの出力結果は以下の通りです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods
NAME                                  READY     STATUS    RESTARTS   AGE
kubernetes-bootcamp-390780338-6j8fn   1/1       Running   0          56m
kubernetes-bootcamp-390780338-jw7cn   1/1       Running   0          1m
kubernetes-bootcamp-390780338-p8jbb   1/1       Running   0          1m
kubernetes-bootcamp-390780338-vq3kx   1/1       Running   0          1m
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;スケールアップ後のデプロイの詳細表示&#34;&gt;スケールアップ後のデプロイの詳細表示&lt;/h3&gt;

&lt;p&gt;Events欄にスケールアップした記録が残っています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe deployments/kubernetes-bootcamp
Name:                   kubernetes-bootcamp
Namespace:              default
CreationTimestamp:      Sat, 31 Dec 2016 17:15:41 +0900
Labels:                 run=kubernetes-bootcamp
Selector:               run=kubernetes-bootcamp
Replicas:               4 updated | 4 total | 4 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     True    MinimumReplicasAvailable
OldReplicaSets: &amp;lt;none&amp;gt;
NewReplicaSet:  kubernetes-bootcamp-390780338 (4/4 replicas created)
Events:
  FirstSeen     LastSeen        Count   From                            SubObjectPath   Type            Reason                  Message
  ---------     --------        -----   ----                            -------------   --------        ------                  -------
  58m           58m             1       {deployment-controller }                        Normal          ScalingReplicaSet       Scaled up replica set kubernetes-bootcamp-390780338 to 1
  3m            3m              1       {deployment-controller }                        Normal          ScalingReplicaSet       Scaled up replica set kubernetes-bootcamp-390780338 to 4
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;スケールアップ後のアプリにcurlでアクセス&#34;&gt;スケールアップ後のアプリにcurlでアクセス&lt;/h3&gt;

&lt;p&gt;アクセスしてみるとリクエストごとにランダムなPodに振り分けられ、負荷分散されていることが確認できました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-6j8fn | v=1
$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-6j8fn | v=1
$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-jw7cn | v=1
$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-p8jbb | v=1
$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-6j8fn | v=1
$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-6j8fn | v=1
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;スケールダウン&#34;&gt;スケールダウン&lt;/h3&gt;

&lt;p&gt;レプリカ数を2に減らします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl scale deployments/kubernetes-bootcamp --replicas=2
deployment &amp;quot;kubernetes-bootcamp&amp;quot; scaled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;デプロイ一覧で2に減ったことを確認しました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get deployments
NAME                  DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kubernetes-bootcamp   2         2         2            2           1h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;直後のPod一覧では2つのコンテナのSTATUSがTerminating （終了中）となっていました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods -o wide
NAME                                  READY     STATUS        RESTARTS   AGE       IP           NODE
kubernetes-bootcamp-390780338-6j8fn   1/1       Running       0          1h        172.17.0.4   minikube
kubernetes-bootcamp-390780338-jw7cn   1/1       Running       0          12m       172.17.0.5   minikube
kubernetes-bootcamp-390780338-p8jbb   1/1       Terminating   0          12m       172.17.0.6   minikube
kubernetes-bootcamp-390780338-vq3kx   1/1       Terminating   0          12m       172.17.0.7   minikube
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;数十秒程度してから再度Pod一覧を見るとSTATUSがRunningの2つだけになっていました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods -o wide
NAME                                  READY     STATUS    RESTARTS   AGE       IP           NODE
kubernetes-bootcamp-390780338-6j8fn   1/1       Running   0          1h        172.17.0.4   minikube
kubernetes-bootcamp-390780338-jw7cn   1/1       Running   0          13m       172.17.0.5   minikube
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;module-6-アプリをローリングアップデート&#34;&gt;Module 6: アプリをローリングアップデート&lt;/h2&gt;

&lt;p&gt;概念の説明は &lt;a href=&#34;https://kubernetesbootcamp.github.io/kubernetes-bootcamp/6-1.html&#34;&gt;Performing a rolling update for an app&lt;/a&gt; を参照してください。&lt;/p&gt;

&lt;p&gt;ローリングアップデートではアプリのダウンタイムをゼロでアプリを更新できるそうです。&lt;/p&gt;

&lt;h3 id=&#34;アップデート前の状態確認&#34;&gt;アップデート前の状態確認&lt;/h3&gt;

&lt;p&gt;Pod一覧。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods -o wide
NAME                                  READY     STATUS    RESTARTS   AGE       IP           NODE
kubernetes-bootcamp-390780338-6j8fn   1/1       Running   0          1h        172.17.0.4   minikube
kubernetes-bootcamp-390780338-jw7cn   1/1       Running   0          21m       172.17.0.5   minikube
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pod詳細情報。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe pods
Name:           kubernetes-bootcamp-390780338-6j8fn
Namespace:      default
Node:           minikube/192.168.99.100
Start Time:     Sat, 31 Dec 2016 17:15:41 +0900
Labels:         pod-template-hash=390780338
                run=kubernetes-bootcamp
Status:         Running
IP:             172.17.0.4
Controllers:    ReplicaSet/kubernetes-bootcamp-390780338
Containers:
  kubernetes-bootcamp:
    Container ID:       docker://f3d04d91e8f27b2b537c20d82253376993483f9bb9c0d1196ba50ecc3a69ff7c
    Image:              docker.io/jocatalin/kubernetes-bootcamp:v1
    Image ID:           docker://sha256:8fafd8af70e9aa7c3ab40222ca4fd58050cf3e49cb14a4e7c0f460cd4f78e9fe
    Port:               8080/TCP
    State:              Running
      Started:          Sat, 31 Dec 2016 17:15:42 +0900
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qqsb7 (ro)
    Environment Variables:      &amp;lt;none&amp;gt;
Conditions:
  Type          Status
  Initialized   True
  Ready         True
  PodScheduled  True
Volumes:
  default-token-qqsb7:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-qqsb7
QoS Class:      BestEffort
Tolerations:    &amp;lt;none&amp;gt;
No events.


Name:           kubernetes-bootcamp-390780338-jw7cn
Namespace:      default
Node:           minikube/192.168.99.100
Start Time:     Sat, 31 Dec 2016 18:10:47 +0900
Labels:         pod-template-hash=390780338
                run=kubernetes-bootcamp
Status:         Running
IP:             172.17.0.5
Controllers:    ReplicaSet/kubernetes-bootcamp-390780338
Containers:
  kubernetes-bootcamp:
    Container ID:       docker://23e8c9c3c2b64701f88a61033b534a23f7f2e4a540afa019eea20050bfd12a39
    Image:              docker.io/jocatalin/kubernetes-bootcamp:v1
    Image ID:           docker://sha256:8fafd8af70e9aa7c3ab40222ca4fd58050cf3e49cb14a4e7c0f460cd4f78e9fe
    Port:               8080/TCP
    State:              Running
      Started:          Sat, 31 Dec 2016 18:10:48 +0900
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qqsb7 (ro)
    Environment Variables:      &amp;lt;none&amp;gt;
Conditions:
  Type          Status
  Initialized   True
  Ready         True
  PodScheduled  True
Volumes:
  default-token-qqsb7:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-qqsb7
QoS Class:      BestEffort
Tolerations:    &amp;lt;none&amp;gt;
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath                           Type            Reason          Message
  ---------     --------        -----   ----                    -------------                           --------        ------          -------
  21m           21m             1       {default-scheduler }                                            Normal          Scheduled       Successfully assigned kubernetes-bootcamp-390780338-jw7cn to minikube
  21m           21m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Pulled          Container image &amp;quot;docker.io/jocatalin/kubernetes-bootcamp:v1&amp;quot; already present on machine
  21m           21m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Created         Created container with docker id 23e8c9c3c2b6; Security:[seccomp=unconfined]
  21m           21m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Started         Started container with docker id 23e8c9c3c2b6
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;アプリのバージョンアップ&#34;&gt;アプリのバージョンアップ&lt;/h3&gt;

&lt;p&gt;以下のコマンドでアプリをv1からv2にバージョンアップします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2$ kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2
deployment &amp;quot;kubernetes-bootcamp&amp;quot; image updated
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;直後のPods一覧。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods -o wide
NAME                                   READY     STATUS        RESTARTS   AGE       IP           NODE
kubernetes-bootcamp-2100875782-0jd0d   1/1       Running       0          26s       172.17.0.6   minikube
kubernetes-bootcamp-2100875782-vnxk1   1/1       Running       0          26s       172.17.0.7   minikube
kubernetes-bootcamp-390780338-6j8fn    1/1       Terminating   0          1h        172.17.0.4   minikube
kubernetes-bootcamp-390780338-jw7cn    1/1       Terminating   0          24m       172.17.0.5   minikube
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;十秒程度したあとのPods一覧。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods -o wide
NAME                                   READY     STATUS    RESTARTS   AGE       IP           NODE
kubernetes-bootcamp-2100875782-0jd0d   1/1       Running   0          55s       172.17.0.6   minikube
kubernetes-bootcamp-2100875782-vnxk1   1/1       Running   0          55s       172.17.0.7   minikube
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;サービス詳細。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
$ kubectl describe services/kubernetes-bootcamp
Name:                   kubernetes-bootcamp
Namespace:              default
Labels:                 run=kubernetes-bootcamp
Selector:               run=kubernetes-bootcamp
Type:                   NodePort
IP:                     10.0.0.228
Port:                   &amp;lt;unset&amp;gt; 8080/TCP
NodePort:               &amp;lt;unset&amp;gt; 31123/TCP
Endpoints:              172.17.0.6:8080,172.17.0.7:8080
Session Affinity:       None
No events.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;アップデート後のアプリにアクセス&#34;&gt;アップデート後のアプリにアクセス&lt;/h3&gt;

&lt;p&gt;curlでアクセスしてみると出力にv=2と表示され、アップデートされたアプリが利用可能になっていることが確認できました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-2100875782-0jd0d | v=2
$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-2100875782-0jd0d | v=2
$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-2100875782-vnxk1 | v=2
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;ローリングアップデート後の状態確認&#34;&gt;ローリングアップデート後の状態確認&lt;/h3&gt;

&lt;p&gt;ローリングアップデートの状態確認。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl rollout status deployments/kubernetes-bootcamp
deployment &amp;quot;kubernetes-bootcamp&amp;quot; successfully rolled out
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pod詳細表示。Imageの値が &lt;code&gt;jocatalin/kubernetes-bootcamp:v2&lt;/code&gt; と v2になっていることが確認できます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe pods
Name:           kubernetes-bootcamp-2100875782-0jd0d
Namespace:      default
Node:           minikube/192.168.99.100
Start Time:     Sat, 31 Dec 2016 18:34:57 +0900
Labels:         pod-template-hash=2100875782
                run=kubernetes-bootcamp
Status:         Running
IP:             172.17.0.6
Controllers:    ReplicaSet/kubernetes-bootcamp-2100875782
Containers:
  kubernetes-bootcamp:
    Container ID:       docker://7438b24d95242018dae9b4e82b93055d772f14650c688203b80204073d67d84b
    Image:              jocatalin/kubernetes-bootcamp:v2
    Image ID:           docker://sha256:b6556396ebd45c517469c522c3c61ecf5ab708cafe0e59df906278d34c255ef8
    Port:               8080/TCP
    State:              Running
      Started:          Sat, 31 Dec 2016 18:34:58 +0900
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qqsb7 (ro)
    Environment Variables:      &amp;lt;none&amp;gt;
Conditions:
  Type          Status
  Initialized   True 
  Ready         True 
  PodScheduled  True 
Volumes:
  default-token-qqsb7:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-qqsb7
QoS Class:      BestEffort
Tolerations:    &amp;lt;none&amp;gt;
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath                           Type            Reason          Message
  ---------     --------        -----   ----                    -------------                           --------        ------          -------
  5m            5m              1       {default-scheduler }                                            Normal          Scheduled       Successfully assigned kubernetes-bootcamp-2100875782-0jd0d to minikube
  5m            5m              1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Pulled          Container image &amp;quot;jocatalin/kubernetes-bootcamp:v2&amp;quot; already present on machine
  5m            5m              1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Created         Created container with docker id 7438b24d9524; Security:[seccomp=unconfined]
  5m            5m              1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Started         Started container with docker id 7438b24d9524


Name:           kubernetes-bootcamp-2100875782-vnxk1
Namespace:      default
Node:           minikube/192.168.99.100
Start Time:     Sat, 31 Dec 2016 18:34:57 +0900
Labels:         pod-template-hash=2100875782
                run=kubernetes-bootcamp
Status:         Running
IP:             172.17.0.7
Controllers:    ReplicaSet/kubernetes-bootcamp-2100875782
Containers:
  kubernetes-bootcamp:
    Container ID:       docker://54c58841abb18466fb0f79636111ef5ff193226f43a5741d1730efeb4689ba58
    Image:              jocatalin/kubernetes-bootcamp:v2
    Image ID:           docker://sha256:b6556396ebd45c517469c522c3c61ecf5ab708cafe0e59df906278d34c255ef8
    Port:               8080/TCP
    State:              Running
      Started:          Sat, 31 Dec 2016 18:34:58 +0900
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qqsb7 (ro)
    Environment Variables:      &amp;lt;none&amp;gt;
Conditions:
  Type          Status
  Initialized   True 
  Ready         True 
  PodScheduled  True 
Volumes:
  default-token-qqsb7:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-qqsb7
QoS Class:      BestEffort
Tolerations:    &amp;lt;none&amp;gt;
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath                           Type            Reason          Message
  ---------     --------        -----   ----                    -------------                           --------        ------          -------
  5m            5m              1       {default-scheduler }                                            Normal          Scheduled       Successfully assigned kubernetes-bootcamp-2100875782-vnxk1 to minikube
  5m            5m              1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Pulled          Container image &amp;quot;jocatalin/kubernetes-bootcamp:v2&amp;quot; already present on machine
  5m            5m              1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Created         Created container with docker id 54c58841abb1; Security:[seccomp=unconfined]
  5m            5m              1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Started         Started container with docker id 54c58841abb1
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;アプリのバージョンアップ失敗の例&#34;&gt;アプリのバージョンアップ失敗の例&lt;/h3&gt;

&lt;h4 id=&#34;存在しないタグのイメージにアップデート&#34;&gt;存在しないタグのイメージにアップデート&lt;/h4&gt;

&lt;p&gt;次はv10とタグ付けされたイメージにアップデートを試みます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v10
deployment &amp;quot;kubernetes-bootcamp&amp;quot; image updated
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;問題なくアップデートされたように見えますが、実はv10というタグのイメージは存在しないのでエラーになります。&lt;/p&gt;

&lt;p&gt;デプロイ一覧のレプリカ数を見ると、DESIREDが2に対してAVAILABLEが1であり希望した状態になっていないことがわかります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get deployments
NAME                  DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kubernetes-bootcamp   2         3         2            1           1h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pods一覧を見ると一部のPodはSTATUSがImagePullBackOffとなっています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods -o wide
NAME                                   READY     STATUS             RESTARTS   AGE       IP           NODE
kubernetes-bootcamp-1951388213-lpc3k   0/1       ImagePullBackOff   0          26s       172.17.0.4   minikube
kubernetes-bootcamp-1951388213-mwx9v   0/1       ImagePullBackOff   0          25s       172.17.0.5   minikube
kubernetes-bootcamp-2100875782-0jd0d   1/1       Terminating        0          10m       172.17.0.6   minikube
kubernetes-bootcamp-2100875782-vnxk1   1/1       Running            0          10m       172.17.0.7   minikube
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;間を置いて何度か試していると、STATUSがErrImagePullとなっているときもありました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods -o wide
NAME                                   READY     STATUS             RESTARTS   AGE       IP           NODE
kubernetes-bootcamp-1951388213-lpc3k   0/1       ImagePullBackOff   0          1m        172.17.0.4   minikube
kubernetes-bootcamp-1951388213-mwx9v   0/1       ErrImagePull       0          1m        172.17.0.5   minikube
kubernetes-bootcamp-2100875782-vnxk1   1/1       Running            0          11m       172.17.0.7   minikube
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pod詳細。
Imageの値が &lt;code&gt;jocatalin/kubernetes-bootcamp:v10&lt;/code&gt; であるコンテナのEventsを見ると
&lt;code&gt;Failed to pull image &amp;quot;jocatalin/kubernetes-bootcamp:v10&amp;quot;: Tag v10 not found in repository docker.io/jocatalin/kubernetes-bootcamp&lt;/code&gt; というエラーがあり、タグv10はレジストリに無かったことがわかります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe pods
Name:           kubernetes-bootcamp-1951388213-lpc3k
Namespace:      default
Node:           minikube/192.168.99.100
Start Time:     Sat, 31 Dec 2016 18:44:43 +0900
Labels:         pod-template-hash=1951388213
                run=kubernetes-bootcamp
Status:         Pending
IP:             172.17.0.4
Controllers:    ReplicaSet/kubernetes-bootcamp-1951388213
Containers:
  kubernetes-bootcamp:
    Container ID:
    Image:              jocatalin/kubernetes-bootcamp:v10
    Image ID:
    Port:               8080/TCP
    State:              Waiting
      Reason:           ImagePullBackOff
    Ready:              False
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qqsb7 (ro)
    Environment Variables:      &amp;lt;none&amp;gt;
Conditions:
  Type          Status
  Initialized   True 
  Ready         False 
  PodScheduled  True 
Volumes:
  default-token-qqsb7:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-qqsb7
QoS Class:      BestEffort
Tolerations:    &amp;lt;none&amp;gt;
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath                           Type            Reason          Message
  ---------     --------        -----   ----                    -------------                           --------        ------          -------
  6m            6m              1       {default-scheduler }                                            Normal          Scheduled       Successfully assigned kubernetes-bootcamp-1951388213-lpc3k to minikube
  6m            36s             6       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Pulling         pulling image &amp;quot;jocatalin/kubernetes-bootcamp:v10&amp;quot;
  6m            30s             6       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Warning         Failed          Failed to pull image &amp;quot;jocatalin/kubernetes-bootcamp:v10&amp;quot;: Tag v10 not found in repository docker.io/jocatalin/kubernetes-bootcamp
  6m            30s             6       {kubelet minikube}                                              Warning         FailedSync      Error syncing pod, skipping: failed to &amp;quot;StartContainer&amp;quot; for &amp;quot;kubernetes-bootcamp&amp;quot; with ErrImagePull: &amp;quot;Tag v10 not found in reposit
ory docker.io/jocatalin/kubernetes-bootcamp&amp;quot;

  6m    2s      22      {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal  BackOff         Back-off pulling image &amp;quot;jocatalin/kubernetes-bootcamp:v10&amp;quot;
  6m    2s      22      {kubelet minikube}                                              Warning FailedSync      Error syncing pod, skipping: failed to &amp;quot;StartContainer&amp;quot; for &amp;quot;kubernetes-bootcamp&amp;quot; with ImagePullBackOff: &amp;quot;Back-off pulling image \&amp;quot;jocatalin/kubernetes-bo
otcamp:v10\&amp;quot;&amp;quot;



Name:           kubernetes-bootcamp-1951388213-mwx9v
Namespace:      default
Node:           minikube/192.168.99.100
Start Time:     Sat, 31 Dec 2016 18:44:44 +0900
Labels:         pod-template-hash=1951388213
                run=kubernetes-bootcamp
Status:         Pending
IP:             172.17.0.5
Controllers:    ReplicaSet/kubernetes-bootcamp-1951388213
Containers:
  kubernetes-bootcamp:
    Container ID:
    Image:              jocatalin/kubernetes-bootcamp:v10
    Image ID:
    Port:               8080/TCP
    State:              Waiting
      Reason:           ImagePullBackOff
    Ready:              False
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qqsb7 (ro)
    Environment Variables:      &amp;lt;none&amp;gt;
Conditions:
  Type          Status
  Initialized   True 
  Ready         False 
  PodScheduled  True 
Volumes:
  default-token-qqsb7:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-qqsb7
QoS Class:      BestEffort
Tolerations:    &amp;lt;none&amp;gt;
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath                           Type            Reason          Message
  ---------     --------        -----   ----                    -------------                           --------        ------          -------
  6m            6m              1       {default-scheduler }                                            Normal          Scheduled       Successfully assigned kubernetes-bootcamp-1951388213-mwx9v to minikube
  6m            2m              5       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Pulling         pulling image &amp;quot;jocatalin/kubernetes-bootcamp:v10&amp;quot;
  6m            2m              5       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Warning         Failed          Failed to pull image &amp;quot;jocatalin/kubernetes-bootcamp:v10&amp;quot;: Tag v10 not found in repository docker.io/jocatalin/kubernetes-bootcamp
  6m            2m              5       {kubelet minikube}                                              Warning         FailedSync      Error syncing pod, skipping: failed to &amp;quot;StartContainer&amp;quot; for &amp;quot;kubernetes-bootcamp&amp;quot; with ErrImagePull: &amp;quot;Tag v10 not found in repository docker.io/jocatalin/kubernetes-bootcamp&amp;quot;

  6m    14s     23      {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal  BackOff         Back-off pulling image &amp;quot;jocatalin/kubernetes-bootcamp:v10&amp;quot;
  6m    14s     23      {kubelet minikube}                                              Warning FailedSync      Error syncing pod, skipping: failed to &amp;quot;StartContainer&amp;quot; for &amp;quot;kubernetes-bootcamp&amp;quot; with ImagePullBackOff: &amp;quot;Back-off pulling image \&amp;quot;jocatalin/kubernetes-bootcamp:v10\&amp;quot;&amp;quot;



Name:           kubernetes-bootcamp-2100875782-vnxk1
Namespace:      default
Node:           minikube/192.168.99.100
Start Time:     Sat, 31 Dec 2016 18:34:57 +0900
Labels:         pod-template-hash=2100875782
                run=kubernetes-bootcamp
Status:         Running
IP:             172.17.0.7
Controllers:    ReplicaSet/kubernetes-bootcamp-2100875782
Containers:
  kubernetes-bootcamp:
    Container ID:       docker://54c58841abb18466fb0f79636111ef5ff193226f43a5741d1730efeb4689ba58
    Image:              jocatalin/kubernetes-bootcamp:v2
    Image ID:           docker://sha256:b6556396ebd45c517469c522c3c61ecf5ab708cafe0e59df906278d34c255ef8
    Port:               8080/TCP
    State:              Running
      Started:          Sat, 31 Dec 2016 18:34:58 +0900
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qqsb7 (ro)
    Environment Variables:      &amp;lt;none&amp;gt;
Conditions:
  Type          Status
  Initialized   True 
  Ready         True 
  PodScheduled  True 
Volumes:
  default-token-qqsb7:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-qqsb7
QoS Class:      BestEffort
Tolerations:    &amp;lt;none&amp;gt;
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath                           Type            Reason          Message
  ---------     --------        -----   ----                    -------------                           --------        ------          -------
  16m           16m             1       {default-scheduler }                                            Normal          Scheduled       Successfully assigned kubernetes-bootcamp-2100875782-vnxk1 to minikube
  16m           16m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Pulled          Container image &amp;quot;jocatalin/kubernetes-bootcamp:v2&amp;quot; already present on machine
  16m           16m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Created         Created container with docker id 54c58841abb1; Security:[seccomp=unconfined]
  16m           16m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Started         Started container with docker id 54c58841abb1
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;v10へのローリングアップデートを中止&#34;&gt;v10へのローリングアップデートを中止&lt;/h4&gt;

&lt;p&gt;ローリングアップデートをアンドゥします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl rollout undo deployments/kubernetes-bootcamp$ kubectl rollout undo deployments/kubernetes-bootcamp
deployment &amp;quot;kubernetes-bootcamp&amp;quot; rolled back
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pod一覧。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods -o wide
NAME                                   READY     STATUS    RESTARTS   AGE       IP           NODE
kubernetes-bootcamp-2100875782-vnxk1   1/1       Running   0          20m       172.17.0.7   minikube
kubernetes-bootcamp-2100875782-x290l   1/1       Running   0          17s       172.17.0.4   minikube
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pod詳細。&lt;/p&gt;

&lt;p&gt;Podのレプリカ数は以前指定した2で、2つのPodともImageが &lt;code&gt;jocatalin/kubernetes-bootcamp:v2&lt;/code&gt; とアップデート前のバージョンに戻ったことが確認できました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ 
kubectl describe pods$ kubectl describe pods
Name:           kubernetes-bootcamp-2100875782-vnxk1
Namespace:      default
Node:           minikube/192.168.99.100
Start Time:     Sat, 31 Dec 2016 18:34:57 +0900
Labels:         pod-template-hash=2100875782
                run=kubernetes-bootcamp
Status:         Running
IP:             172.17.0.7
Controllers:    ReplicaSet/kubernetes-bootcamp-2100875782
Containers:
  kubernetes-bootcamp:
    Container ID:       docker://54c58841abb18466fb0f79636111ef5ff193226f43a5741d1730efeb4689ba58
    Image:              jocatalin/kubernetes-bootcamp:v2
    Image ID:           docker://sha256:b6556396ebd45c517469c522c3c61ecf5ab708cafe0e59df906278d34c255ef8
    Port:               8080/TCP
    State:              Running
      Started:          Sat, 31 Dec 2016 18:34:58 +0900
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qqsb7 (ro)
    Environment Variables:      &amp;lt;none&amp;gt;
Conditions:
  Type          Status
  Initialized   True 
  Ready         True 
  PodScheduled  True 
Volumes:
  default-token-qqsb7:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-qqsb7
QoS Class:      BestEffort
Tolerations:    &amp;lt;none&amp;gt;
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath                           Type            Reason          Message
  ---------     --------        -----   ----                    -------------                           --------        ------          -------
  21m           21m             1       {default-scheduler }                                            Normal          Scheduled       Successfully assigned kubernetes-bootcamp-2100875782-vnxk1 to minikube
  21m           21m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Pulled          Container image &amp;quot;jocatalin/kubernetes-bootcamp:v2&amp;quot; already present on machine
  21m           21m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Created         Created container with docker id 54c58841abb1; Security:[seccomp=unconfined]
  21m           21m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Started         Started container with docker id 54c58841abb1


Name:           kubernetes-bootcamp-2100875782-x290l
Namespace:      default
Node:           minikube/192.168.99.100
Start Time:     Sat, 31 Dec 2016 18:55:31 +0900
Labels:         pod-template-hash=2100875782
                run=kubernetes-bootcamp
Status:         Running
IP:             172.17.0.4
Controllers:    ReplicaSet/kubernetes-bootcamp-2100875782
Containers:
  kubernetes-bootcamp:
    Container ID:       docker://523dca2d5839e832942af50d52fe8008c16862c19ebed553e50293765f4cf12c
    Image:              jocatalin/kubernetes-bootcamp:v2
    Image ID:           docker://sha256:b6556396ebd45c517469c522c3c61ecf5ab708cafe0e59df906278d34c255ef8
    Port:               8080/TCP
    State:              Running
      Started:          Sat, 31 Dec 2016 18:55:32 +0900
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qqsb7 (ro)
    Environment Variables:      &amp;lt;none&amp;gt;
Conditions:
  Type          Status
  Initialized   True 
  Ready         True 
  PodScheduled  True 
Volumes:
  default-token-qqsb7:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-qqsb7
QoS Class:      BestEffort
Tolerations:    &amp;lt;none&amp;gt;
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath                           Type            Reason          Message
  ---------     --------        -----   ----                    -------------                           --------        ------          -------
  21m           21m             1       {default-scheduler }                                            Normal          Scheduled       Successfully assigned kubernetes-bootcamp-2100875782-vnxk1 to minikube
  21m           21m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Pulled          Container image &amp;quot;jocatalin/kubernetes-bootcamp:v2&amp;quot; already present on machine
  21m           21m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Created         Created container with docker id 54c58841abb1; Security:[seccomp=unconfined]
  21m           21m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Started         Started container with docker id 54c58841abb1


Name:           kubernetes-bootcamp-2100875782-x290l
Namespace:      default
Node:           minikube/192.168.99.100
Start Time:     Sat, 31 Dec 2016 18:55:31 +0900
Labels:         pod-template-hash=2100875782
                run=kubernetes-bootcamp
Status:         Running
IP:             172.17.0.4
Controllers:    ReplicaSet/kubernetes-bootcamp-2100875782
Containers:
  kubernetes-bootcamp:
    Container ID:       docker://523dca2d5839e832942af50d52fe8008c16862c19ebed553e50293765f4cf12c
    Image:              jocatalin/kubernetes-bootcamp:v2
    Image ID:           docker://sha256:b6556396ebd45c517469c522c3c61ecf5ab708cafe0e59df906278d34c255ef8
    Port:               8080/TCP
    State:              Running
      Started:          Sat, 31 Dec 2016 18:55:32 +0900
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qqsb7 (ro)
    Environment Variables:      &amp;lt;none&amp;gt;
Conditions:
  Type          Status
  Initialized   True 
  Ready         True 
  PodScheduled  True 
Volumes:
  default-token-qqsb7:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-qqsb7
QoS Class:      BestEffort
Tolerations:    &amp;lt;none&amp;gt;
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath                           Type            Reason          Message
  ---------     --------        -----   ----                    -------------                           --------        ------          -------
  1m            1m              1       {default-scheduler }                                            Normal          Scheduled       Successfully assigned kubernetes-bootcamp-2100875782-x290l to minikube
  1m            1m              1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Pulled          Container image &amp;quot;jocatalin/kubernetes-bootcamp:v2&amp;quot; already present on machine
  1m            1m              1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Created         Created container with docker id 523dca2d5839; Security:[seccomp=unconfined]
  1m            1m              1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Started         Started container with docker id 523dca2d5839
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;curlでアクセスしてみても v2 と表示されています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-2100875782-vnxk1 | v=2
$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-2100875782-vnxk1 | v=2
$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-2100875782-x290l | v=2
$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-2100875782-x290l | v=2
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;module-4のstep-2-ラベルを付ける&#34;&gt;Module 4のStep 2: ラベルを付ける&lt;/h2&gt;

&lt;p&gt;デプロイの詳細表示。
Labelsにrun=kubernetes-bootcampというのがデフォルトで付いていることがわかります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe deployment
Name:                   kubernetes-bootcamp
Namespace:              default
CreationTimestamp:      Sat, 31 Dec 2016 17:15:41 +0900
Labels:                 run=kubernetes-bootcamp
Selector:               run=kubernetes-bootcamp
Replicas:               2 updated | 2 total | 2 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     True    MinimumReplicasAvailable
OldReplicaSets: &amp;lt;none&amp;gt;
NewReplicaSet:  kubernetes-bootcamp-2100875782 (2/2 replicas created)
Events:
  FirstSeen     LastSeen        Count   From                            SubObjectPath   Type            Reason                  Message
  ---------     --------        -----   ----                            -------------   --------        ------                  -------
  55m           55m             1       {deployment-controller }                        Normal          ScalingReplicaSet       Scaled up replica set kubernetes-bootcamp-390780338 to 4
  43m           43m             1       {deployment-controller }                        Normal          ScalingReplicaSet       Scaled down replica set kubernetes-bootcamp-390780338 to 2
  31m           31m             1       {deployment-controller }                        Normal          ScalingReplicaSet       Scaled up replica set kubernetes-bootcamp-2100875782 to 1
  31m           31m             1       {deployment-controller }                        Normal          ScalingReplicaSet       Scaled down replica set kubernetes-bootcamp-390780338 to 1
  31m           31m             1       {deployment-controller }                        Normal          ScalingReplicaSet       Scaled down replica set kubernetes-bootcamp-390780338 to 0
  21m           21m             1       {deployment-controller }                        Normal          ScalingReplicaSet       Scaled up replica set kubernetes-bootcamp-1951388213 to 1
  21m           21m             1       {deployment-controller }                        Normal          ScalingReplicaSet       Scaled down replica set kubernetes-bootcamp-2100875782 to 1
  21m           21m             1       {deployment-controller }                        Normal          ScalingReplicaSet       Scaled up replica set kubernetes-bootcamp-1951388213 to 2
  31m           11m             2       {deployment-controller }                        Normal          ScalingReplicaSet       Scaled up replica set kubernetes-bootcamp-2100875782 to 2
  11m           11m             1       {deployment-controller }                        Normal          DeploymentRollback      Rolled back deployment &amp;quot;kubernetes-bootcamp&amp;quot; to revision 2
  11m           11m             1       {deployment-controller }                        Normal          ScalingReplicaSet       Scaled down replica set kubernetes-bootcamp-1951388213 to 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;指定したラベルを持つPods一覧表示。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods -l run=kubernetes-bootcamp
NAME                                   READY     STATUS    RESTARTS   AGE
kubernetes-bootcamp-2100875782-vnxk1   1/1       Running   0          32m
kubernetes-bootcamp-2100875782-x290l   1/1       Running   0          11m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;指定したラベルを持つサービス一覧表示。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get services -l run=kubernetes-bootcamp
NAME                  CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE
kubernetes-bootcamp   10.0.0.228   &amp;lt;nodes&amp;gt;       8080:31123/TCP   1h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pod名を取得して環境変数POD_NAMEに設定。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export POD_NAME=$(kubectl get pods -o go-template --template &#39;{{range .items}}{{.metadata.name}}{{&amp;quot;\n&amp;quot;}}{{end}}&#39;)
$ echo Name of the Pod: $POD_NAME
Name of the Pod: kubernetes-bootcamp-2100875782-vnxk1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Podにapp=v2というラベルを設定。チュートリアルではapp=v1というラベルを指定していますが、この記事ではバージョンアップ後に実行しているのでapp=v2にします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl label pod $POD_NAME app=v2
pod &amp;quot;kubernetes-bootcamp-2100875782-vnxk1&amp;quot; labeled
pod &amp;quot;kubernetes-bootcamp-2100875782-x290l&amp;quot; labeled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pod詳細表示。
Labelsにapp=v2が付いています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl describe pods $POD_NAME
Name:           kubernetes-bootcamp-2100875782-vnxk1
Namespace:      default
Node:           minikube/192.168.99.100
Start Time:     Sat, 31 Dec 2016 18:34:57 +0900
Labels:         app=v2
                pod-template-hash=2100875782
                run=kubernetes-bootcamp
Status:         Running
IP:             172.17.0.7
Controllers:    ReplicaSet/kubernetes-bootcamp-2100875782
Containers:
  kubernetes-bootcamp:
    Container ID:       docker://54c58841abb18466fb0f79636111ef5ff193226f43a5741d1730efeb4689ba58
    Image:              jocatalin/kubernetes-bootcamp:v2
    Image ID:           docker://sha256:b6556396ebd45c517469c522c3c61ecf5ab708cafe0e59df906278d34c255ef8
    Port:               8080/TCP
    State:              Running
      Started:          Sat, 31 Dec 2016 18:34:58 +0900
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qqsb7 (ro)
    Environment Variables:      &amp;lt;none&amp;gt;
Conditions:
  Type          Status
  Initialized   True 
  Ready         True 
  PodScheduled  True 
Volumes:
  default-token-qqsb7:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-qqsb7
QoS Class:      BestEffort
Tolerations:    &amp;lt;none&amp;gt;
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath                           Type            Reason          Message
  ---------     --------        -----   ----                    -------------                           --------        ------          -------
  36m           36m             1       {default-scheduler }                                            Normal          Scheduled       Successfully assigned kubernetes-bootcamp-2100875782-vnxk1 to minikube
  36m           36m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Pulled          Container image &amp;quot;jocatalin/kubernetes-bootcamp:v2&amp;quot; already present on machine
  36m           36m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Created         Created container with docker id 54c58841abb1; Security:[seccomp=unconfined]
  36m           36m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Started         Started container with docker id 54c58841abb1


Name:           kubernetes-bootcamp-2100875782-x290l
Namespace:      default
Node:           minikube/192.168.99.100
Start Time:     Sat, 31 Dec 2016 18:55:31 +0900
Labels:         app=v2
                pod-template-hash=2100875782
                run=kubernetes-bootcamp
Status:         Running
IP:             172.17.0.4
Controllers:    ReplicaSet/kubernetes-bootcamp-2100875782
Containers:
  kubernetes-bootcamp:
    Container ID:       docker://523dca2d5839e832942af50d52fe8008c16862c19ebed553e50293765f4cf12c
    Image:              jocatalin/kubernetes-bootcamp:v2
    Image ID:           docker://sha256:b6556396ebd45c517469c522c3c61ecf5ab708cafe0e59df906278d34c255ef8
    Port:               8080/TCP
    State:              Running
      Started:          Sat, 31 Dec 2016 18:55:32 +0900
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qqsb7 (ro)
    Environment Variables:      &amp;lt;none&amp;gt;
Conditions:
  Type          Status
  Initialized   True 
  Ready         True 
  PodScheduled  True 
Volumes:
  default-token-qqsb7:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-qqsb7
QoS Class:      BestEffort
Tolerations:    &amp;lt;none&amp;gt;
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath                           Type            Reason          Message
  ---------     --------        -----   ----                    -------------                           --------        ------          -------
  15m           15m             1       {default-scheduler }                                            Normal          Scheduled       Successfully assigned kubernetes-bootcamp-2100875782-x290l to minikube
  15m           15m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Pulled          Container image &amp;quot;jocatalin/kubernetes-bootcamp:v2&amp;quot; already present on machine
  15m           15m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Created         Created container with docker id 523dca2d5839; Security:[seccomp=unconfined]
  15m           15m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Started         Started container with docker id 523dca2d5839
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;今つけたラベルを持つPod一覧表示。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods -l app=v2
NAME                                   READY     STATUS    RESTARTS   AGE
kubernetes-bootcamp-2100875782-vnxk1   1/1       Running   0          40m
kubernetes-bootcamp-2100875782-x290l   1/1       Running   0          19m
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;module-4のstep-3-サービスを削除&#34;&gt;Module 4のStep 3: サービスを削除&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl delete service -l run=kubernetes-bootcamp
service &amp;quot;kubernetes-bootcamp&amp;quot; deleted
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;サービス一覧を確認すると kubernetes-bootcamp が消えていることがわかります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get services
NAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   10.0.0.1     &amp;lt;none&amp;gt;        443/TCP   13h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;curlでアクセスすると接続拒否という期待される結果になりました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl $NODE_IP:$NODE_PORT
curl: (7) Failed to connect to 192.168.99.100 port 31123: Connection refused
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pod一覧を見るとPod自体は存在します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods
NAME                                   READY     STATUS    RESTARTS   AGE
kubernetes-bootcamp-2100875782-vnxk1   1/1       Running   0          40m
kubernetes-bootcamp-2100875782-x290l   1/1       Running   0          19m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Podに入ってアクセスするとアプリ自体は引き続き稼働中であることがわかります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export POD_NAME=kubernetes-bootcamp-2100875782-vnxk1
$ kubectl exec -ti $POD_NAME curl localhost:8080
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-2100875782-vnxk1 | v=2
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;

&lt;p&gt;ローリングアップデートとアップデート失敗時の切り戻しが簡単に行えるのは良いなと思いました。&lt;/p&gt;

&lt;p&gt;このチュートリアルはステートレスなアプリケーションの例でしたが、
&lt;a href=&#34;http://kubernetes.io/docs/tutorials/&#34;&gt;Tutorials - Kubernetes&lt;/a&gt;
には Stateful Applications というチュートリアルもあるので、こちらも後日試してみたいです。　&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LXD で privileged な CentOS 7コンテナを作る</title>
      <link>https://hnakamur.github.io/blog/2016/10/22/lxd-privileged-centos-container/</link>
      <pubDate>Sat, 22 Oct 2016 18:54:49 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2016/10/22/lxd-privileged-centos-container/</guid>
      <description>

&lt;p&gt;小ネタのメモです。&lt;/p&gt;

&lt;p&gt;先日 LXD 2.0.5 で CentOS 7 コンテナを起動して &lt;code&gt;journalctl -xe&lt;/code&gt; を実行すると以下のようなエラーが出ていました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Oct 22 09:53:58 centos systemd-sysctl[36]: Failed to write &#39;16&#39; to &#39;/proc/sys/kernel/sysrq&#39;: Permission denied
Oct 22 09:53:58 centos systemd-sysctl[36]: Failed to write &#39;1&#39; to &#39;/proc/sys/fs/protected_hardlinks&#39;: Permission denied
Oct 22 09:53:58 centos systemd-sysctl[36]: Failed to write &#39;1&#39; to &#39;/proc/sys/kernel/core_uses_pid&#39;: Permission denied
Oct 22 09:53:58 centos systemd-sysctl[36]: Failed to write &#39;1&#39; to &#39;/proc/sys/fs/protected_symlinks&#39;: Permission denied
Oct 22 09:53:58 centos systemd-remount-fs[35]: /bin/mount for / exited with exit status 32.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;コンテナ作成時に以下のように config で &lt;code&gt;security.privileged&lt;/code&gt; を true に設定しておけば出なくなりました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lxc launch -c security.privileged=true images:centos/7/amd64 コンテナ名
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;設定の確認は以下のコマンドで行います。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ lxc config show コンテナ名
name: centos
profiles:
- default
config:
  security.privileged: &amp;quot;true&amp;quot;
  volatile.base_image: d2a0b3cf928778ad1582ee1feb39a0bbcd57edce01a60868f04e78d959886d71
  volatile.eth0.hwaddr: 00:16:3e:b2:dc:5e
  volatile.last_state.idmap: &#39;[]&#39;
devices:
  root:
    path: /
    type: disk
ephemeral: false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;もっと限定した設定でも対応可能かもしれませんが、とりあえずこれで。&lt;/p&gt;

&lt;h2 id=&#34;2016-10-23-追記&#34;&gt;2016-10-23 追記&lt;/h2&gt;

&lt;p&gt;security.privileged を true にするのは良くないと指摘されました。&lt;/p&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/hnakamur2&#34;&gt;@hnakamur2&lt;/a&gt; Don&amp;#39;t do that! This errors are actually bugs ( see &lt;a href=&#34;https://t.co/5IuFQzMI9u&#34;&gt;https://t.co/5IuFQzMI9u&lt;/a&gt; + &lt;a href=&#34;https://t.co/4ypMXS5FTq&#34;&gt;https://t.co/4ypMXS5FTq&lt;/a&gt; ), so report them to CentOS&lt;/p&gt;&amp;mdash; Marqin (@mrMarqin) &lt;a href=&#34;https://twitter.com/mrMarqin/status/789838146083098625&#34;&gt;October 22, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;CentOS にバグ報告というのはよくわからなかったので、LXDにイシューを立ててみました。
&lt;a href=&#34;https://github.com/lxc/lxd/issues/2544&#34;&gt;CentOS 7 container gets errors like systemd-sysctl[36]: Failed to write &amp;lsquo;16&amp;rsquo; to &amp;lsquo;/proc/sys/kernel/sysrq&amp;rsquo;: Permission denied · Issue #2544 · lxc/lxd&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Go言語のos.Chtimesで設定可能な最大日時は 2262-04-11 23:47:16.854775807 &#43;0000 UTC</title>
      <link>https://hnakamur.github.io/blog/2016/10/22/max-time-for-golang-os-chtimes/</link>
      <pubDate>Sat, 22 Oct 2016 18:32:50 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2016/10/22/max-time-for-golang-os-chtimes/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://golang.org/pkg/os/#Chtimes&#34;&gt;os.Chtimes&lt;/a&gt; のソース&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://golang.org/src/os/file_posix.go?s=3693:3758#L123&#34;&gt;src/os/file_posix.go - The Go Programming Language&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/golang/go/blob/go1.7.3/src/os/file_posix.go#L133-L141&#34;&gt;go/file_posix.go at go1.7.3 · golang/go&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;を見ると、引数は &lt;code&gt;time.Time&lt;/code&gt; なのですが、 &lt;code&gt;syscall.Timespec&lt;/code&gt; に変換するときに &lt;code&gt;time&lt;/code&gt; の &lt;code&gt;UnixNano()&lt;/code&gt; を使っています。 &lt;code&gt;UnixNano()&lt;/code&gt; は 1970-01-01T00:00:00Z からの通算ミリ秒です。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;UnixNano()&lt;/code&gt; で int64 の最大値を設定したときと、 &lt;code&gt;time.Time&lt;/code&gt; で表現可能な最大の日時を調べてみました。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://play.golang.org/p/eUj5L-eEkS&#34;&gt;https://play.golang.org/p/eUj5L-eEkS&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
	&amp;quot;fmt&amp;quot;
	&amp;quot;math&amp;quot;
	&amp;quot;time&amp;quot;
)

func main() {
	fmt.Println(time.Unix(int64(math.MaxInt64)/1e9, int64(math.MaxInt64)%1e9).UTC())
	fmt.Println(time.Unix(math.MaxInt64, 1e9-1).UTC())
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;2262-04-11 23:47:16.854775807 +0000 UTC
219250468-12-04 15:30:07.999999999 +0000 UTC
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;となりました。&lt;/p&gt;

&lt;p&gt;Linux amd64 環境だと &lt;a href=&#34;https://github.com/golang/go/blob/go1.7.3/src/syscall/syscall_linux_amd64.go#L91-L95&#34;&gt;NsecToTimespec&lt;/a&gt; と &lt;a href=&#34;https://github.com/golang/go/blob/go1.7.3/src/syscall/ztypes_linux_amd64.go#L24-L27&#34;&gt;Timespec&lt;/a&gt; は&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NsecToTimespec(nsec int64) (ts Timespec) {
	ts.Sec = nsec / 1e9
	ts.Nsec = nsec % 1e9
	return
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;type Timespec struct {
	Sec  int64
	Nsec int64
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;となっているので、 &lt;code&gt;NsecToTimespec&lt;/code&gt; を使わずに&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Chtimes(name string, atime time.Time, mtime time.Time) error {
	var utimes [2]syscall.Timespec
	utimes[0] = syscall.Timespec(atime.Unix(), atime.Nanosecond())
	utimes[1] = syscall.Timespec(mtime.Unix(), mtime.Nanosecond())
	if e := syscall.UtimesNano(name, utimes[0:]); e != nil {
		return &amp;amp;PathError{&amp;quot;chtimes&amp;quot;, name, e}
	}
	return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;と書けば &lt;code&gt;time.Time&lt;/code&gt; の限界まで渡すことは出来ます。とは言え &lt;code&gt;syscall.UtimesNano&lt;/code&gt; が対応しているかはまた別問題ですが。&lt;/p&gt;

&lt;p&gt;2262 年まで表現できれば個人的には困らないので、メモだけ残しておくということで。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LocaleOverlaySwaggerというgoaプラグインを書いた</title>
      <link>https://hnakamur.github.io/blog/2016/10/22/localeoverlayswagger-goa-plugin/</link>
      <pubDate>Sat, 22 Oct 2016 16:52:02 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2016/10/22/localeoverlayswagger-goa-plugin/</guid>
      <description>

&lt;h2 id=&#34;まず-swagger-仕様を複数ファイル出力する-goa-プラグイン-multiswagger-を試してみました&#34;&gt;まず Swagger 仕様を複数ファイル出力する goa プラグイン Multiswagger を試してみました&lt;/h2&gt;

&lt;p&gt;まずは &lt;a href=&#34;http://tchssk.hatenablog.com/entry/2016/10/18/122215&#34;&gt;Swagger 仕様を複数ファイル出力する goa プラグイン Multiswagger を作った - tchsskのブログ&lt;/a&gt; を読んで試してみました。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/goadesign/goa/&#34;&gt;goadesign/goa: Design-based APIs and microservices in Go&lt;/a&gt; の README からリンクされているサンプル &lt;a href=&#34;https://github.com/goadesign/goa-cellar&#34;&gt;goadesign/goa-cellar: goa winecellar example service&lt;/a&gt; の &lt;code&gt;design.go&lt;/code&gt; の各種項目の &lt;code&gt;Title&lt;/code&gt; や &lt;code&gt;Description&lt;/code&gt; の値に JSON を書いて英語と日本語の説明を書いてみた例が &lt;a href=&#34;https://github.com/hnakamur/goa-getting-started/blob/use_multiswagger/design/design.go&#34;&gt;goa-getting-started/design.go&lt;/a&gt; です。　&lt;/p&gt;

&lt;p&gt;私が試したバージョンの &lt;a href=&#34;https://github.com/tchssk/multiswagger/tree/7ad4f69b2209316035dd222819228f90327cd1f3&#34;&gt;Multiswagger at 7ad4f69b2209316035dd222819228f90327cd1f3&lt;/a&gt; では &lt;a href=&#34;https://github.com/hnakamur/goa-getting-started/blob/4bef7925510700d8797831f3bb665eb87c8ca6b9/design/design.go#L8-L19&#34;&gt;API定義&lt;/a&gt; の &lt;code&gt;Title&lt;/code&gt; や &lt;code&gt;Definition&lt;/code&gt; は非対応だったので、 &lt;a href=&#34;https://github.com/tchssk/multiswagger/compare/master...hnakamur:support_more_fields&#34;&gt;Comparing tchssk:master&amp;hellip;hnakamur:support_more_fields · tchssk/multiswagger&lt;/a&gt; のように変更して試してみました。&lt;/p&gt;

&lt;p&gt;変更に際して以下の点にハマりました。&lt;/p&gt;

&lt;p&gt;ハマった点その1。 API定義は &lt;a href=&#34;https://godoc.org/github.com/goadesign/goa/goagen/gen_swagger#Swagger&#34;&gt;github.com/goadesign/goagen/genswagger/Swagger&lt;/a&gt; の &lt;code&gt;Definitions&lt;/code&gt; に保持されるのですが、値の型が &lt;code&gt;map[string]*genschema.JSONSchema&lt;/code&gt; となっていて、 &lt;code&gt;JSONSchema&lt;/code&gt; の値は &lt;a href=&#34;https://github.com/goadesign/goa/blob/4d19425396efa86b61d97c3cda0b00ec21f103f7/goagen/gen_schema/json_schema.go#L100&#34;&gt;goa/json_schema.go のグローバル変数 Definitions&lt;/a&gt; に保持されています。&lt;/p&gt;

&lt;p&gt;このため &lt;a href=&#34;https://github.com/hnakamur/multiswagger/blob/ec57ee4e1b17d0b13091e0b3d17649796967ed64/generator.go#L142-L173&#34;&gt;extract 関数&lt;/a&gt; 内で JSON 文字列から最初のキーの値を取り出して書き変えた後、 &lt;a href=&#34;https://github.com/hnakamur/multiswagger/blob/ec57ee4e1b17d0b13091e0b3d17649796967ed64/generator.go#L72&#34;&gt;generator.go#L72&lt;/a&gt; で次のキー用に &lt;code&gt;Swagger&lt;/code&gt; の値を作り直しても JSONSchema は古い値が再利用されてしまいます。そこで &lt;a href=&#34;https://github.com/hnakamur/multiswagger/blob/ec57ee4e1b17d0b13091e0b3d17649796967ed64/generator.go#L71&#34;&gt;generator.go#L71&lt;/a&gt; で &lt;code&gt;genschema.Definitions&lt;/code&gt; を初期化することで対応できました。&lt;/p&gt;

&lt;p&gt;ハマった点その2。生成された &lt;a href=&#34;https://github.com/hnakamur/goa-getting-started/blob/4bef7925510700d8797831f3bb665eb87c8ca6b9/swagger/swagger.ja.yaml#L8&#34;&gt;swagger.ja.yaml の 8 行目&lt;/a&gt;  を見ると &lt;code&gt;definitions&lt;/code&gt; の &lt;code&gt;description&lt;/code&gt; に &lt;code&gt;(default view)&lt;/code&gt; という値が自動的に追加されています。  &lt;a href=&#34;https://github.com/hnakamur/goa-getting-started/blob/4bef7925510700d8797831f3bb665eb87c8ca6b9/design/design.go#L42-L45&#34;&gt;design.go#L42-L45&lt;/a&gt; に JSON を書いていても &lt;code&gt;(default view)&lt;/code&gt; という値が追加されるので JSON としてパースしようとするとエラーになってしまいます。そこで、値が &lt;code&gt;(default view)&lt;/code&gt; で終わっていたら、それを取り除いてから JSON としてパース可能か調べるようにしました。そしてパースできる場合はパースして特定のキーの値を取り出してから最後に &lt;code&gt;(default view)&lt;/code&gt; とつけるようにしました。&lt;/p&gt;

&lt;p&gt;やれやれこれで大丈夫かと思ったのですが、 &lt;a href=&#34;https://github.com/hnakamur/goa-getting-started/blob/4bef7925510700d8797831f3bb665eb87c8ca6b9/swagger/swagger.ja.yaml#L33-L69&#34;&gt;swagger.ja.yaml#L33-L69&lt;/a&gt; の &lt;code&gt;error&lt;/code&gt; の &lt;code&gt;description&lt;/code&gt; は英語になっています。 &lt;a href=&#34;https://github.com/hnakamur/goa-getting-started/blob/4bef7925510700d8797831f3bb665eb87c8ca6b9/design/design.go&#34;&gt;design.go&lt;/a&gt; に書いていないデフォルト値が出力されているようです。&lt;/p&gt;

&lt;p&gt;また、 &lt;a href=&#34;https://github.com/hnakamur/goa-getting-started/blob/4bef7925510700d8797831f3bb665eb87c8ca6b9/swagger/swagger.ja.yaml#L95&#34;&gt;swagger.ja.yaml#L95&lt;/a&gt; の &lt;code&gt;summary&lt;/code&gt; も show bottle と英語になっています。これは今はコメントにしていますが &lt;a href=&#34;https://github.com/hnakamur/goa-getting-started/blob/4bef7925510700d8797831f3bb665eb87c8ca6b9/design/design.go#L36&#34;&gt;design.go#L36&lt;/a&gt; のように &lt;code&gt;Metadata(&amp;quot;swagger:summary&amp;quot;, value)&lt;/code&gt; で設定可能なことがわかりました。&lt;/p&gt;

&lt;p&gt;しかしこの値を JSON で書くとなると &lt;a href=&#34;https://github.com/hnakamur/multiswagger/blob/ec57ee4e1b17d0b13091e0b3d17649796967ed64/generator.go#L175-L253&#34;&gt;walk 関数&lt;/a&gt; で Metadata で &lt;code&gt;&amp;quot;swagger:summary&amp;quot;&lt;/code&gt; 特定のキーの場合だけ処理するという改修が必要です。&lt;/p&gt;

&lt;p&gt;このあたりで辛くなってきました。 &lt;code&gt;design.go&lt;/code&gt; の DSL はそのままで値に JSON を書くという設計は &lt;code&gt;design.go&lt;/code&gt; で各言語のメッセージが一覧できるという利点がある一方、 generator の実装が面倒だと思います。あと、言語が増えると &lt;code&gt;design.go&lt;/code&gt; の API 定義に対するメッセージ文字列の行が増えて API 定義が見にくくなるという欠点もあると思いました。&lt;/p&gt;

&lt;h2 id=&#34;ということで-localeovrerlayswagger-という別の-swagger-仕様生成プラグインを作りました&#34;&gt;ということで LocaleOvrerlaySwagger という別の Swagger 仕様生成プラグインを作りました&lt;/h2&gt;

&lt;p&gt;ソースは &lt;a href=&#34;https://github.com/hnakamur/localeoverlayswagger&#34;&gt;hnakamur/localeoverlayswagger&lt;/a&gt; で公開しています。&lt;/p&gt;

&lt;p&gt;使い方は &lt;a href=&#34;https://github.com/hnakamur/localeoverlayswagger#usage&#34;&gt;README の Usage&lt;/a&gt; をご参照ください。メッセージの書き方ですが、 &lt;a href=&#34;https://github.com/hnakamur/goa-getting-started/blob/overlay_japanese_yaml/design/design.go&#34;&gt;design.go&lt;/a&gt; の各種 Description は標準通り英語で書きます。&lt;/p&gt;

&lt;p&gt;英語の Swagger 仕様は標準と同じ内容で &lt;a href=&#34;https://github.com/hnakamur/goa-getting-started/blob/overlay_japanese_yaml/swagger/swagger.yaml&#34;&gt;swagger/swagger.yaml&lt;/a&gt; のように生成されます。 この内置き換えた部分だけのキーを含む YAML ファイルを &lt;a href=&#34;https://github.com/hnakamur/goa-getting-started/blob/overlay_japanese_yaml/locales/ja.yaml&#34;&gt;overlay_japanese_yaml&lt;/a&gt; のように書いておくと、 &lt;a href=&#34;https://github.com/hnakamur/goa-getting-started/blob/overlay_japanese_yaml/swagger/swagger.ja.yaml&#34;&gt;swagger/swagger.ja.yaml&lt;/a&gt; のようにその部分だけ上書きされた YAML が生成されるという仕組みです。&lt;/p&gt;

&lt;p&gt;英語のメッセージに対応する日本語のメッセージを離れたところに書く必要があるのでその点は不便なのですが、生成された英語の YAML を見ながら対応するキーに日本語メッセージを書くだけで良いので、トータルではこちらのほうが管理が楽だと個人的には思います。&lt;/p&gt;

&lt;p&gt;ということで、良かったらご利用ください。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>pgpool-IIを使ってPostgreSQLのアクティブ・スタンバイ(1&#43;1構成)を試してみた</title>
      <link>https://hnakamur.github.io/blog/2016/09/15/experiment-postgresql-active-standby-using-pgpool-ii/</link>
      <pubDate>Thu, 15 Sep 2016 06:28:34 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2016/09/15/experiment-postgresql-active-standby-using-pgpool-ii/</guid>
      <description>

&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;

&lt;p&gt;pgool-IIを使ってPostgreSQLのアクティブ・スタンバイ(1+1構成)を試したのでメモです。&lt;/p&gt;

&lt;p&gt;以下のページを参考にしました。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html&#34;&gt;pgpool-II ユーザマニュアル&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.pgpool.net/pgpool-web/contrib_docs/watchdog_master_slave_3.3/ja.html&#34;&gt;pgpool-II watchdog チュートリアル（master-slave mode）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://lets.postgresql.jp/documents/technical/pgpool-II-3.3-watchdog/1&#34;&gt;pgpool-II 3.3 の watchdog 機能 — Let&amp;rsquo;s Postgres&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;テスト用のansible-playbook&#34;&gt;テスト用のAnsible playbook&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/hnakamur/postgresql-pgpool2-failover-example-playbook&#34;&gt;https://github.com/hnakamur/postgresql-pgpool2-failover-example-playbook&lt;/a&gt; に置きました。&lt;/p&gt;

&lt;p&gt;LXD をセットアップ済みの Ubuntu 16.04 上で試しました。&lt;/p&gt;

&lt;p&gt;LXD で CentOS 7 のコンテナを2つ作って環境構築しています。
PostgreSQL と pgpool-II は &lt;a href=&#34;http://yum.postgresql.org/repopackages.php&#34;&gt;PostgreSQL RPM Repository (with Yum)&lt;/a&gt; からインストールしました。
PostgreSQL のバージョンは 9.5.4、 pgpool-II のバージョンは 3.5.4 です。&lt;/p&gt;

&lt;h2 id=&#34;今回の構成&#34;&gt;今回の構成&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.pgpool.net/pgpool-web/contrib_docs/watchdog_master_slave_3.3/ja.html&#34;&gt;pgpool-II watchdog チュートリアル（master-slave mode）&lt;/a&gt; の図と同様の構成となっています。&lt;/p&gt;

&lt;p&gt;ただし、今回の構成では pgpool-II と PostgreSQL を別のコンテナにせず1つのコンテナに同居させていて、以下の2つのコンテナで構成しています。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;pgsql1 (IPアドレス &lt;code&gt;10.155.92.101&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;pgsql2 (IPアドレス &lt;code&gt;10.155.92.102&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;pgpool-II は watchdog で相互監視するマスタ・スタンバイ構成 (
&lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html#config&#34;&gt;pgpool-II の設定&lt;/a&gt; のマスタスレーブモード ) です。
pgpool-II のマスタが仮想 IP &lt;code&gt;10.155.92.100&lt;/code&gt; を持ちます。&lt;/p&gt;

&lt;p&gt;pgpool-II から PostgreSQL を監視するのは heartbeat という仕組みを今回は使っています。&lt;/p&gt;

&lt;p&gt;レプリケーションは pgpool-II ではなく PostgreSQL の非同期ストリーミング・レプリケーションを使っています。&lt;/p&gt;

&lt;p&gt;また、 pgpool-II の負荷分散 (ロードバランサ) 機能は今回は使っていません。&lt;/p&gt;

&lt;p&gt;なお、仮想 IP はあくまで pgpool-II のマスタと連動するもので、 PostgreSQL のプライマリとは別のコンテナになることもあります。&lt;/p&gt;

&lt;p&gt;pgpool-II のドキュメントやコマンドの出力を見ると、 pgpool-II のマスタはマスタ、 PostgreSQL のマスタはプライマリと用語を使い分けているようです。この記事もそれに従います。&lt;/p&gt;

&lt;p&gt;pgpool-II と PostgreSQL のポートはそれぞれデフォルトの 9999 と 5432 としています。
pgpool-II は他に管理用のポートとして 9898、 watchdog 用のポートで 9000 を使います。&lt;/p&gt;

&lt;h2 id=&#34;セットアップの事前準備&#34;&gt;セットアップの事前準備&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://hnakamur.github.io/blog/blog/2016/08/21/experiment-postgresql-active-standby-cluster-using-pacemaker/&#34;&gt;Pacemakerを使ってPostgreSQLのアクティブ・スタンバイ(1+1構成)を試してみた · hnakamur&amp;rsquo;s blog at github&lt;/a&gt; と同様です。&lt;/p&gt;

&lt;h2 id=&#34;pgpool-ii-の管理者ユーザとパスワード&#34;&gt;pgpool-II の管理者ユーザとパスワード&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html#pcp_config&#34;&gt;pcp.conf の設定&lt;/a&gt; に従って pgpool-II の管理者のユーザ名と md5 暗号化したパスワードを &lt;code&gt;/etc/pgpool-II-95/pcp.conf&lt;/code&gt; に設定しています。&lt;/p&gt;

&lt;p&gt;管理者ユーザ名は &lt;code&gt;pgpool2&lt;/code&gt; としました。&lt;/p&gt;

&lt;p&gt;パスワードは以下のコマンドを実行して &lt;code&gt;group_vars/development/secrets.yml&lt;/code&gt; を復号化し、 &lt;code&gt;development.secrets.pgpool2_admin_password&lt;/code&gt; の値を参照してください。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ansible-vault decrypt group_vars/development/secrets.yml 
Vault password: 
Decryption successful
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;コンテナの作成&#34;&gt;コンテナの作成&lt;/h2&gt;

&lt;p&gt;以下のコマンドを実行して &lt;code&gt;pgsql1&lt;/code&gt; と &lt;code&gt;pgsql2&lt;/code&gt; という2つのコンテナを作成します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ansible-playbook launch_containers.yml -D -v
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;vaultのパスワードを聞かれますので入力してください。&lt;/p&gt;

&lt;h2 id=&#34;コンテナ内に-postgresql-と-pgpool-ii-をセットアップ&#34;&gt;コンテナ内に PostgreSQL と pgpool-II をセットアップ&lt;/h2&gt;

&lt;p&gt;以下のコマンドを実行して、コンテナ内に PostgreSQL と pgpool-II をセットアップします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ansible-playbook setup_containers.yml -D -v
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;セットアップが完了したときの初期状態では &lt;code&gt;pgsql1&lt;/code&gt; の pgpool-II がマスタで仮想IPを持ち、 PostgreSQL も &lt;code&gt;pgsql1&lt;/code&gt; がプライマリとなっています。&lt;/p&gt;

&lt;h2 id=&#34;状態確認のコマンド説明&#34;&gt;状態確認のコマンド説明&lt;/h2&gt;

&lt;h3 id=&#34;postgresql-のプロセス確認&#34;&gt;PostgreSQL のプロセス確認&lt;/h3&gt;

&lt;p&gt;起動してしばらく経ってから &lt;code&gt;pgsql1&lt;/code&gt; コンテナの PostgreSQL プロセスを ps で見ると以下のようになります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql1 ~]# ps axf | grep [p]ostgres
 1464 ?        S      0:00 /usr/pgsql-9.5/bin/postgres -D /var/lib/pgsql/9.5/data
 1465 ?        Ss     0:00  \_ postgres: logger process   
 1467 ?        Ss     0:00  \_ postgres: checkpointer process   
 1468 ?        Ss     0:00  \_ postgres: writer process   
 1469 ?        Ss     0:00  \_ postgres: wal writer process   
 1470 ?        Ss     0:00  \_ postgres: autovacuum launcher process   
 1471 ?        Ss     0:00  \_ postgres: archiver process   last was 000000010000000000000002.00000028.backup
 1472 ?        Ss     0:00  \_ postgres: stats collector process   
 1720 ?        Ss     0:00  \_ postgres: wal sender process repl_user 10.155.92.102(43074) streaming 0/3000060
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;pgsql2&lt;/code&gt; ではこうなります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 ~]# ps axf | grep [p]ostgres
 1386 ?        S      0:00 /usr/pgsql-9.5/bin/postgres -D /var/lib/pgsql/9.5/data
 1387 ?        Ss     0:00  \_ postgres: logger process   
 1388 ?        Ss     0:00  \_ postgres: startup process   recovering 000000010000000000000003
 1394 ?        Ss     0:00  \_ postgres: checkpointer process   
 1395 ?        Ss     0:00  \_ postgres: writer process   
 1396 ?        Ss     0:00  \_ postgres: stats collector process   
 1399 ?        Ss     0:00  \_ postgres: wal receiver process   streaming 0/3000060
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;postgres: wal sender&lt;/code&gt; のプロセスがあれば PostgreSQL のプライマリ、 &lt;code&gt;postgres: wal receiver&lt;/code&gt; のプロセスがあれば PostgreSQL のスタンバイと判断することが出来ます。&lt;/p&gt;

&lt;p&gt;ただし、PostgreSQL のプライマリが切り替わってしばらくの間はこのプロセスは存在しないので、 次項の方法を使います。&lt;/p&gt;

&lt;h3 id=&#34;pgpool-ii-から見た-postgresql-ノードの状態確認&#34;&gt;pgpool-II から見た PostgreSQL ノードの状態確認&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://tyawan080.hatenablog.com/entry/2014/05/12/234226&#34;&gt;PostgreSQLのマスタ判断 - Marlock Homes Diary&lt;/a&gt; で知りました。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;select pg_is_in_recovery()&lt;/code&gt; を実行して &lt;code&gt;t&lt;/code&gt; であればスタンバイ、 &lt;code&gt;f&lt;/code&gt; であればプライマリかスタンドアロンです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql1 ~]# sudo -i -u postgres psql -c &amp;quot;select pg_is_in_recovery()&amp;quot;
 pg_is_in_recovery 
-------------------
 f
(1 row)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 ~]# sudo -i -u postgres psql -c &amp;quot;select pg_is_in_recovery()&amp;quot;
 pg_is_in_recovery 
-------------------
 t
(1 row)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;pgpool-ii-から見た-postgresql-ノードの状態確認-1&#34;&gt;pgpool-II から見た PostgreSQL ノードの状態確認&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html&#34;&gt;pgpool-II ユーザマニュアル&lt;/a&gt; の &lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html#show-commands&#34;&gt;SHOWコマンド&lt;/a&gt; の &lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html#pool_nodes&#34;&gt;pool_nodes&lt;/a&gt; に対応します。&lt;/p&gt;

&lt;p&gt;どちらかのコンテナで以下のコマンドを実行します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql1 ~]# sudo -i -u postgres psql -h localhost -p 9999 -c &amp;quot;show pool_nodes&amp;quot;
 node_id |   hostname    | port | status | lb_weight |  role   | select_cnt 
---------+---------------+------+--------+-----------+---------+------------
 0       | 10.155.92.101 | 5432 | 2      | 0.500000  | primary | 0
 1       | 10.155.92.102 | 5432 | 2      | 0.500000  | standby | 0
(2 rows)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;なお、 今回の設定では &lt;code&gt;postgres&lt;/code&gt; ユーザのパスワードを &lt;code&gt;/var/lib/pgsql/.pgpass&lt;/code&gt; に書いているのでパスワード入力は不要です。実運用時は書かないほうが良いでしょう。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;role&lt;/code&gt; 列の primary か standby で PostgreSQL のプライマリかスタンバイもわかるようですが、切替時はすぐに更新されなかったことがあったような気がします。&lt;/p&gt;

&lt;p&gt;切り替え直後は &lt;code&gt;select pg_is_in_recovery()&lt;/code&gt; を実行する方式のほうが良さそうです。&lt;/p&gt;

&lt;p&gt;status 列の値については &lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html&#34;&gt;pgpool-II ユーザマニュアル&lt;/a&gt; の &lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html#pcp_node_info&#34;&gt;pcp_node_info&lt;/a&gt; に説明があります。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;0 - 初期化時のみに表われる。PCP コマンドで表示されることはない。&lt;/li&gt;
&lt;li&gt;1 - ノード稼働中。接続無し&lt;/li&gt;
&lt;li&gt;2 - ノード稼働中。接続有り&lt;/li&gt;
&lt;li&gt;3 - ノードダウン&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;watchdog-から見た-pgpool-ii-の状態確認&#34;&gt;watchdog から見た pgpool-II の状態確認&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html&#34;&gt;pgpool-II ユーザマニュアル&lt;/a&gt; の &lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html#pcp_watchdog_info&#34;&gt;pcp_watchdog_info&lt;/a&gt; に対応します。&lt;/p&gt;

&lt;p&gt;以下のコマンドを実行します。 pgpool-II の管理者 &lt;code&gt;pgpool2&lt;/code&gt; のパスワードを聞かれますので入力してください。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql1 ~]# sudo -i -u postgres /usr/pgpool-9.5/bin/pcp_watchdog_info -h localhost -U pgpool2 -v
Password: 
Watchdog Cluster Information 
Total Nodes          : 2
Remote Nodes         : 1
Quorum state         : QUORUM EXIST
Alive Remote Nodes   : 1
VIP up on local node : YES
Master Node Name     : Linux_pgsql1_9999
Master Host Name     : 10.155.92.101

Watchdog Node Information 
Node Name      : Linux_pgsql1_9999
Host Name      : 10.155.92.101
Delegate IP    : 10.155.92.100
Pgpool port    : 9999
Watchdog port  : 9000
Node priority  : 1
Status         : 4
Status Name    : MASTER

Node Name      : Linux_pgsql2_9999
Host Name      : 10.155.92.102
Delegate IP    : 10.155.92.100
Pgpool port    : 9999
Watchdog port  : 9000
Node priority  : 1
Status         : 7
Status Name    : STANDBY

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;Status Name&lt;/code&gt; の種類は src/watchdog/watchdog.c 内にで定義されていました。 &lt;code&gt;Status&lt;/code&gt; はこの配列内のゼロオリジンのインデクスです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;char *wd_state_names[] = {
        &amp;quot;DEAD&amp;quot;,
        &amp;quot;LOADING&amp;quot;,
        &amp;quot;JOINING&amp;quot;,
        &amp;quot;INITIALIZING&amp;quot;,
        &amp;quot;MASTER&amp;quot;,
        &amp;quot;PARTICIPATING IN ELECTION&amp;quot;,
        &amp;quot;STANDING FOR MASTER&amp;quot;,
        &amp;quot;STANDBY&amp;quot;,
        &amp;quot;LOST&amp;quot;,
        &amp;quot;IN NETWORK TROUBLE&amp;quot;,
        &amp;quot;SHUTDOWN&amp;quot;,
        &amp;quot;ADD MESSAGE SENT&amp;quot;};
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;pgsql1-プライマリ-の-postgresql-を強制停止してフェイルオーバーのテスト&#34;&gt;pgsql1 (プライマリ)の PostgreSQL を強制停止してフェイルオーバーのテスト&lt;/h2&gt;

&lt;h3 id=&#34;フェイルオーバー時に呼び出されるスクリプト&#34;&gt;フェイルオーバー時に呼び出されるスクリプト&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html&#34;&gt;pgpool-II ユーザマニュアル&lt;/a&gt; の &lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html#failover_in_stream_mode&#34;&gt;Streaming Replicationでのフェイルオーバ&lt;/a&gt; に対応します。&lt;/p&gt;

&lt;p&gt;フェイルオーバー時には &lt;code&gt;/etc/pgpool-II-95/pgpool.conf&lt;/code&gt; の &lt;code&gt;failover_command&lt;/code&gt; に設定したスクリプトが実行されます。今回の構成では以下のような設定にしました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;failover_command = &#39;/var/lib/pgsql/9.5/data/pgpool_failover %d %P %H %R&#39;
                   # NOTE: %dなどの値は src/main/pgpool_main.c の trigger_failover_command で設定しています。
                   # Executes this command at failover
                   # Special values:
                   #   %d = node id
                   #   %h = host name
                   #   %p = port number
                   #   %D = database cluster path
                   #   %m = new master node id
                   #   %H = hostname of the new master node
                   #   %M = old master node id
                   #   %P = old primary node id
                   #   %r = new master port number
                   #   %R = new master database cluster path
                   #   %% = &#39;%&#39; character
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;/var/lib/pgsql/9.5/data/pgpool_failover&lt;/code&gt; は &lt;a href=&#34;https://github.com/hnakamur/postgresql-pgpool2-failover-example-playbook/blob/7f3ef8af54a4f9ec948d65bfc47e33db4792737d/roles/postgresql_db/templates/pgpool_failover.j2&#34;&gt;roles/postgresql_db/templates/pgpool_failover.j2&lt;/a&gt; から Ansible の template モジュールで生成しています。&lt;/p&gt;

&lt;h3 id=&#34;フェイルオーバーの実行&#34;&gt;フェイルオーバーの実行&lt;/h3&gt;

&lt;p&gt;実行後にどう動いたか確認できるように &lt;code&gt;logger&lt;/code&gt; コマンドでログを出力するようにしています。 &lt;code&gt;journalctl -f&lt;/code&gt; でログを &lt;code&gt;tail -f&lt;/code&gt; 的な感じで見られるので、これで見ながら実行します。&lt;/p&gt;

&lt;p&gt;初期状態では pgsql1 が PostgreSQL のプライマリになっています。&lt;/p&gt;

&lt;p&gt;pgsql2 で以下のコマンドを実行します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 ~]# journalctl -f | grep pgpool_failover
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pgsql1 で以下のコマンドを実行しバックグラウンド ( &lt;code&gt;&amp;amp;&lt;/code&gt; は 1つ) で PostgreSQL を強制停止しつつ、 journald のログを表示します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql1 ~]# sudo -i -u postgres /usr/pgsql-9.5/bin/pg_ctl stop -m immediate -D /var/lib/pgsql/9.5/data &amp;amp; journalctl -f | grep pgpool_failover
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;何回か試してみたのですが、 &lt;code&gt;/var/lib/pgsql/9.5/data/pgpool_failover&lt;/code&gt; は pgsql1 で実行される場合と pgsql2 で実行される場合があり、どうやらランダムにどちらか一方で実行されるということのようです。また今回の構成では root ユーザで実行されました。&lt;/p&gt;

&lt;p&gt;以下は pgsql1 での出力結果です。
Ctrl-C で &lt;code&gt;journalctl -f&lt;/code&gt; を停止します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[1] 2319
waiting for server to shut down.... done
server stopped

^C
[1]+  Done                  sudo -i -u postgres /usr/pgsql-9.5/bin/pg_ctl stop -m immediate -D /var/lib/pgsql/9.5/data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下は pgsql2 での出力結果です。
Ctrl-C で &lt;code&gt;journalctl -f&lt;/code&gt; を停止します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Sep 15 13:06:47 pgsql2 pgpool[1432]: 2016-09-15 13:06:47: pid 1432: LOG:  execute command: /var/lib/pgsql/9.5/data/pgpool_failover 0 0 10.155.92.102 /var/lib/pgsql/9.5/data
Sep 15 13:06:47 pgsql2 pgpool_failover[32612]: start args=0 0 10.155.92.102 /var/lib/pgsql/9.5/data UID=0
Sep 15 13:06:47 pgsql2 pgpool_failover[32620]: created promote_trigger file
^C
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;少ししてから PostgreSQL のノードの状態を確認すると以下のようになりました。
pgsql2 (10.155.92.102) の role が primary になり、 pgsql1 (10.155.92.101) は
role が standby で status が &lt;code&gt;3&lt;/code&gt; (ノードダウン) になっています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 ~]# sudo -i -u postgres psql -h localhost -p 9999 -c &amp;quot;show pool_nodes&amp;quot;
 node_id |   hostname    | port | status | lb_weight |  role   | select_cnt 
---------+---------------+------+--------+-----------+---------+------------
 0       | 10.155.92.101 | 5432 | 3      | 0.500000  | standby | 0
 1       | 10.155.92.102 | 5432 | 2      | 0.500000  | primary | 0
(2 rows)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pgsql2 で &lt;code&gt;select pg_is_in_recovery()&lt;/code&gt; を実行すると &lt;code&gt;f&lt;/code&gt; になってプライマリになっていることがわかります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 pgsql]# sudo -i -u postgres psql -c &amp;quot;select pg_is_in_recovery()&amp;quot;
 pg_is_in_recovery 
-------------------
 f
(1 row)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pgsql2 側での変更実験としてデータベースを作成してみます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 ~]# sudo -i -u postgres createdb foo
[root@pgsql2 ~]# sudo -i -u postgres psql -l
                             List of databases
   Name    |  Owner   | Encoding | Collate | Ctype |   Access privileges   
-----------+----------+----------+---------+-------+-----------------------
 foo       | postgres | UTF8     | C       | C     | 
 postgres  | postgres | UTF8     | C       | C     | 
 template0 | postgres | UTF8     | C       | C     | =c/postgres          +
           |          |          |         |       | postgres=CTc/postgres
 template1 | postgres | UTF8     | C       | C     | =c/postgres          +
           |          |          |         |       | postgres=CTc/postgres
(4 rows)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;なお、 pgpool-II 自体のマスタ・スタンバイの役割は変わらず同じで、 pgsql1 がマスタ、 pgsql2 がスタンバイで、 仮想IP は pgsql1 についた状態です。&lt;/p&gt;

&lt;h2 id=&#34;pgsql1-の-postgresql-をオンラインリカバリしスタンバイとして復帰させる&#34;&gt;pgsql1 の PostgreSQL をオンラインリカバリしスタンバイとして復帰させる&lt;/h2&gt;

&lt;h3 id=&#34;オンラインリカバリで呼び出される2つのスクリプト&#34;&gt;オンラインリカバリで呼び出される2つのスクリプト&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html&#34;&gt;pgpool-II ユーザマニュアル&lt;/a&gt; の &lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html#online_recovery_in_stream_mode&#34;&gt;Streaming Replicationでのオンラインリカバリ&lt;/a&gt; に対応します。&lt;/p&gt;

&lt;p&gt;オンラインリカバリで呼び出されるスクリプトは2つあります。この2つのスクリプトは &lt;code&gt;failover_command&lt;/code&gt; とは違って、必ずプライマリ側で &lt;code&gt;postgres&lt;/code&gt; ユーザで実行されます。&lt;/p&gt;

&lt;p&gt;1つ目は &lt;code&gt;/etc/pgpool-II-95/pgpool.conf&lt;/code&gt; の &lt;code&gt;recovery_1st_stage_command&lt;/code&gt; に指定したスクリプトです。
ここではファイル名のみが指定可能で、ディレクトリは PostgreSQL のデータディレクトリ (今回の構成では /var/lib/pgsql/9.5/data ) と決められています。
&lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html&#34;&gt;pgpool-II ユーザマニュアル&lt;/a&gt; の &lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html#RECOVERY_1ST_STAGE_COMMAND&#34;&gt;recovery_1st_stage_command&lt;/a&gt; によるとセキュリティ上の観点からそうしているそうです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;recovery_1st_stage_command = &#39;recovery_1st_stage&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;/var/lib/pgsql/9.5/data/recovery_1st_stage&lt;/code&gt; は &lt;a href=&#34;https://github.com/hnakamur/postgresql-pgpool2-failover-example-playbook/blob/7f3ef8af54a4f9ec948d65bfc47e33db4792737d/roles/postgresql_db/templates/recovery_1st_stage.j2&#34;&gt;roles/postgresql_db/templates/recovery_1st_stage.j2&lt;/a&gt; から Ansible の template モジュールで生成しています。&lt;/p&gt;

&lt;p&gt;今回の構成では古いデータディレクトリを &lt;code&gt;mv&lt;/code&gt; コマンドでリネームして、 &lt;code&gt;pg_basebackup&lt;/code&gt; コマンドでプライマリ・データベースの複製を作り、 &lt;code&gt;recovery.conf&lt;/code&gt; を作ってスタンバイとして稼働させる準備をしています。&lt;/p&gt;

&lt;p&gt;2つ目は PostgreSQL のデータディレクトリ下の &lt;code&gt;pgpool_remote_start&lt;/code&gt; というファイル名のスクリプトです。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html&#34;&gt;pgpool-II ユーザマニュアル&lt;/a&gt; の &lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html#pool_remote_start&#34;&gt;pgpool_remote_start&lt;/a&gt; に説明があります。&lt;/p&gt;

&lt;p&gt;こちらはファイル名が pgpool-II のソースコード &lt;code&gt;src/sql/pgpool-recovery/pgpool-recovery.c&lt;/code&gt; 内に&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#define REMOTE_START_FILE &amp;quot;pgpool_remote_start&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;のように固定の定義になっています。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;/var/lib/pgsql/9.5/data/pgpool_remote_start&lt;/code&gt; は &lt;a href=&#34;https://github.com/hnakamur/postgresql-pgpool2-failover-example-playbook/blob/7f3ef8af54a4f9ec948d65bfc47e33db4792737d/roles/postgresql_db/templates/pgpool_remote_start.j2&#34;&gt;roles/postgresql_db/templates/pgpool_remote_start.j2&lt;/a&gt; から Ansible の template モジュールで生成しています。&lt;/p&gt;

&lt;p&gt;処理内容はリモートのノードの PostgreSQL を起動するというものになっています。
ファイル名は &lt;code&gt;pgpool_remote_start&lt;/code&gt; なので最初見たときは &lt;code&gt;pgpool&lt;/code&gt; を起動するのかと勘違いしましたが、 &lt;code&gt;pgpool_&lt;/code&gt; は &lt;code&gt;pgpool&lt;/code&gt; のファイルであることを示す接頭辞的な意味合いのようです。
&lt;code&gt;pgpool&lt;/code&gt; を起動するなら &lt;code&gt;start_remote_pgoool&lt;/code&gt; のほうがわかりやすいでしょうしね。&lt;/p&gt;

&lt;p&gt;なお、 &lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html&#34;&gt;pgpool-II ユーザマニュアル&lt;/a&gt; の &lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html#pool_remote_start&#34;&gt;pgpool_remote_start&lt;/a&gt; で書かれているサンプルスクリプトでは &lt;code&gt;pg_ctl&lt;/code&gt; コマンドで PostgreSQL を起動していますが、 &lt;code&gt;systemctl status postgresql-9.5&lt;/code&gt; でサービス状態が確認できなくなってしまうため、 &lt;a href=&#34;https://github.com/hnakamur/postgresql-pgpool2-failover-example-playbook/blob/7f3ef8af54a4f9ec948d65bfc47e33db4792737d/roles/postgresql_db/templates/pgpool_remote_start.j2&#34;&gt;roles/postgresql_db/templates/pgpool_remote_start.j2&lt;/a&gt; では &lt;code&gt;sudo systemctl start postgresql-9.5&lt;/code&gt; で起動しています。
またそのために postgres ユーザ用の sudoers 設定も &lt;a href=&#34;https://github.com/hnakamur/postgresql-pgpool2-failover-example-playbook/blob/7f3ef8af54a4f9ec948d65bfc47e33db4792737d/roles/postgresql/templates/sudoers_postgres.j2&#34;&gt;roles/postgresql/templates/sudoers_postgres.j2&lt;/a&gt; を元に &lt;code&gt;/etc/sudoers.d/01_postgres&lt;/code&gt; を生成し行っています。&lt;/p&gt;

&lt;h3 id=&#34;リカバリの実行&#34;&gt;リカバリの実行&lt;/h3&gt;

&lt;p&gt;今回は &lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html#perform_online_recovery&#34;&gt;リカバリの実行&lt;/a&gt; で説明されている &lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html#pcp_recovery_node&#34;&gt;pcp_recovery_node&lt;/a&gt; コマンドを使います。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;pcp_recovery_node&lt;/code&gt; コマンドは pgpool-II のユーザ &lt;code&gt;pgpool2&lt;/code&gt; のパスワードを入力する必要があるためフォアグラウンドで実行し ( &lt;code&gt;&amp;amp;&lt;/code&gt; は &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt; と2つ)、ログを表示します。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;pcp_recovery_node&lt;/code&gt; コマンドは pgsql1 と pgsql2 のどちらで実行しても良いようです。ここでは対象のサーバが pgsql1 なので対応する &lt;code&gt;node_id&lt;/code&gt; の &lt;code&gt;0&lt;/code&gt; を引数の最後に指定しています。 &lt;code&gt;node_id&lt;/code&gt; は上記の &lt;code&gt;show pool_nodes&lt;/code&gt; の出力で確認できます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 pgsql]# sudo -i -u postgres /usr/pgpool-9.5/bin/pcp_recovery_node -h localhost -p 9898 -U pgpool2 0
Password: 
pcp_recovery_node -- Command Successful
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ログを確認すると以下のようになっていました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 pgsql]# journalctl | grep -E &#39;(recovery_1st_stage|pgpool_remote_start)&#39;
Sep 15 14:03:38 pgsql2 pgpool[1416]: 2016-09-15 14:03:38: pid 1700: DETAIL:  starting recovery command: &amp;quot;SELECT pgpool_recovery(&#39;recovery_1st_stage&#39;, &#39;10.155.92.101&#39;, &#39;/var/lib/pgsql/9.5/data&#39;, &#39;5432&#39;)&amp;quot;
Sep 15 14:03:38 pgsql2 recovery_1st_stage[1704]: start args=/var/lib/pgsql/9.5/data 10.155.92.101 /var/lib/pgsql/9.5/data 5432 UID=26
Sep 15 14:03:39 pgsql2 recovery_1st_stage[1713]: pg_basebackup done.
Sep 15 14:03:39 pgsql2 recovery_1st_stage[1716]: created archive_status.
Sep 15 14:03:39 pgsql2 recovery_1st_stage[1719]: created recovery.conf.
Sep 15 14:03:39 pgsql2 pgpool_remote_start[1722]: start args=10.155.92.101 /var/lib/pgsql/9.5/data UID=26
Sep 15 14:03:42 pgsql2 pgpool_remote_start[1738]: finished to start postgresql service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;しばらくして PostgreSQL の状態を確認すると以下のように pgsql1 の role が standby で status が 2 (ノード稼働中。接続有り) になりました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 pgsql]# sudo -i -u postgres psql -h localhost -p 9999 -c &amp;quot;show pool_nodes&amp;quot;
 node_id |   hostname    | port | status | lb_weight |  role   | select_cnt 
---------+---------------+------+--------+-----------+---------+------------
 0       | 10.155.92.101 | 5432 | 2      | 0.500000  | standby | 0
 1       | 10.155.92.102 | 5432 | 2      | 0.500000  | primary | 0
(2 rows)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ps を実行してみると pgsql2 で &lt;code&gt;postgres: wal sender process&lt;/code&gt; が稼働しており、 pgsql1 で &lt;code&gt;postgres: wal receiver process&lt;/code&gt; が稼働しており、ストリーミング・レプリケーションが動いていることが確認できました。&lt;/p&gt;

&lt;h2 id=&#34;pgsql1-スタンバイ-の-postgresql-を強制停止してみる&#34;&gt;pgsql1 (スタンバイ) の PostgreSQL を強制停止してみる&lt;/h2&gt;

&lt;h3 id=&#34;スタンバイの-postgresql-を強制停止&#34;&gt;スタンバイの PostgreSQL を強制停止&lt;/h3&gt;

&lt;p&gt;pgsql2 で以下のコマンドを実行します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 ~]# journalctl -f | grep pgpool_failover
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pgsql1 で以下のコマンドを実行します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql1 ~]# sudo -i -u postgres /usr/pgsql-9.5/bin/pg_ctl stop -m immediate -D /var/lib/pgsql/9.5/data &amp;amp; journalctl -f | grep pgpool_failover
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;今回は pgsql2 に以下のログが出ました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Sep 15 14:21:05 pgsql2 pgpool[1416]: 2016-09-15 14:21:05: pid 1416: LOG:  execute command: /var/lib/pgsql/9.5/data/pgpool_failover 0 1 10.155.92.102 /var/lib/pgsql/9.5/data
Sep 15 14:21:05 pgsql2 pgpool_failover[2466]: start args=0 1 10.155.92.102 /var/lib/pgsql/9.5/data UID=0
Sep 15 14:21:05 pgsql2 pgpool_failover[2468]: do nothing since failed node was not primary
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;停止された PostgreSQL がプライマリではなかったのでフェイルオーバは行わず、プライマリ (pgsql2) をそのままスタンドアロンで稼働させています。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html&#34;&gt;pgpool-II ユーザマニュアル&lt;/a&gt; の &lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html#failover_in_stream_mode&#34;&gt;Streaming Replicationでのフェイルオーバ&lt;/a&gt; に上げられているフェイルオーバ用スクリプトではノード 0 がプライマリで 1 がスタンバイという想定で書かれているため、今回の状況ではうまく行きませんでした。
が、 &lt;a href=&#34;https://github.com/hnakamur/postgresql-pgpool2-failover-example-playbook/blob/7f3ef8af54a4f9ec948d65bfc47e33db4792737d/roles/postgresql_db/templates/pgpool_failover.j2&#34;&gt;roles/postgresql_db/templates/pgpool_failover.j2&lt;/a&gt; では &lt;code&gt;pgpool.conf&lt;/code&gt; の &lt;code&gt;failover_command&lt;/code&gt; の4つの引数のうち、最初の2つに停止したノード &lt;code&gt;%d&lt;/code&gt; と旧プライマリノード %P を渡していて、値が違う場合はスタンバイと判定していますので、一度フェイルオーバ→リカバリをした後でも大丈夫です。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;show pool_nodes&lt;/code&gt; の実行結果は以下の通りです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 pgsql]# sudo -u postgres psql -h localhost -p 9999 -c &amp;quot;show pool_nodes&amp;quot;
 node_id |   hostname    | port | status | lb_weight |  role   | select_cnt 
---------+---------------+------+--------+-----------+---------+------------
 0       | 10.155.92.101 | 5432 | 3      | 0.500000  | standby | 0
 1       | 10.155.92.102 | 5432 | 2      | 0.500000  | primary | 0
(2 rows)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pgsql2 で postgres のプロセスを見ると &lt;code&gt;wal sender&lt;/code&gt; がいないので、スタンドアロン状態であることがわかります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 pgsql]# ps axf | grep [p]ostgres
 1370 ?        S      0:00 /usr/pgsql-9.5/bin/postgres -D /var/lib/pgsql/9.5/data
 1371 ?        Ss     0:00  \_ postgres: logger process   
 1378 ?        Ss     0:00  \_ postgres: checkpointer process   
 1379 ?        Ss     0:00  \_ postgres: writer process   
 1381 ?        Ss     0:00  \_ postgres: stats collector process   
 1554 ?        Ss     0:00  \_ postgres: wal writer process   
 1555 ?        Ss     0:00  \_ postgres: autovacuum launcher process   
 1556 ?        Ss     0:00  \_ postgres: archiver process   last was 000000020000000000000004.00000060.backup
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;スタンドアロン状態であることは &lt;code&gt;select * from pg_stat_replication&lt;/code&gt; の結果が 0であることからもわかります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 pgsql]# sudo -i -u postgres psql -x -c &amp;quot;select * from pg_stat_replication&amp;quot;
(0 rows)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;スタンバイの-postgresql-を復活させる&#34;&gt;スタンバイの PostgreSQL を復活させる&lt;/h3&gt;

&lt;h4 id=&#34;単に-postgresql-を起動すれば動く場合&#34;&gt;単に PostgreSQL を起動すれば動く場合&lt;/h4&gt;

&lt;p&gt;実運用の際はデータディレクトリの状況を調査したりするところですが、ここでは問題ないという前提で、 pgsql1 で PostgreSQL を起動して pgpool-II の管理下に追加します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql1 pgsql]# systemctl start postgresql-9.5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;で起動し&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql1 pgsql]# systemctl status postgresql-9.5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;で状態を確認します。
Active の行が以下のように &lt;code&gt;active (running)&lt;/code&gt; になっていれば OK です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   Active: active (running) since Thu 2016-09-15 14:36:12 UTC; 3s ago
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;しばらくすると pgpool-II の heartbeat で pgsql1 の PostgreSQL が稼働していることを検知し、 pgsql2 をプライマリとするリプリケーションが再開されます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql1 pgsql]# ps axf | grep [p]ostgres
 2778 ?        S      0:00 /usr/pgsql-9.5/bin/postgres -D /var/lib/pgsql/9.5/data
 2779 ?        Ss     0:00  \_ postgres: logger process   
 2780 ?        Ss     0:00  \_ postgres: startup process   recovering 000000020000000000000005
 2785 ?        Ss     0:00  \_ postgres: checkpointer process   
 2786 ?        Ss     0:00  \_ postgres: writer process   
 2787 ?        Ss     0:00  \_ postgres: stats collector process   
 2788 ?        Ss     0:00  \_ postgres: wal receiver process   streaming 0/5000680
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 pgsql]# ps axf | grep [p]ostgres
 1370 ?        S      0:00 /usr/pgsql-9.5/bin/postgres -D /var/lib/pgsql/9.5/data
 1371 ?        Ss     0:00  \_ postgres: logger process   
 1378 ?        Ss     0:00  \_ postgres: checkpointer process   
 1379 ?        Ss     0:00  \_ postgres: writer process   
 1381 ?        Ss     0:00  \_ postgres: stats collector process   
 1554 ?        Ss     0:00  \_ postgres: wal writer process   
 1555 ?        Ss     0:00  \_ postgres: autovacuum launcher process   
 1556 ?        Ss     0:00  \_ postgres: archiver process   last was 000000020000000000000004.00000060.backup
 3111 ?        Ss     0:00  \_ postgres: wal sender process repl_user 10.155.92.101(57142) streaming 0/5000680
[root@pgsql2 pgsql]# sudo -i -u postgres psql -x -c &amp;quot;select * from pg_stat_replication&amp;quot;
-[ RECORD 1 ]----+------------------------------
pid              | 3111
usesysid         | 16394
usename          | repl_user
application_name | walreceiver
client_addr      | 10.155.92.101
client_hostname  | 
client_port      | 57142
backend_start    | 2016-09-15 14:36:12.312321+00
backend_xmin     | 1842
state            | streaming
sent_location    | 0/5000680
write_location   | 0/5000680
flush_location   | 0/5000680
replay_location  | 0/5000680
sync_priority    | 0
sync_state       | async
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;スタンバイデータベースを作り直す場合&#34;&gt;スタンバイデータベースを作り直す場合&lt;/h4&gt;

&lt;p&gt;スタンバイデータベースの損傷がひどくて起動できない場合は、プライマリデータベースを &lt;code&gt;pg_basebackup&lt;/code&gt; コマンドで複製して作り直し、スタンバイ用の設定を加えて起動することになります。&lt;/p&gt;

&lt;p&gt;これは上記の「リカバリの実行」でやっていることと同じなので、今のプライマリである pgsql2 で以下のコマンドを実行すれば OK です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 pgsql]# sudo -i -u postgres /usr/pgpool-9.5/bin/pcp_recovery_node -h localhost -p 9898 -U pgpool2 0
Password: 
pcp_recovery_node -- Command Successful
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;

&lt;p&gt;さらに pgpool-II が落ちたときやサーバ全体が落ちたときなども検証が必要ですがこのブログ記事を書くのに、今日の早朝と今でなんだかんだで4時間ぐらいはかかっているので (環境構築と動作検証には3日かかってます)、一旦ここまでとします。&lt;/p&gt;

&lt;p&gt;pgpool-II でのフェイルオーバ、リカバリは呼び出されるスクリプトをどうすれば良いのかが最初は全くわからなくて苦労しましたが、ドキュメントとソースを読んで理解できたので良かったです。&lt;/p&gt;

&lt;p&gt;Pacemaker は高機能なんですが複雑すぎるように私には思えたので、 pgpool-II のほうが好感触です。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pacemakerを使ってPostgreSQLのアクティブ・スタンバイ(1&#43;1構成)を試してみた</title>
      <link>https://hnakamur.github.io/blog/2016/08/21/experiment-postgresql-active-standby-cluster-using-pacemaker/</link>
      <pubDate>Sun, 21 Aug 2016 11:23:01 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2016/08/21/experiment-postgresql-active-standby-cluster-using-pacemaker/</guid>
      <description>

&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;

&lt;p&gt;STONITH無し、quorum無しのアクティブ・スタンバイ(1+1構成)がとりあえず動くところまでは来たので、一旦メモです。&lt;/p&gt;

&lt;h2 id=&#34;参考資料&#34;&gt;参考資料&lt;/h2&gt;

&lt;p&gt;以下の資料と連載記事がわかりやすくて非常に参考になりました。ありがとうございます！&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://linux-ha.osdn.jp/wp/archives/3244&#34;&gt;JPUG 第23回しくみ+アプリケーション勉強会 セミナー資料公開 « Linux-HA Japan&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://linux-ha.osdn.jp/wp/wp-content/uploads/pacemaker_20120526JPUG.pdf&#34;&gt;HAクラスタでPostgreSQLを高可用化(前編) ～Pacemaker入門編～(PDF)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://linux-ha.osdn.jp/wp/wp-content/uploads/b754c737d835c2546415009387407b7b.pdf&#34;&gt;PostgreSQLを高可用化(後編) 〜レプリケーション編〜(PDF)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://linux-ha.osdn.jp/wp/archives/3589&#34;&gt;OSC 2013 Tokyo/Spring 講演資料公開 « Linux-HA Japan&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.slideshare.net/takmatsuo/osc-tokyospring2013-16694861&#34;&gt;Pacemaker+PostgreSQLレプリケーションで共有ディスクレス高信頼クラスタの構築＠OSC 2013 Tokyo/Spring&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://gihyo.jp/admin/serial/01/pacemaker&#34;&gt;Pacemakerでかんたんクラスタリング体験してみよう！：連載｜gihyo.jp … 技術評論社&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;さらに以下の記事と電子書籍も参考にしました。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://clusterlabs.org/wiki/PgSQL_Replicated_Cluster&#34;&gt;PgSQL Replicated Cluster - ClusterLabs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://shop.oreilly.com/product/9781783550609.do&#34;&gt;PostgreSQL Replication, 2nd Edition - O&amp;rsquo;Reilly Media&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;テスト用のansible-playbook&#34;&gt;テスト用のAnsible playbook&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/hnakamur/postgresql-pacemaker-example-playbook&#34;&gt;https://github.com/hnakamur/postgresql-pacemaker-example-playbook&lt;/a&gt;
に置きました。&lt;/p&gt;

&lt;p&gt;LXD をセットアップ済みの Ubuntu 16.04 上で試しました。&lt;/p&gt;

&lt;h2 id=&#34;セットアップの事前準備&#34;&gt;セットアップの事前準備&lt;/h2&gt;

&lt;p&gt;上記のplaybookを取得します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/hnakamur/postgresql-pacemaker-example-playbook
cd postgresql-pacemaker-example-playbook
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ansibleの &lt;code&gt;lxd_container&lt;/code&gt; モジュールを使うので、virtualenvで仮想環境を作ってAnsibleのmaster版をインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;virtualenv venv
source venv/bin/activate
pip install git+https://github.com/ansible/ansible
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;今回はコンテナのIPアドレスをDHCPではなく静的アドレスを使うようにしてみました。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;/etc/default/lxd-bridge&lt;/code&gt; の &lt;code&gt;LXD_IPV4_DHCP_RANGE&lt;/code&gt; に DHCP のアドレス範囲が設定されているので、ファイルを編集して範囲を狭めます。私の環境では以下のようにしました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## IPv4 network (e.g. 10.0.8.0/24)
LXD_IPV4_NETWORK=&amp;quot;10.155.92.1/24&amp;quot;

## IPv4 DHCP range (e.g. 10.0.8.2,10.0.8.254)
LXD_IPV4_DHCP_RANGE=&amp;quot;10.155.92.200,10.155.92.254&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;LXDをインストールしたときに &lt;code&gt;LXD_IPV4_NETWORK&lt;/code&gt; はランダムなアドレスになるかあるいは自分で指定しますので、それに応じた値に適宜変更してください。&lt;/p&gt;

&lt;p&gt;変更したら &lt;code&gt;lxd-bridge&lt;/code&gt; を再起動して変更を反映します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl restart lxd-bridge
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;group_vars/development/vars.yml&lt;/code&gt; ファイル内のIPアドレスも適宜変更します。&lt;/p&gt;

&lt;p&gt;また、 &lt;code&gt;group_vars/development/secrets.yml&lt;/code&gt; 内にパスワードやsshの鍵ペアなどが含まれています。これを違う値に変更したい場合は以下のようにします。&lt;/p&gt;

&lt;p&gt;まず、以下のコマンドを実行して一旦復号化します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ansible-vault decrypt group_vars/development/secrets.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;vaultのパスワードを聞かれますので入力します。この例では &lt;code&gt;password&lt;/code&gt; としています。これはあくまで例なのでこういう弱いパスワードにしていますが、実際の案件で使うときは、もっと強いパスワードを指定してください。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;group_vars/development/secrets.yml&lt;/code&gt; 内の変数を適宜変更したら、以下のコマンドを実行して暗号化します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ansible-vault encrypt group_vars/development/secrets.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;vaultの新しいパスワードを聞かれますので入力してください。&lt;/p&gt;

&lt;h2 id=&#34;コンテナの作成&#34;&gt;コンテナの作成&lt;/h2&gt;

&lt;p&gt;以下のコマンドを実行して &lt;code&gt;node1&lt;/code&gt; と &lt;code&gt;node2&lt;/code&gt; という2つのコンテナを作成します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ansible-playbook launch_containers.yml -D -v
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;vaultのパスワードを聞かれますので入力してください。&lt;/p&gt;

&lt;h2 id=&#34;コンテナ内にpostgresqlとpacemakerをセットアップ&#34;&gt;コンテナ内にPostgreSQLとPacemakerをセットアップ&lt;/h2&gt;

&lt;p&gt;以下のコマンドを実行して、コンテナ内にPostgreSQLとPacemakerをセットアップします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ansible-playbook setup_containers.yml -D -v
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ここでは、セットアップ完了後、アクティブスタンバイ構成が開始するまでの時間を図りたいので、以下のように &lt;code&gt;date -u&lt;/code&gt; コマンドも実行するようにします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ansible-playbook setup_containers.yml -D -v; date -u
…(略)…
Sun Aug 21 13:51:21 UTC 2016
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下のコマンドを実行して &lt;code&gt;node2&lt;/code&gt; コンテナに入ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ lxc exec node2 bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下のコマンドを実行して、クラスタの状態をモニターします。
&lt;code&gt;node1&lt;/code&gt;, &lt;code&gt;node2&lt;/code&gt; が両方 Slaves の状態を経て、 &lt;code&gt;node1&lt;/code&gt; が Master になり master-ip が &lt;code&gt;node1&lt;/code&gt; につくまで待ちます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Last updated: Sun Aug 21 13:52:07 2016          Last change: Sun Aug 21 13:52:03 2016 by root via crm_attribute on node1
Stack: corosync
Current DC: node1 (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum
2 nodes and 3 resources configured

Online: [ node1 node2 ]

 Master/Slave Set: pgsql-master [pgsql]
     Masters: [ node1 ]
     Slaves: [ node2 ]
master-ip       (ocf::heartbeat:IPaddr2):       Started node1

Node Attributes:
* Node node1:
    + master-pgsql                      : 1000
    + pgsql-data-status                 : LATEST
    + pgsql-master-baseline             : 0000000003000098
    + pgsql-status                      : PRI
    + pgsql-xlog-loc                    : 0000000003000098
* Node node2:
    + master-pgsql                      : -INFINITY
    + pgsql-data-status                 : STREAMING|ASYNC
    + pgsql-status                      : HS:async
    + pgsql-xlog-loc                    : 0000000003000000

Migration Summary:
* Node node2:
* Node node1:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;この端末は開いたままにしておきます。&lt;/p&gt;

&lt;h2 id=&#34;node1-コンテナを強制停止してフェールオーバのテスト&#34;&gt;node1 コンテナを強制停止してフェールオーバのテスト&lt;/h2&gt;

&lt;p&gt;別の端末を開いて以下のコマンドを実行し、 &lt;code&gt;node1&lt;/code&gt; コンテナを強制停止し時刻を記録します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ lxc stop -f node1; date -u
Sun Aug 21 13:52:57 UTC 2016
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;しばらくすると　&lt;code&gt;crm_mon -fA&lt;/code&gt; の出力が以下のようになります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Last updated: Sun Aug 21 13:53:11 2016          Last change: Sun Aug 21 13:53:05 2016 by root via crm_attribute on node2
Stack: corosync
Current DC: node2 (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum
2 nodes and 3 resources configured

Online: [ node2 ]
OFFLINE: [ node1 ]

 Master/Slave Set: pgsql-master [pgsql]
     Masters: [ node2 ]
master-ip       (ocf::heartbeat:IPaddr2):       Started node2

Node Attributes:
* Node node2:
    + master-pgsql                      : 1000
    + pgsql-data-status                 : LATEST
    + pgsql-master-baseline             : 00000000030001A8
    + pgsql-status                      : PRI
    + pgsql-xlog-loc                    : 0000000003000000

Migration Summary:
* Node node2:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;LXDホストで以下のコマンドを実行して &lt;code&gt;node1&lt;/code&gt; を起動します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ lxc start node1; date -u
Sun Aug 21 13:53:58 UTC 2016
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;起動後しばらくしても &lt;code&gt;node1&lt;/code&gt; はオフラインのままですが、これは意図した挙動です。実際のケースではディスク障害などが起きているかもしれないので、マシンの状況を確認してから手動でクラスタに復帰させることになるためです。&lt;/p&gt;

&lt;p&gt;以下のコマンドで &lt;code&gt;node1&lt;/code&gt; コンテナに入ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ lxc exec node1 bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;PacemakerがPostgreSQLのロックファイルを作っているのでそれを削除します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ll /var/run/postgresql/
total 4
-rw-r----- 1 root     root      0 Aug 21 13:52 PGSQL.lock
-rw-r----- 1 postgres postgres 36 Aug 21 13:52 rep_mode.conf
[root@node1 ~]# rm /var/run/postgresql/PGSQL.lock
rm: remove regular empty file &#39;/var/run/postgresql/PGSQL.lock&#39;? y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下のコマンドで &lt;code&gt;node1&lt;/code&gt; をクラスタに復帰させ、時刻を記録します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@node1 ~]# pcs cluster start node1; date -u
node1: Starting Cluster...
Sun Aug 21 13:55:30 UTC 2016
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;15秒後、 &lt;code&gt;crm_mon -fA&lt;/code&gt; の画面で &lt;code&gt;node1&lt;/code&gt; の PostgreSQL が Slaves に追加されました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Last updated: Sun Aug 21 13:55:45 2016          Last change: Sun Aug 21 13:55:42 2016 by root via crm_attribute on node2
Stack: corosync
Current DC: node2 (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum
2 nodes and 3 resources configured

Online: [ node1 node2 ]

 Master/Slave Set: pgsql-master [pgsql]
     Masters: [ node2 ]
     Slaves: [ node1 ]
master-ip       (ocf::heartbeat:IPaddr2):       Started node2

Node Attributes:
* Node node1:
    + master-pgsql                      : 100
    + pgsql-data-status                 : STREAMING|SYNC
    + pgsql-status                      : HS:sync
* Node node2:
    + master-pgsql                      : 1000
    + pgsql-data-status                 : LATEST
    + pgsql-master-baseline             : 00000000030001A8
    + pgsql-status                      : PRI
    + pgsql-xlog-loc                    : 0000000003000000

Migration Summary:
* Node node2:
* Node node1:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ここで、 &lt;code&gt;node2&lt;/code&gt; で &lt;code&gt;crm_mon -fA&lt;/code&gt; を実行していた端末で Control-C を入力してモニターを終了します。&lt;/p&gt;

&lt;h2 id=&#34;postgresqlのプロセスを強制終了してフェールオーバのテスト&#34;&gt;PostgreSQLのプロセスを強制終了してフェールオーバのテスト&lt;/h2&gt;

&lt;p&gt;今度は &lt;code&gt;node2&lt;/code&gt; の PostgreSQL のプロセスを強制終了してフェールオーバしてみます。&lt;/p&gt;

&lt;p&gt;経過を見るために &lt;code&gt;node1&lt;/code&gt; で以下のコマンドを実行して、その端末を開いたままにしておきます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@node1 ~]# crm_mon -fA
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;開始時点では以下のような出力になっていました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Last updated: Sun Aug 21 13:57:17 2016          Last change: Sun Aug 21 13:55:42 2016 by root via crm_attribute on node2
Stack: corosync
Current DC: node2 (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum
2 nodes and 3 resources configured

Online: [ node1 node2 ]

 Master/Slave Set: pgsql-master [pgsql]
     Masters: [ node2 ]
     Slaves: [ node1 ]
master-ip       (ocf::heartbeat:IPaddr2):       Started node2

Node Attributes:
* Node node1:
    + master-pgsql                      : 100
    + pgsql-data-status                 : STREAMING|SYNC
    + pgsql-status                      : HS:sync
* Node node2:
    + master-pgsql                      : 1000
    + pgsql-data-status                 : LATEST
    + pgsql-master-baseline             : 00000000030001A8
    + pgsql-status                      : PRI
    + pgsql-xlog-loc                    : 0000000003000000

Migration Summary:
* Node node2:
* Node node1:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;node2&lt;/code&gt; で以下のコマンドを実行して PostgreSQL のプロセスを強制終了し、時刻を記録します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@node2 ~]# kill -KILL `head -1 /var/lib/pgsql/9.5/data/postmaster.pid`; date -u
Sun Aug 21 13:58:20 UTC 2016
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;11秒後 &lt;code&gt;node1&lt;/code&gt; の PostgreSQL が Masterに昇格されました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Last updated: Sun Aug 21 13:58:31 2016          Last change: Sun Aug 21 13:58:27 2016 by root via crm_attribute on node1
Stack: corosync
Current DC: node2 (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum
2 nodes and 3 resources configured

Online: [ node1 node2 ]

 Master/Slave Set: pgsql-master [pgsql]
     Masters: [ node1 ]
master-ip       (ocf::heartbeat:IPaddr2):       Started node1

Node Attributes:
* Node node1:
    + master-pgsql                      : 1000
    + pgsql-data-status                 : LATEST
    + pgsql-master-baseline             : 0000000003000398
    + pgsql-status                      : PRI
* Node node2:
    + master-pgsql                      : -INFINITY
    + pgsql-data-status                 : DISCONNECT
    + pgsql-status                      : STOP

Migration Summary:
* Node node2:
   pgsql: migration-threshold=2 fail-count=1000000 last-failure=&#39;Sun Aug 21 13:58:23 2016&#39;
* Node node1:

Failed Actions:
* pgsql_start_0 on node2 &#39;unknown error&#39; (1): call=23, status=complete, exitreason=&#39;My data may be inconsistent. You have to remove /va
r/run/postgresql/PGSQL.lock file to force start.&#39;,
    last-rc-change=&#39;Sun Aug 21 13:58:23 2016&#39;, queued=0ms, exec=383ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;次に、 &lt;code&gt;node2&lt;/code&gt; の PostgreSQL を再び稼働してスタンバイにさせてみます。&lt;/p&gt;

&lt;p&gt;まず Pacemaker が作成した PostgreSQL のロックファイル &lt;code&gt;/var/run/postgresql/PGSQL.lock&lt;/code&gt; を削除します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@node2 ~]# ll /var/run/postgresql/
total 4
-rw-r----- 1 root     root      0 Aug 21 13:53 PGSQL.lock
-rw-r----- 1 postgres postgres 31 Aug 21 13:58 rep_mode.conf
[root@node2 ~]# \rm /var/run/postgresql/PGSQL.lock
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;次に以下のコマンドを実行して &lt;code&gt;node2&lt;/code&gt; のPostgreSQL の failcount をリセットし、時刻を記録します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@node2 ~]# pcs resource failcount reset pgsql node2; date -u
Sun Aug 21 14:00:04 UTC 2016
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;9秒後、 &lt;code&gt;node1&lt;/code&gt; での &lt;code&gt;crm_mon -fA&lt;/code&gt; の出力を見ると &lt;code&gt;node2&lt;/code&gt; がスタンバイになりました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Last updated: Sun Aug 21 14:00:13 2016          Last change: Sun Aug 21 14:00:10 2016 by root via crm_attribute on node1
Stack: corosync
Current DC: node2 (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum
2 nodes and 3 resources configured

Online: [ node1 node2 ]

 Master/Slave Set: pgsql-master [pgsql]
     Masters: [ node1 ]
     Slaves: [ node2 ]
master-ip       (ocf::heartbeat:IPaddr2):       Started node1

Node Attributes:
* Node node1:
    + master-pgsql                      : 1000
    + pgsql-data-status                 : LATEST
    + pgsql-master-baseline             : 0000000003000398
    + pgsql-status                      : PRI
* Node node2:
    + master-pgsql                      : 100
    + pgsql-data-status                 : STREAMING|SYNC
    + pgsql-status                      : HS:sync

Migration Summary:
* Node node2:
* Node node1:

Failed Actions:
* pgsql_start_0 on node2 &#39;unknown error&#39; (1): call=23, status=complete, exitreason=&#39;My data may be inconsistent. You have to remove /va
r/run/postgresql/PGSQL.lock file to force start.&#39;,
    last-rc-change=&#39;Sun Aug 21 13:58:23 2016&#39;, queued=0ms, exec=383ms
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;

&lt;p&gt;STONITH無し、quorum無しという簡易構成ですが、アクティブ・スタンバイ(1+1構成)でフフェールオーバする検証ができました。本番運用するにはSTONITHやquorumも重要そうなので、そちらも調べて行きたいです。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LXDコンテナ上でPacemakerを使って仮想IPとApacheのアクティブ・パッシブ・クラスタを試してみた</title>
      <link>https://hnakamur.github.io/blog/2016/08/12/experiment-vip-and-apache-with-pacemaker-on-lxd-containers/</link>
      <pubDate>Fri, 12 Aug 2016 18:54:27 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2016/08/12/experiment-vip-and-apache-with-pacemaker-on-lxd-containers/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://clusterlabs.org/doc/&#34;&gt;Cluster Labs - Pacemaker Documentation&lt;/a&gt; の &amp;ldquo;Pacemaker 1.1 for Corosync 2.x and pcs&amp;rdquo; の &amp;ldquo;Clusters from Scratch (en-US)&amp;rdquo; を参考にしつつ、多少手順を変更して試してみました。&lt;/p&gt;

&lt;h2 id=&#34;実験用コンテナの環境構築&#34;&gt;実験用コンテナの環境構築&lt;/h2&gt;

&lt;h3 id=&#34;コンテナの作成&#34;&gt;コンテナの作成&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://hnakamur.github.io/blog/blog/2016/08/12/update-lxd-dnsmasq-dhcp-hosts-config-with-sighup/&#34;&gt;LXDのdnsmasqの固定IP設定をSIGHUPで更新する · hnakamur&amp;rsquo;s blog at github&lt;/a&gt; の手法を使って、2つのコンテナ用のIPアドレスを設定しておきます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lxdhost:~$ cat /var/lib/lxd-bridge/dhcp-hosts 
pcmk-1,10.155.92.101
pcmk-2,10.155.92.102
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;また、仮想IPとして &lt;code&gt;10.155.92.100&lt;/code&gt; を使用しますので、 &lt;code&gt;/var/lib/lxd-bridge/dnsmasq.lxdbr0.leases&lt;/code&gt; で使われていないことを確認しておきます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo kill -HUP `cat /var/run/lxd-bridge/dnsmasq.pid`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;で設定を dnsmasq に反映します。&lt;/p&gt;

&lt;p&gt;なお、  &lt;a href=&#34;http://clusterlabs.org/doc/en-US/Pacemaker/1.1-pcs/html/Clusters_from_Scratch/ch02.html#_configure_network&#34;&gt;2.1.3. Configure Network&lt;/a&gt; の &amp;ldquo;Important&amp;rdquo; の囲み部分によるとDHCPはcorosyncと干渉するので、 &lt;strong&gt;クラスタのマシンはDHCPを決して使うべきではない&lt;/strong&gt; そうです。この記事はあくまでPacemakerの使い方を把握するために試してみるだけなので気にしないことにしますが、実運用の際には DHCP を使わない構成にする必要があります。&lt;/p&gt;

&lt;p&gt;以下のコマンドでコンテナ &lt;code&gt;pcmk-1&lt;/code&gt; と &lt;code&gt;pcmk-2&lt;/code&gt; を作成します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lxdhost:~$ lxc launch images:centos/7/amd64 pcmk-1
lxdhost:~$ lxc launch images:centos/7/amd64 pcmk-2
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;コンテナ内の-etc-hosts-設定&#34;&gt;コンテナ内の /etc/hosts 設定&lt;/h3&gt;

&lt;p&gt;端末を2つ開いて &lt;code&gt;lxc exec pcmk-1 bash&lt;/code&gt; と &lt;code&gt;lxc exec pcmk-2 bash&lt;/code&gt; を実行し、それぞれ環境構築していきます。&lt;/p&gt;

&lt;p&gt;まず、コンテナ作成直後の &lt;code&gt;pcmk-1&lt;/code&gt; の &lt;code&gt;/etc/hosts&lt;/code&gt; を確認すると以下のようになっていました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;127.0.0.1   localhost
127.0.1.1   pcmk-1

# The following lines are desirable for IPv6 capable hosts
::1     ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当初 &lt;code&gt;pcmk-1&lt;/code&gt; ではIPv4の部分を&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;127.0.0.1   localhost
127.0.1.1   pcmk-1
10.155.92.102 pcmk-2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;と変更し、 &lt;code&gt;pcmk-2&lt;/code&gt; では&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;127.0.0.1   localhost
127.0.1.1   pcmk-2
10.155.92.101 pcmk-1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;と変更してみたのですが、Pacemakerがうまく動かなかったようです（要追試）。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;pcmk-1&lt;/code&gt; と &lt;code&gt;pcmk-2&lt;/code&gt; で &lt;code&gt;/etc/hosts&lt;/code&gt; の IPv4 部分を以下のコマンドで変更したら、うまくいったので、とりあえずこれで試しました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# cat &amp;gt; /etc/hosts &amp;lt;&amp;lt;&#39;EOF&#39;
127.0.0.1   localhost

10.155.92.101   pcmk-1
10.155.92.102   pcmk-2

# The following lines are desirable for IPv6 capable hosts
::1     ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;pacemakerのインストール&#34;&gt;Pacemakerのインストール&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;pcmk-1&lt;/code&gt; と &lt;code&gt;pcmk-2&lt;/code&gt; で以下のコマンドを実行します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# yum -y update
# yum -y install pacemaker pcs psmisc policycoreutils-python which
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;whichは仮想IPを使うための ocf:heartbeat:IPaddr2 のリソース用の resource agent スクリプト &lt;code&gt;/usr/lib/ocf/resource.d/heartbeat/IPaddr2&lt;/code&gt; で必要となります。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;pcsd&lt;/code&gt; サービスを起動し、OS起動時に自動起動するようにします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# systemctl start pcsd
# systemctl enable pcsd
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;クラスタを作成して仮想ipの作成-移動実験&#34;&gt;クラスタを作成して仮想IPの作成・移動実験&lt;/h2&gt;

&lt;h3 id=&#34;クラスタの作成と開始&#34;&gt;クラスタの作成と開始&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;hacluster&lt;/code&gt; のパスワードを設定します。ここでは &lt;code&gt;password&lt;/code&gt; という値にしていますが適宜変更してください。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# echo password | passwd --stdin hacluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ここから先は &lt;code&gt;pcmk-1&lt;/code&gt; だけでコマンドを実行します。 &lt;code&gt;-p&lt;/code&gt; の値は上で設定したパスワードに合わせてください。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# pcs cluster auth pcmk-1 pcmk-2 -u hacluster -p password
# pcs cluster setup --name mycluster pcmk-1 pcmk-2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;この時点で &lt;code&gt;/etc/corosync/corosync.conf&lt;/code&gt; が作られます。&lt;/p&gt;

&lt;p&gt;以下のコマンドでクラスタを開始します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# pcs cluster start --all
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;起動してすぐにステータスを確認すると Node の行が UNCLEAN (offline) になりました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-1 ~]# pcs status
Cluster name: mycluster
WARNING: no stonith devices and stonith-enabled is not false
Last updated: Thu Aug 11 15:55:17 2016          Last change: Thu Aug 11 15:55:16 2016 by hacluster via crmd on pcmk-1
Stack: unknown
Current DC: NONE
2 nodes and 0 resources configured

Node pcmk-1: UNCLEAN (offline)
Node pcmk-2: UNCLEAN (offline)

Full list of resources:


PCSD Status:
  pcmk-1: Online
  pcmk-2: Online

Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;しばらくしてから再度ステータスを確認すると pcmk-1 も pcmk-2 も Online になっていました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# pcs status
Cluster name: mycluster
WARNING: no stonith devices and stonith-enabled is not false
Last updated: Thu Aug 11 15:56:41 2016          Last change: Thu Aug 11 15:55:37 2016 by hacluster via crmd on pcmk-2
Stack: corosync
Current DC: pcmk-2 (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum
2 nodes and 0 resources configured

Online: [ pcmk-1 pcmk-2 ]

Full list of resources:


PCSD Status:
  pcmk-1: Online
  pcmk-2: Online

Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;stonithを無効化&#34;&gt;STONITHを無効化&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://clusterlabs.org/doc/en-US/Pacemaker/1.1-pcs/html/Clusters_from_Scratch/ch05.html&#34;&gt;Chapter 5. Create an Active/Passive Cluster&lt;/a&gt;の手順でクラスタの設定エラーを確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# crm_verify -L -V
   error: unpack_resources:     Resource start-up disabled since no STONITH resources have been defined
   error: unpack_resources:     Either configure some or disable STONITH with the stonith-enabled option
   error: unpack_resources:     NOTE: Clusters with shared data need STONITH to ensure data integrity
Errors found during check: config not valid
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ここでは簡単に Pacemaker を試すために STONITH を無効にします。無効にしたあと設定エラーを再度確認すると、今度はエラーが無くなりました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# pcs property set stonith-enabled=false
# crm_verify -L -V
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;なお、 &lt;a href=&#34;http://clusterlabs.org/doc/en-US/Pacemaker/1.1-pcs/html/Clusters_from_Scratch/ch05.html&#34;&gt;Chapter 5. Create an Active/Passive Cluster&lt;/a&gt; の最後の Warning にもある通り、 &lt;strong&gt;実運用では STONITH を無効にするのは全く不適切&lt;/strong&gt; とのことなので、きちんと設定する必要があります。 STONITH についての説明は上記の Warning の囲み内からもリンクされている &lt;a href=&#34;http://clusterlabs.org/doc/en-US/Pacemaker/1.1-pcs/html/Clusters_from_Scratch/ch08.html#_what_is_stonith&#34;&gt;Chapter 8. Configure STONITH&lt;/a&gt; を参照してください。&lt;/p&gt;

&lt;h3 id=&#34;仮想ipアドレス用のリソース作成&#34;&gt;仮想IPアドレス用のリソース作成&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;pcmk-1&lt;/code&gt; で以下のコマンドを実行して、仮想IPアドレス &lt;code&gt;10.155.92.100&lt;/code&gt; 用のリソースを作成します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# pcs resource create ClusterIP ocf:heartbeat:IPaddr2 \
    ip=10.155.92.100 cidr_netmask=32 op monitor interval=30s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;数秒してから状態を確認してみます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# pcs status
Cluster name: mycluster
Last updated: Thu Aug 11 16:04:50 2016          Last change: Thu Aug 11 16:04:47 2016 by root via cibadmin on pcmk-1
Stack: corosync
Current DC: pcmk-2 (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum
2 nodes and 1 resource configured

Online: [ pcmk-1 pcmk-2 ]

Full list of resources:

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-1

PCSD Status:
  pcmk-1: Online
  pcmk-2: Online

Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;# pcs resource --full
 Resource: ClusterIP (class=ocf provider=heartbeat type=IPaddr2)
  Attributes: ip=10.155.92.100 cidr_netmask=32 
  Operations: start interval=0s timeout=20s (ClusterIP-start-interval-0s)
              stop interval=0s timeout=20s (ClusterIP-stop-interval-0s)
              monitor interval=30s (ClusterIP-monitor-interval-30s)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;ip&lt;/code&gt; コマンドを実行して、仮想IPアドレスが &lt;code&gt;pcmk-1&lt;/code&gt; 側についており &lt;code&gt;pcmk-2&lt;/code&gt; 側にはついていないことを確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-1 ~]# ip a s eth0
120: eth0@if121: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP qlen 1000
    link/ether 00:16:3e:e6:fb:ab brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.155.92.101/24 brd 10.155.92.255 scope global dynamic eth0
       valid_lft 3203sec preferred_lft 3203sec
    inet 10.155.92.100/32 brd 10.155.92.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::216:3eff:fee6:fbab/64 scope link 
       valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-2 ~]# ip a s eth0
122: eth0@if123: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP qlen 1000
    link/ether 00:16:3e:4b:6d:b1 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.155.92.102/24 brd 10.155.92.255 scope global dynamic eth0
       valid_lft 3560sec preferred_lft 3560sec
    inet6 fe80::216:3eff:fe4b:6db1/64 scope link 
       valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;pcmk-1-をクラスタから離脱させて仮想ipアドレスが-pcmk-2-に移動するか確認&#34;&gt;&lt;code&gt;pcmk-1&lt;/code&gt; をクラスタから離脱させて仮想IPアドレスが &lt;code&gt;pcmk-2&lt;/code&gt; に移動するか確認&lt;/h3&gt;

&lt;p&gt;以下のコマンドで &lt;code&gt;pcmk-1&lt;/code&gt; をクラスタから離脱させます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-1 ~]# pcs cluster stop pcmk-1
pcmk-1: Stopping Cluster (pacemaker)...
pcmk-1: Stopping Cluster (corosync)...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;pcmk-1&lt;/code&gt; で状態を確認すると以下のようになります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-1 ~]# pcs status
Error: cluster is not currently running on this node
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;pcmk-2&lt;/code&gt; で状態を確認すると以下のようになります。 &lt;code&gt;pcmk-1&lt;/code&gt; は &lt;code&gt;OFFLINE&lt;/code&gt; となっていますが、 &lt;code&gt;pcsd&lt;/code&gt; は動いているので &lt;code&gt;PCSD Status&lt;/code&gt; のほうは &lt;code&gt;Online&lt;/code&gt; のままです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-2 ~]# pcs status
Cluster name: mycluster
Last updated: Thu Aug 11 16:10:04 2016          Last change: Thu Aug 11 16:04:47 2016 by root via cibadmin on pcmk-1
Stack: corosync
Current DC: pcmk-2 (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum
2 nodes and 1 resource configured

Online: [ pcmk-2 ]
OFFLINE: [ pcmk-1 ]

Full list of resources:

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-2

PCSD Status:
  pcmk-1: Online
  pcmk-2: Online

Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;ip&lt;/code&gt; コマンドを実行して、仮想IPアドレスが &lt;code&gt;pcmk-2&lt;/code&gt; 側についており &lt;code&gt;pcmk-1&lt;/code&gt; 側にはついていないことを確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-1 ~]# ip a s eth0
120: eth0@if121: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP qlen 1000
    link/ether 00:16:3e:e6:fb:ab brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.155.92.101/24 brd 10.155.92.255 scope global dynamic eth0
       valid_lft 3024sec preferred_lft 3024sec
    inet6 fe80::216:3eff:fee6:fbab/64 scope link 
       valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-2 ~]# ip a s eth0
122: eth0@if123: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP qlen 1000
    link/ether 00:16:3e:4b:6d:b1 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.155.92.102/24 brd 10.155.92.255 scope global dynamic eth0
       valid_lft 3385sec preferred_lft 3385sec
    inet 10.155.92.100/32 brd 10.155.92.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::216:3eff:fe4b:6db1/64 scope link 
       valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;pcmk-1-をクラスタに復帰させる&#34;&gt;&lt;code&gt;pcmk-1&lt;/code&gt; をクラスタに復帰させる&lt;/h3&gt;

&lt;p&gt;以下のコマンドで &lt;code&gt;pcmk-1&lt;/code&gt; をクラスタに復帰させます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-1 ~]# pcs cluster start pcmk-1
pcmk-1: Starting Cluster...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;状態を確認してみると、仮想IP は &lt;code&gt;pcmk-2&lt;/code&gt; のほうについたままです。
&lt;a href=&#34;http://clusterlabs.org/doc/en-US/Pacemaker/1.1-pcs/html/Clusters_from_Scratch/_perform_a_failover.html&#34;&gt;5.3. Perform a Failover&lt;/a&gt; の最後の Note によると Pacemakerのより古いバージョンでは &lt;code&gt;pcmk-1&lt;/code&gt; のほうに切り替わっていたそうですが、挙動が変更されたとのことです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-1 ~]# pcs status
Cluster name: mycluster
Last updated: Thu Aug 11 16:12:35 2016          Last change: Thu Aug 11 16:04:47 2016 by root via cibadmin on pcmk-1
Stack: corosync
Current DC: pcmk-2 (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum
2 nodes and 1 resource configured

Online: [ pcmk-1 pcmk-2 ]

Full list of resources:

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-2

PCSD Status:
  pcmk-1: Online
  pcmk-2: Online

Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下のコマンドを実行して、仮想IPを &lt;code&gt;pcmk-1&lt;/code&gt; のほうに移動します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pcs cluster stop pcmk-2 &amp;amp;&amp;amp; pcs cluster start pcmk-2
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;apacheのactive-passiveクラスタを作って仮想ipと連動させる&#34;&gt;ApacheのActive/Passiveクラスタを作って仮想IPと連動させる&lt;/h2&gt;

&lt;h3 id=&#34;リソースのスティッキネスのデフォルト値を設定&#34;&gt;リソースのスティッキネスのデフォルト値を設定&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://clusterlabs.org/doc/en-US/Pacemaker/1.1-pcs/html/Clusters_from_Scratch/_prevent_resources_from_moving_after_recovery.html&#34;&gt;5.4. Prevent Resources from Moving after Recovery&lt;/a&gt; を見て設定します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-1 ~]# pcs resource defaults resource-stickiness=100
[root@pcmk-1 ~]# pcs resource defaults 
resource-stickiness: 100
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;apache-のインストールと設定&#34;&gt;Apache のインストールと設定&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;pcmk-1&lt;/code&gt; と &lt;code&gt;pcmk-2&lt;/code&gt; で以下のコマンドを実行します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# yum -y install httpd wget
# mkdir -p /var/www/html /var/log/httpd
# cat &amp;gt; /var/www/html/index.html &amp;lt;&amp;lt;EOF
&amp;lt;html&amp;gt;
&amp;lt;body&amp;gt;My Test Site - $(hostname)&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
EOF
# cat &amp;gt; /etc/httpd/conf.d/status.conf &amp;lt;&amp;lt;&#39;EOF&#39;
&amp;lt;Location /server-status&amp;gt;
  SetHandler server-status
  Require ip 127.0.0.1
&amp;lt;/Location&amp;gt;
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;apache-の-active-passive-クラスタ作成&#34;&gt;Apache の Active/Passive クラスタ作成&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;pcmk-1&lt;/code&gt; か &lt;code&gt;pcmk-2&lt;/code&gt; のどちらか一方で以下のコマンドを実行します。
公式のドキュメントでは、 &lt;code&gt;pcmk-2&lt;/code&gt; で開始した後、制約を追加しただけでは &lt;code&gt;pcmk-1&lt;/code&gt; に移動しないというデモをしていますが、ここでは &lt;code&gt;--disabled&lt;/code&gt; つきでリソースを作成後、制約を追加してから有効化することで最初から &lt;code&gt;pcmk-1&lt;/code&gt; で開始させています。&lt;/p&gt;

&lt;p&gt;また、制約を追加するごとに &lt;code&gt;crm_simulate -sL&lt;/code&gt; を実行してリソースをどのノードに割り当てるかのスコアを確認しています。&lt;/p&gt;

&lt;p&gt;まず WebSite という名前のリソースを &lt;code&gt;disabled&lt;/code&gt; 状態で作成します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# pcs resource create WebSite ocf:heartbeat:apache \
    configfile=/etc/httpd/conf/httpd.conf \
    statusurl=&amp;quot;http://localhost/server-status&amp;quot; \
    op monitor interval=3s on-fail=restart \
    --disabled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;WebSite リソースは ClusterIP リソースと同じノードで動かすという制約を追加します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# pcs constraint colocation add WebSite with ClusterIP INFINITY
# crm_simulate -sL

Current cluster status:
Online: [ pcmk-1 pcmk-2 ]

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-1
 WebSite        (ocf::heartbeat:apache):        (target-role:Stopped) Stopped

Allocation scores:
native_color: ClusterIP allocation score on pcmk-1: 100
native_color: ClusterIP allocation score on pcmk-2: 0
native_color: WebSite allocation score on pcmk-1: -INFINITY
native_color: WebSite allocation score on pcmk-2: -INFINITY

Transition Summary:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;リソースの開始順序を ClusterIP、 WebSite にする制約を追加します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# pcs constraint order ClusterIP then WebSite
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;WebSite のリソースをなるべく &lt;code&gt;pcmk-1&lt;/code&gt; 側で動かすようにする制約を追加します。 &lt;code&gt;250&lt;/code&gt; という値はこの後の操作を一度試行錯誤してみて適当に選びましたが、希望通りの動作が実現できさえすれば違う値でも構いません。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# pcs constraint location WebSite prefers pcmk-1=250
# crm_simulate -sL

Current cluster status:
Online: [ pcmk-1 pcmk-2 ]

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-1
 WebSite        (ocf::heartbeat:apache):        (target-role:Stopped) Stopped

Allocation scores:
native_color: ClusterIP allocation score on pcmk-1: 350
native_color: ClusterIP allocation score on pcmk-2: 0
native_color: WebSite allocation score on pcmk-1: -INFINITY
native_color: WebSite allocation score on pcmk-2: -INFINITY

Transition Summary:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;希望する制約を一通り追加したので、 WebSite リソースを稼働開始します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# pcs resource enable WebSite
# crm_simulate -sL

Current cluster status:
Online: [ pcmk-1 pcmk-2 ]

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-1
 WebSite        (ocf::heartbeat:apache):        Started pcmk-1

Allocation scores:
native_color: ClusterIP allocation score on pcmk-1: 450
native_color: ClusterIP allocation score on pcmk-2: 0
native_color: WebSite allocation score on pcmk-1: 350
native_color: WebSite allocation score on pcmk-2: -INFINITY

Transition Summary:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;pcmk-1&lt;/code&gt; と &lt;code&gt;pcmk-2&lt;/code&gt; で &lt;code&gt;ps auxww | grep httpd&lt;/code&gt; すると &lt;code&gt;pcmk-1&lt;/code&gt; 側で Apache が稼働して &lt;code&gt;pcmk-2&lt;/code&gt; 側では稼働していないことを確認できます。&lt;/p&gt;

&lt;h3 id=&#34;手動で制約を調整して仮想ipとapacheを-pcmk-2-に移動する&#34;&gt;手動で制約を調整して仮想IPとApacheを &lt;code&gt;pcmk-2&lt;/code&gt; に移動する&lt;/h3&gt;

&lt;p&gt;以下のコマンドで制約を調整し、移動が完了するまでのスコアの変遷を確認します。 &lt;code&gt;500&lt;/code&gt; という値は前項の最後の &lt;code&gt;pcmk-1&lt;/code&gt; の ClusterIP のスコアを上回る値として選びました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# pcs constraint location WebSite prefers pcmk-2=500 \
  &amp;amp;&amp;amp; for i in `seq 1 20`; do crm_simulate -sL; sleep 0.1; done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;出力結果のうち変化があったものだけを抜粋します。
まず開始直後の状態。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Current cluster status:
Online: [ pcmk-1 pcmk-2 ]

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-1
 WebSite        (ocf::heartbeat:apache):        Started pcmk-1

Allocation scores:
native_color: ClusterIP allocation score on pcmk-1: 450
native_color: ClusterIP allocation score on pcmk-2: 500
native_color: WebSite allocation score on pcmk-1: -INFINITY
native_color: WebSite allocation score on pcmk-2: 500

Transition Summary:
 * Move    ClusterIP    (Started pcmk-1 -&amp;gt; pcmk-2)
 * Move    WebSite      (Started pcmk-1 -&amp;gt; pcmk-2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;WebSiteが停止した状態。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Current cluster status:
Online: [ pcmk-1 pcmk-2 ]

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-1
 WebSite        (ocf::heartbeat:apache):        Stopped

Allocation scores:
native_color: ClusterIP allocation score on pcmk-1: 350
native_color: ClusterIP allocation score on pcmk-2: 500
native_color: WebSite allocation score on pcmk-1: -INFINITY
native_color: WebSite allocation score on pcmk-2: 500

Transition Summary:
 * Move    ClusterIP    (Started pcmk-1 -&amp;gt; pcmk-2)
 * Start   WebSite      (pcmk-2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ClusterIPがpcmk-2に移った状態。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Current cluster status:
Online: [ pcmk-1 pcmk-2 ]

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-2
 WebSite        (ocf::heartbeat:apache):        Stopped

Allocation scores:
native_color: ClusterIP allocation score on pcmk-1: 250
native_color: ClusterIP allocation score on pcmk-2: 600
native_color: WebSite allocation score on pcmk-1: -INFINITY
native_color: WebSite allocation score on pcmk-2: 500

Transition Summary:
 * Start   WebSite      (pcmk-2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;WebSiteが &lt;code&gt;pcmk-2&lt;/code&gt; で稼働開始した状態。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Current cluster status:
Online: [ pcmk-1 pcmk-2 ]

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-2
 WebSite        (ocf::heartbeat:apache):        Started pcmk-2

Allocation scores:
native_color: ClusterIP allocation score on pcmk-1: 250
native_color: ClusterIP allocation score on pcmk-2: 700
native_color: WebSite allocation score on pcmk-1: -INFINITY
native_color: WebSite allocation score on pcmk-2: 600

Transition Summary:
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;手動で制約を調整して仮想ipとapacheを-pcmk-1-に戻す&#34;&gt;手動で制約を調整して仮想IPとApacheを &lt;code&gt;pcmk-1&lt;/code&gt; に戻す&lt;/h3&gt;

&lt;p&gt;次に &lt;code&gt;pcmk-2&lt;/code&gt; から &lt;code&gt;pcmk-1&lt;/code&gt; に戻してみます。
以下のコマンドで制約のIDを確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-1 ~]# pcs constraint --full
Location Constraints:
  Resource: WebSite
    Enabled on: pcmk-1 (score:250) (id:location-WebSite-pcmk-1-250)
    Enabled on: pcmk-2 (score:500) (id:location-WebSite-pcmk-2-500)
Ordering Constraints:
  start ClusterIP then start WebSite (kind:Mandatory) (id:order-ClusterIP-WebSite-mandatory)
Colocation Constraints:
  WebSite with ClusterIP (score:INFINITY) (id:colocation-WebSite-ClusterIP-INFINITY)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下のコマンドで &lt;code&gt;pcmk-2&lt;/code&gt; 側の制約を削除し、 &lt;code&gt;pcmk-1&lt;/code&gt; 側に戻るまでのスコアの動きを確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# pcs constraint remove location-WebSite-pcmk-2-500 \
  &amp;amp;&amp;amp; for i in `seq 1 20`; do crm_simulate -sL; sleep 0.1; done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;開始直後の状態。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Current cluster status:
Online: [ pcmk-1 pcmk-2 ]

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-2
 WebSite        (ocf::heartbeat:apache):        Started pcmk-2

Allocation scores:
native_color: ClusterIP allocation score on pcmk-1: 250
native_color: ClusterIP allocation score on pcmk-2: 200
native_color: WebSite allocation score on pcmk-1: 250
native_color: WebSite allocation score on pcmk-2: -INFINITY

Transition Summary:
 * Move    ClusterIP    (Started pcmk-2 -&amp;gt; pcmk-1)
 * Move    WebSite      (Started pcmk-2 -&amp;gt; pcmk-1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;WebSiteが停止した状態。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Current cluster status:
Online: [ pcmk-1 pcmk-2 ]

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-2
 WebSite        (ocf::heartbeat:apache):        Stopped

Allocation scores:
native_color: ClusterIP allocation score on pcmk-1: 250
native_color: ClusterIP allocation score on pcmk-2: 100
native_color: WebSite allocation score on pcmk-1: 250
native_color: WebSite allocation score on pcmk-2: -INFINITY

Transition Summary:
 * Move    ClusterIP    (Started pcmk-2 -&amp;gt; pcmk-1)
 * Start   WebSite      (pcmk-1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ClusterIPが &lt;code&gt;pcmk-1&lt;/code&gt; に移動した状態。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Current cluster status:
Online: [ pcmk-1 pcmk-2 ]

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-1
 WebSite        (ocf::heartbeat:apache):        Stopped

Allocation scores:
native_color: ClusterIP allocation score on pcmk-1: 350
native_color: ClusterIP allocation score on pcmk-2: 0
native_color: WebSite allocation score on pcmk-1: 250
native_color: WebSite allocation score on pcmk-2: -INFINITY

Transition Summary:
 * Start   WebSite      (pcmk-1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;WebSiteが &lt;code&gt;pcmk-1&lt;/code&gt; で稼働開始した状態。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Current cluster status:
Online: [ pcmk-1 pcmk-2 ]

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-1
 WebSite        (ocf::heartbeat:apache):        Started pcmk-1

Allocation scores:
native_color: ClusterIP allocation score on pcmk-1: 450
native_color: ClusterIP allocation score on pcmk-2: 0
native_color: WebSite allocation score on pcmk-1: 350
native_color: WebSite allocation score on pcmk-2: -INFINITY

Transition Summary:
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;active側のコンテナに障害が発生してコンテナごと落ちるケースの模擬実験&#34;&gt;Active側のコンテナに障害が発生してコンテナごと落ちるケースの模擬実験&lt;/h2&gt;

&lt;h3 id=&#34;active側-pcmk-1-のコンテナを停止させたときの挙動を確認&#34;&gt;Active側 &lt;code&gt;pcmk-1&lt;/code&gt; のコンテナを停止させたときの挙動を確認&lt;/h3&gt;

&lt;p&gt;LXDホストで以下のコマンドを実行して &lt;code&gt;pcmk-1&lt;/code&gt; を停止させます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ lxc stop -f pcmk-1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;pcmk-2&lt;/code&gt; で &lt;code&gt;pcs status&lt;/code&gt; を実行すると &lt;code&gt;pcmk-1&lt;/code&gt; が OFFLINE になったことがわかりますが、 &lt;code&gt;PCSD Status:&lt;/code&gt; の後を表示するところでブロックしたので Ctrl-C で止めました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-2 ~]# pcs status
Cluster name: mycluster
Last updated: Fri Aug 12 13:13:37 2016          Last change: Fri Aug 12 09:34:39 2016 by root via cibadmin on pcmk-1
Stack: corosync
Current DC: pcmk-2 (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum
2 nodes and 2 resources configured

Online: [ pcmk-2 ]
OFFLINE: [ pcmk-1 ]

Full list of resources:

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-2
 WebSite        (ocf::heartbeat:apache):        Started pcmk-2

PCSD Status:
^C
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;ip a s eth0&lt;/code&gt; と &lt;code&gt;ps auxww | grep httpd&lt;/code&gt; で仮想IPとApacheが &lt;code&gt;pcmk-2&lt;/code&gt; で動いていることが確認できました。&lt;/p&gt;

&lt;h3 id=&#34;pcmk-1-のコンテナを起動させた時の挙動を確認&#34;&gt;&lt;code&gt;pcmk-1&lt;/code&gt; のコンテナを起動させた時の挙動を確認&lt;/h3&gt;

&lt;p&gt;LXDホストで以下のコマンドを実行して &lt;code&gt;pcmk-1&lt;/code&gt; を起動し、コンテナ内に入ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ lxc satrt pcmk-1
$ lxc exec pcmk-1 bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;pcmk-1&lt;/code&gt; 側は &lt;code&gt;pcsd&lt;/code&gt; は起動していますが、クラスタには所属していない状態です。&lt;/p&gt;

&lt;p&gt;理由は &lt;a href=&#34;http://clusterlabs.org/doc/en-US/Pacemaker/1.1-pcs/html/Clusters_from_Scratch/ch04.html#_start_the_cluster&#34;&gt;Chapter 4. Start and Verify Cluster&lt;/a&gt; に説明があります。 &lt;code&gt;pcsd&lt;/code&gt; は &lt;code&gt;systemctl enable&lt;/code&gt; でOS起動時の自動起動を有効にしていますが &lt;code&gt;corosync&lt;/code&gt; と &lt;code&gt;pacemaker&lt;/code&gt; はしていないからです。&lt;/p&gt;

&lt;p&gt;実運用時に物理的な障害などで &lt;code&gt;pcmk-1&lt;/code&gt; がクラスタから外れた場合、その後電源をいれて起動できたとしても、障害の原因を調査して、正常にサービスを稼働できるかを確認してからクラスタに復帰させたいので、この設定で良いと思います。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-1 ~]# pcs status
Error: cluster is not currently running on this node
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;pcmk-2&lt;/code&gt; で &lt;code&gt;pcs status&lt;/code&gt; は今度はブロックせずに完了します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-2 ~]# pcs status
Cluster name: mycluster
Last updated: Fri Aug 12 13:18:22 2016          Last change: Fri Aug 12 09:34:39 2016 by root via cibadmin on pcmk-1
Stack: corosync
Current DC: pcmk-2 (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum
2 nodes and 2 resources configured

Online: [ pcmk-2 ]
OFFLINE: [ pcmk-1 ]

Full list of resources:

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-2
 WebSite        (ocf::heartbeat:apache):        Started pcmk-2

PCSD Status:
  pcmk-1: Online
  pcmk-2: Online

Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下のコマンドを実行して &lt;code&gt;pcmk-1&lt;/code&gt; をクラスタに復帰させます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-1 ~]# pcs cluster start pcmk-1
pcmk-1: Starting Cluster...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;しばらくして &lt;code&gt;pcs status&lt;/code&gt; を確認すると &lt;code&gt;pcmk-1&lt;/code&gt; がオンラインになり、 ClusterIP と WebSite リソースが &lt;code&gt;pcmk-1&lt;/code&gt; に移動することが確認できました。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LXDのdnsmasqの固定IP設定をSIGHUPで更新する</title>
      <link>https://hnakamur.github.io/blog/2016/08/12/update-lxd-dnsmasq-dhcp-hosts-config-with-sighup/</link>
      <pubDate>Fri, 12 Aug 2016 06:38:18 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2016/08/12/update-lxd-dnsmasq-dhcp-hosts-config-with-sighup/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://hnakamur.github.io/blog/blog/2016/05/07/how-to-use-fixed-ip-address-for-a-lxd-container/&#34;&gt;LXDコンテナで固定IPアドレスを使うための設定 · hnakamur&amp;rsquo;s blog at github&lt;/a&gt; では &lt;code&gt;/etc/dnsmasq.conf&lt;/code&gt; に直接 &lt;code&gt;dhcp-host&lt;/code&gt; で設定を書いていましたが、変更するためには &lt;code&gt;lxd-bridge&lt;/code&gt; の再起動が必要でした。&lt;/p&gt;

&lt;p&gt;その後 &lt;a href=&#34;http://manpages.ubuntu.com/manpages/xenial/en/man8/dnsmasq.8.html&#34;&gt;Ubuntu Manpage: dnsmasq - A lightweight DHCP and caching DNS server.&lt;/a&gt; を見て &lt;code&gt;--dhcp-hostsfile=&amp;lt;path&amp;gt;&lt;/code&gt; または &lt;code&gt;--dhcp-hostsdir=&amp;lt;path&amp;gt;&lt;/code&gt; を使っておけば &lt;code&gt;lxd-bridge&lt;/code&gt; を再起動しなくても &lt;code&gt;dnsmasq&lt;/code&gt; に &lt;code&gt;SIGHUP&lt;/code&gt; を送れば更新できることを知りました。 &lt;code&gt;--dhcp-hostsdir=&amp;lt;path&amp;gt;&lt;/code&gt; の場合は、指定したディレクトリ以下のファイルを追加・更新する場合は SIGHUP すら不要で、ファイルを削除した後に反映するときだけ SIGHUP が必要です。&lt;/p&gt;

&lt;p&gt;ですが、実際に試してみると &lt;code&gt;--dhcp-hostsdir&lt;/code&gt; のほうは SIGHUP を送ると &lt;code&gt;duplicate dhcp-host IP address&lt;/code&gt; というエラーになってしまったので (下記のハマりメモ参照)、 &lt;code&gt;--dhcp-hostsfile&lt;/code&gt; のほうを使うことにしました。&lt;/p&gt;

&lt;h2 id=&#34;lxd-bridgeのdnsmasqで-dhcp-hostsfile-を使う設定&#34;&gt;lxd-bridgeのdnsmasqで&amp;ndash;dhcp-hostsfile を使う設定&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;/var/lib/lxd-bridge/dhcp-hosts&lt;/code&gt; というファイルを作って、そこを見るように切り替えてみます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo touch /var/lib/lxd-bridge/dhcp-hosts
echo &#39;dhcp-hostsfile=/var/lib/lxd-bridge/dhcp-hosts&#39; | sudo tee /etc/dnsmasq.conf &amp;gt; /dev/null
sudo systemctl restart lxd-bridge
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;ipアドレスを指定して新規コンテナを作成する&#34;&gt;IPアドレスを指定して新規コンテナを作成する&lt;/h2&gt;

&lt;p&gt;例えば &lt;code&gt;web01&lt;/code&gt; というコンテナを &lt;code&gt;10.155.92.201&lt;/code&gt; というアドレスで作成したい場合は以下のようにします。 &lt;a href=&#34;http://manpages.ubuntu.com/manpages/xenial/en/man8/dnsmasq.8.html&#34;&gt;Ubuntu Manpage: dnsmasq - A lightweight DHCP and caching DNS server.&lt;/a&gt; によると &lt;code&gt;--dhcp-range&lt;/code&gt; で指定した範囲の外でも良いが &lt;code&gt;--dhcp-range&lt;/code&gt; と同じサブネットである必要があるとのことです。 &lt;code&gt;ps auxww | grep dnsmasq&lt;/code&gt; で見たところ &lt;code&gt;/etc/default/lxd-bridge&lt;/code&gt; の &lt;code&gt;LXD_IPV4_DHCP_RANGE&lt;/code&gt; の値が &lt;code&gt;--dhcp-range&lt;/code&gt; に使われています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo web01,10.155.92.201 | sudo tee /var/lib/lxd-bridge/dhcp-hosts &amp;gt; /dev/null
sudo kill -HUP `cat /var/run/lxd-bridge/dnsmasq.pid`
lxc launch images:centos/7/amd64 web01
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;数秒してから &lt;code&gt;lxc list&lt;/code&gt; を実行すると指定したアドレスになっていることが確認できます。&lt;/p&gt;

&lt;p&gt;なお、この例では dhcp-hosts 内のエントリが web01 の1つだけなので echo と tee で作成・更新していますが、実際の利用時には複数エントリがあるので既存のエントリを残しつつエントリを追加・更新する必要がありますのでご注意ください。&lt;/p&gt;

&lt;h2 id=&#34;既存のコンテナのipアドレスを変更する&#34;&gt;既存のコンテナのIPアドレスを変更する&lt;/h2&gt;

&lt;p&gt;上記で作成した &lt;code&gt;web01&lt;/code&gt; というコンテナのアドレスを &lt;code&gt;10.155.92.202&lt;/code&gt; に変更してみます。変更にはコンテナの再起動が必要になります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo web01,10.155.92.202 | sudo tee /var/lib/lxd-bridge/dhcp-hosts &amp;gt; /dev/null
sudo kill -HUP `cat /var/run/lxd-bridge/dnsmasq.pid`
lxc restart -f web01
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;数秒してから &lt;code&gt;lxc list&lt;/code&gt; を実行すると指定したアドレスになっていることが確認できます。&lt;/p&gt;

&lt;p&gt;この方法でIPアドレスを変更すると &lt;code&gt;/var/lib/lxd-bridge/dnsmasq.lxdbr0.leases&lt;/code&gt; に変更前のアドレスが残らないので、そのアドレスをすぐに他で再利用することが出来ます。&lt;/p&gt;

&lt;h2 id=&#34;コンテナを削除後-同じipアドレスを他のコンテナで使う&#34;&gt;コンテナを削除後、同じIPアドレスを他のコンテナで使う&lt;/h2&gt;

&lt;p&gt;一方、コンテナを削除しても使っていたIPアドレスはまだ貸出中になっています。&lt;/p&gt;

&lt;p&gt;上記の状態の後&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lxc delete -f web01
: | sudo tee /var/lib/lxd-bridge/dhcp-hosts
sudo kill -HUP `cat /var/run/lxd-bridge/dnsmasq.pid`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;としても &lt;code&gt;/var/lib/lxd-bridge/dnsmasq.lxdbr0.leases&lt;/code&gt; には&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1470963716 00:16:3e:45:a6:d1 10.155.92.202 web01 *
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;のようなエントリが残っています。
このアドレスを他のコンテナで使うためには一旦解放する必要があります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo dhcp_release lxdbr0 10.155.92.202 00:16:3e:45:a6:d1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;のように実行するか、あるいは &lt;a href=&#34;https://hnakamur.github.io/blog/blog/2016/08/11/release-all-unused-addresses-of-lxd-bridge/&#34;&gt;LXDのDHCPで使っていないIPアドレスを一括で解放するスクリプトを書いた · hnakamur&amp;rsquo;s blog at github&lt;/a&gt; で書いたスクリプトを実行して解放します。以下では後者のスクリプトを &lt;code&gt;~/bin/lxd-bridge-release-all-unused-addresses.sh&lt;/code&gt; に保存してあるものとして説明します。&lt;/p&gt;

&lt;p&gt;IPアドレスを解放した後で&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo web02,10.155.92.202 | sudo tee /var/lib/lxd-bridge/dhcp-hosts &amp;gt; /dev/null
sudo kill -HUP `cat /var/run/lxd-bridge/dnsmasq.pid`
lxc launch images:centos/7/amd64 web02
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;のように実行すれば、IPアドレスを &lt;code&gt;10.155.92.202&lt;/code&gt; にして &lt;code&gt;web02&lt;/code&gt; というコンテナを作成・起動できました。&lt;/p&gt;

&lt;h2 id=&#34;dhcp-hostsdirのハマりメモ&#34;&gt;&amp;ndash;dhcp-hostsdirのハマりメモ&lt;/h2&gt;

&lt;h3 id=&#34;lxd-bridgeのdnsmasqで-dhcp-hostsdir-を使う設定&#34;&gt;lxd-bridgeのdnsmasqで&amp;ndash;dhcp-hostsdir を使う設定&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;/var/lib/lxd-bridge/dhcp-hosts&lt;/code&gt; というディレクトリを作って、そこを見るように切り替えてみます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[ -f /var/lib/lxd-bridge/dhcp-hosts ] &amp;amp;&amp;amp; sudo rm /var/lib/lxd-bridge/dhcp-hosts
sudo mkdir -p /var/lib/lxd-bridge/dhcp-hosts
echo &#39;dhcp-hostsdir=/var/lib/lxd-bridge/dhcp-hosts&#39; | sudo tee /etc/dnsmasq.conf &amp;gt; /dev/null
sudo systemctl restart lxd-bridge
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;ipアドレスを指定して新規コンテナを作成する-1&#34;&gt;IPアドレスを指定して新規コンテナを作成する&lt;/h3&gt;

&lt;p&gt;例えば &lt;code&gt;web01&lt;/code&gt; というコンテナを &lt;code&gt;10.155.92.201&lt;/code&gt; というアドレスで作成したい場合は以下のようにします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo web01,10.155.92.201 | sudo tee /var/lib/lxd-bridge/dhcp-hosts/web01 &amp;gt; /dev/null
lxc launch images:centos/7/amd64 web01
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;数秒してから &lt;code&gt;lxc list&lt;/code&gt; を実行すると指定したアドレスになっていることが確認できます。&lt;/p&gt;

&lt;p&gt;と、ここまでは良かったのですが、&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;journalctl -f
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;でログを見ておいて、別端末で&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo kill -HUP `cat /var/run/lxd-bridge/dnsmasq.pid`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;と実行すると&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Aug 12 08:39:56 lxdhostname dnsmasq-dhcp[2455]: read /var/lib/lxd-bridge/dhcp-hosts/web01
Aug 12 08:39:56 lxdhostname dnsmasq[2455]: duplicate dhcp-host IP address 10.155.92.201 at line 1 of /var/lib/lxd-bridge/dhcp-hosts/web01
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;のようなエラーが出てしまいました。 &lt;code&gt;duplicate dhcp-host IP address&lt;/code&gt; から後ろは赤字で表示されました。&lt;/p&gt;

&lt;h3 id=&#34;コンテナを削除後-同じipアドレスを他のコンテナで使いたいが失敗&#34;&gt;コンテナを削除後、同じIPアドレスを他のコンテナで使いたいが失敗&lt;/h3&gt;

&lt;p&gt;以下では &lt;a href=&#34;https://hnakamur.github.io/blog/blog/2016/08/11/release-all-unused-addresses-of-lxd-bridge/&#34;&gt;LXDのDHCPで使っていないIPアドレスを一括で解放するスクリプトを書いた · hnakamur&amp;rsquo;s blog at github&lt;/a&gt; のスクリプトを &lt;code&gt;~/bin/lxd-bridge-release-all-unused-addresses.sh&lt;/code&gt; に保存してあるものとして説明します。&lt;/p&gt;

&lt;p&gt;上記の状態の後、 &lt;code&gt;journalctl -f&lt;/code&gt; を引き続き別端末で実行しておいて&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lxc delete -f web01
sudo rm /var/lib/lxd-bridge/dhcp-hosts/web01
sudo kill -HUP `cat /var/run/lxd-bridge/dnsmasq.pid`
~/bin/lxd-bridge-release-all-unused-addresses.sh
echo web02,10.155.92.201 | sudo tee /var/lib/lxd-bridge/dhcp-hosts/web02 &amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;と実行すると&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Aug 12 08:44:13 lxdhostname dnsmasq-dhcp[2455]: read /var/lib/lxd-bridge/dhcp-hosts/web02
Aug 12 08:44:13 lxdhostname dnsmasq[2455]: duplicate dhcp-host IP address 10.155.92.201 at line 1 of /var/lib/lxd-bridge/dhcp-hosts/web02
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;と先程と同様のエラーが出ました。ここから&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lxc launch images:centos/7/amd64 web02
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;と実行しても、指定した &lt;code&gt;10.155.92.201&lt;/code&gt; とは異なるアドレスになってしまいました。&lt;/p&gt;

&lt;p&gt;ということで &lt;code&gt;--dhcp-hostsdir=&amp;lt;path&amp;gt;&lt;/code&gt; は正しい使い方がわからなかったので、諦めて &lt;code&gt;--dhcp-hostsfile=&amp;lt;path&amp;gt;&lt;/code&gt; のほうを使うことにしました。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LXDのDHCPで使っていないIPアドレスを一括で解放するスクリプトを書いた</title>
      <link>https://hnakamur.github.io/blog/2016/08/11/release-all-unused-addresses-of-lxd-bridge/</link>
      <pubDate>Thu, 11 Aug 2016 22:58:21 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2016/08/11/release-all-unused-addresses-of-lxd-bridge/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://hnakamur.github.io/blog/blog/2016/05/07/how-to-use-fixed-ip-address-for-a-lxd-container/&#34;&gt;LXDコンテナで固定IPアドレスを使うための設定 · hnakamur&amp;rsquo;s blog at github&lt;/a&gt; の設定を行ってもIPアドレスが指定通りにならないことがありました。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;journal -xe&lt;/code&gt; で見てみると&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Aug 11 22:46:55 bai1b7faf04 dnsmasq-dhcp[11082]: not using configured address 10.155.92.102 because it is leased to 00:16:3e:1e:08:8a
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;というメッセージが出ていて、他のMACアドレスに貸出中になっています。&lt;/p&gt;

&lt;p&gt;ググってみると &lt;a href=&#34;http://www.linuxquestions.org/questions/linux-newbie-8/dnsmasq-force-release-renew-of-dhcp-clients-how-933535/&#34;&gt;[SOLVED] dnsmasq force release/renew of dhcp clients, how?&lt;/a&gt; に回答がありました。&lt;/p&gt;

&lt;h2 id=&#34;使っていないipアドレスを手動で消す&#34;&gt;使っていないIPアドレスを手動で消す&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl stop lxd-bridge
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;で止めて&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo vi /var/lib/lxd-bridge/dnsmasq.lxdbr0.leases
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;で使っていないIPアドレスの行を全て削除します。&lt;/p&gt;

&lt;p&gt;その後&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl start lxd-bridge
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;で再起動します。&lt;/p&gt;

&lt;h2 id=&#34;自動で消すスクリプトも書きました&#34;&gt;自動で消すスクリプトも書きました&lt;/h2&gt;

&lt;p&gt;これでよいかと思ったら、
&lt;a href=&#34;http://lists.thekelleys.org.uk/pipermail/dnsmasq-discuss/2013q3/007356.html&#34;&gt;http://lists.thekelleys.org.uk/pipermail/dnsmasq-discuss/2013q3/007356.html&lt;/a&gt;
を見て &lt;code&gt;dhcp_release&lt;/code&gt; というコマンドを使えば &lt;code&gt;lxd-bridge&lt;/code&gt; の再起動が不要なことを知りました。&lt;/p&gt;

&lt;p&gt;ということでスクリプトを書いてみました。
&lt;a href=&#34;https://gist.github.com/hnakamur/7ed3f7c6175817b633586a1b468bd5c1&#34;&gt;https://gist.github.com/hnakamur/7ed3f7c6175817b633586a1b468bd5c1&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/sh
set -eu

# Set value of LXD_BRIDGE
. /etc/default/lxd-bridge

addr_list_file=/tmp/lxd-addr-list.`date +%Y-%m-%dT%H:%M:%S`
lxc list | awk &#39;$4==&amp;quot;RUNNING&amp;quot;{print $6}&#39; &amp;gt; $addr_list_file
cleanup() {
  rm $addr_list_file
}
trap cleanup EXIT

awk -v addr_list_file=$addr_list_file -v interface=$LXD_BRIDGE &#39;{
  mac_addr = $2
  addr = $3
  ret = system(sprintf(&amp;quot;awk -v addr=%s &#39;\&#39;&#39;BEGIN{rc=1} $1==addr{rc=0} END{exit rc}&#39;\&#39;&#39; %s&amp;quot;, addr,  addr_list_file))
  if (ret == 1) {
    system(sprintf(&amp;quot;sudo dhcp_release %s %s %s&amp;quot;, interface, addr, mac_addr))
  }
}&#39; /var/lib/lxd-bridge/dnsmasq.$LXD_BRIDGE.leases
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ubuntu 16.04 の場合 &lt;code&gt;dhcp_release&lt;/code&gt; コマンドを使うには以下のように &lt;code&gt;dnsmasq-utils&lt;/code&gt; パッケージをインストールする必要があります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt -y install dnsmasq-utils
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>ブログ記事「Go言語(Golang) はまりどころと解決策」についてのコメント</title>
      <link>https://hnakamur.github.io/blog/2016/08/02/about-go-pitfalls/</link>
      <pubDate>Tue, 02 Aug 2016 05:57:52 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2016/08/02/about-go-pitfalls/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://www.yunabe.jp/docs/golang_pitfall.html&#34;&gt;Go言語(Golang) はまりどころと解決策&lt;/a&gt;の記事についてのコメント記事を誰かが書くだろうと思ってスルーしてましたが、見かけないので書いてみます。&lt;/p&gt;

&lt;p&gt;ただし私はGo言語を使って開発していますが、言語自体を詳細に知るエキスパートでは無いです。Go言語にかぎらず個人的にはややこしいところにはなるべく近づかないスタンスなので、詳しい方から見ると物足りないかもしれません。そう感じた方は是非ブログ記事なりを書いていただけると嬉しいです。&lt;/p&gt;

&lt;h2 id=&#34;interface-とnil-goのinterfaceは単なる参照ではない&#34;&gt;interface とnil (Goのinterfaceは単なる参照ではない)&lt;/h2&gt;

&lt;p&gt;特にコメントはなくてそのとおりだと思います。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://golang.org/doc/faq&#34;&gt;Frequently Asked Questions (FAQ)&lt;/a&gt;に加えて &lt;a href=&#34;https://golang.org/doc/effective_go.html&#34;&gt;Effective Go&lt;/a&gt;も早めに読んでおいたほうが良いと思います。&lt;/p&gt;

&lt;p&gt;またnilに関する文献としては &lt;a href=&#34;https://speakerdeck.com/campoy/understanding-nil&#34;&gt;Understanding Nil // Speaker Deck&lt;/a&gt; もおすすめです。&lt;/p&gt;

&lt;h2 id=&#34;メソッド内でレシーバ-this-self-がnilでないことをチェックすることに意味がある&#34;&gt;メソッド内でレシーバ(this, self)がnilでないことをチェックすることに意味がある&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://golang.org/ref/spec#Method_declarations&#34;&gt;Method declarations&lt;/a&gt; に&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The type of a method is the type of a function with the receiver as first argument.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;とあります。メソッドの型はメソッドの引数の前にレシーバを第一引数として入れた関数の型になるとのことです。&lt;/p&gt;

&lt;p&gt;大雑把に言えば、メソッドは第一引数にレシーバを追加した関数と実質同じです。と考えればメソッド内でポインタ型のレシーバのnilチェックをすることは特に違和感ないと思います。&lt;/p&gt;

&lt;h2 id=&#34;errorしか返り値がない関数でerrorを処理し忘れる&#34;&gt;errorしか返り値がない関数でerrorを処理し忘れる&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/alecthomas/gometalinter&#34;&gt;alecthomas/gometalinter&lt;/a&gt;でチェックできました。&lt;/p&gt;

&lt;p&gt;実行例を示します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ gometalinter 
main.go:8:6:warning: exported type Data should have comment or be unexported (golint)
main.go:4:2:error: could not import encoding/json (reading export data: /usr/local/go1.7rc3/pkg/linux_amd64/encoding/json.a: unknown version: v1json    E$GOROOT/src/encoding/json/decode.go?Un) (gotype)
-$GOROOT/src/fmt/scan.go not impStatr) (gotype)g export data: /usr/local/go1.7rc3/pkg/linux_amd64/fmt.a: unknown version: v1fmt
main.go:14:2:error: undeclared name: json (gotype)
main.go:15:2:error: undeclared name: fmt (gotype)
main.go:14:16:warning: error return value not checked (json.Unmarshal([]byte(&amp;quot;not json&amp;quot;), d)) (errcheck)
main.go:9:2:warning: unused struct field github.com/hnakamur/forgotten-error-experiment.Data.a (structcheck)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;jsonやfmt関連のエラーは何言ってるのかよくわからないで無視するとして&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;main.go:14:16:warning: error return value not checked (json.Unmarshal([]byte(&amp;quot;not json&amp;quot;), d)) (errcheck)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;で error の戻り値がチェックされていないことを指摘されています。&lt;/p&gt;

&lt;p&gt;gometalinterのセットアップと使い方は&lt;a href=&#34;http://qiita.com/spiegel-im-spiegel/items/238f6f0ee27bdf1de2a0&#34;&gt;gometalinter で楽々 lint - Qiita&lt;/a&gt;にわかりやすい記事がありました。&lt;/p&gt;

&lt;h2 id=&#34;基本型がメソッドを持たない&#34;&gt;基本型がメソッドを持たない&lt;/h2&gt;

&lt;p&gt;FAQの&lt;a href=&#34;https://golang.org/doc/faq#methods_on_basics&#34;&gt;Why is len a function and not a method?&lt;/a&gt;によると &lt;code&gt;len&lt;/code&gt; などをメソッドにすることも検討したけど、 &lt;code&gt;len&lt;/code&gt; がメソッドではなく関数でも実用上困らないし、そのほうが基本型の (Go言語の型の意味での) インタフェースについての質問を複雑にしないので、 &lt;code&gt;len&lt;/code&gt; などは関数として実装することにしたそうです。&lt;/p&gt;

&lt;p&gt;「インタフェースについての質問」あたりはとりあえずそう訳しましたが、意味はよくわかりません。詳しい方のコメントを期待したいところです。&lt;/p&gt;

&lt;h2 id=&#34;stringが単なるバイト列&#34;&gt;stringが単なるバイト列&lt;/h2&gt;

&lt;p&gt;「正直本当に正しいのかはよく分かりません」については私は正しいかどうかという話というよりは、Go言語ではそう決めたというだけの話かと思っています。&lt;/p&gt;

&lt;p&gt;言語の利用者がハマりにくい決定をするほうが望ましいという意味で「正しいか」と言われているのだとは思いますが、私自身はほぼ常にUTF-8の文字列しか使ってないので特にハマったことはないです。&lt;/p&gt;

&lt;p&gt;文字コード変換には&lt;a href=&#34;https://github.com/golang/text&#34;&gt;golang/text: [mirror] Go text processing support&lt;/a&gt;というパッケージがあります。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;io.Reader&lt;/code&gt; からEUC-JP, Shift_JIS, ISO-2022-JPの文字列を読み込んで UTF-8に変換するのは&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/hnakamur/goqueryja/blob/01aead01dd3ac586c6256140a26a50fb30451971/lib.go#L27-L40&#34;&gt;https://github.com/hnakamur/goqueryja/blob/01aead01dd3ac586c6256140a26a50fb30451971/lib.go#L27-L40&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;というコードで実現できます。&lt;/p&gt;

&lt;h2 id=&#34;継承がない&#34;&gt;継承がない&lt;/h2&gt;

&lt;p&gt;継承を敢えて排除したのはGoの好きな点の1つです。&lt;/p&gt;

&lt;h2 id=&#34;genericsがない&#34;&gt;Genericsがない&lt;/h2&gt;

&lt;p&gt;私が他の言語で知ってるのはJavaのGenericsとHaskellの型クラスです。Haskellは軽く勉強した程度ですが、型クラスはシンプルで汎用的で美しさを感じました。&lt;/p&gt;

&lt;p&gt;一方Javaは10年近く仕事で書いてましたが、 &lt;code&gt;? extends&lt;/code&gt; とか &lt;code&gt;? super&lt;/code&gt; のあたりはよくわからなくて避けてました。当時はそれでも困らなかったです。&lt;/p&gt;

&lt;p&gt;複雑なものが苦手な私としては、Javaのような複雑さになるぐらいならGenericsは無いほうが良いと思うので、Goの決断は私は賛成です。&lt;/p&gt;

&lt;p&gt;Genericsが無いとMap, Each, Selectのような関数を []interface{} に対して書いてみたくなると思います。 &lt;a href=&#34;http://qiita.com/hnakamur/items/76b06603013279b14aeb&#34;&gt;goでEach, Map, Selectのサンプル - Qiita&lt;/a&gt;で私も昔書いてみました。でも&lt;a href=&#34;http://qiita.com/hnakamur/items/76b06603013279b14aeb#comment-3d16d66e68bad9626f56&#34;&gt;コメント&lt;/a&gt;に書いたように、Goの開発者のRob Pikeさんもこういう関数は使わずに &lt;code&gt;for&lt;/code&gt; ループを使うべきと書かれています。&lt;/p&gt;

&lt;p&gt;Goに入ってはGoに従え (When in Go, do as the gophers do) ということで &lt;code&gt;for&lt;/code&gt; で書くのが良いと思います。&lt;/p&gt;

&lt;h2 id=&#34;goroutine-はgcされない&#34;&gt;goroutine はGCされない&lt;/h2&gt;

&lt;p&gt;同意です。&lt;/p&gt;

&lt;p&gt;Dave Cheneyさんのツイート
&lt;a href=&#34;https://twitter.com/davecheney/status/714053897841577985&#34;&gt;https://twitter.com/davecheney/status/714053897841577985&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;とスライド
&lt;a href=&#34;https://github.com/davecheney/high-performance-go-workshop/blob/ee2e7a82092a72d742b12b00308b0145f124d593/high-performance-go-workshop.slide#L648-L658&#34;&gt;https://github.com/davecheney/high-performance-go-workshop/blob/ee2e7a82092a72d742b12b00308b0145f124d593/high-performance-go-workshop.slide#L648-L658&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;にある&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Never start a goroutine without knowing how it will stop.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;というルールを守るのが良い習慣だと思います。&lt;/p&gt;

&lt;h2 id=&#34;goroutineはgenerator-yield-の実装には使えない&#34;&gt;goroutineはgenerator (yield) の実装には使えない&lt;/h2&gt;

&lt;p&gt;内容自体は同意です。&lt;/p&gt;

&lt;p&gt;ちょっと脱線になりますが、こういう他の言語の仕組みを同じようなものを作ろうとするのは、そもそもGoの文化になじまないです。Goは他の言語では常識とされている仕組みも一から吟味して取捨選択して最低限のものだけを残して、それ以外は敢えて含めていないと感じていて、ミニマリストな私には非常に魅力的です。&lt;/p&gt;

&lt;p&gt;less is moreの精神を感じます。言語の仕組みが最低限で、同じようなことは同じように書くことになるので、サードパーティのライブラリなど人のコードを読むときに非常に読みやすいというメリットがあります。&lt;/p&gt;

&lt;p&gt;また、自分でコードを書くときにも、似たようなことを実現するために複数の仕組みがあるとこのケースではどれを選ぶべきかと考える必要がありますが、決まったパターンがあれば悩む時間がありません。&lt;/p&gt;

&lt;p&gt;この結果Go言語だと言語でどう書くかよりもアプリケーションやライブラリの問題領域の方に注力しやすいと感じています。&lt;/p&gt;

&lt;p&gt;yieldみたいなことはせずに、goroutineを複数動かしてchannelでデータをやり取りするか、変数を sync.Mutex などで排他制御してデータをやり取りするのがGo流だと思います。あるいは簡単なイテレータなら関数を返すような関数で実現可能だと思います。&lt;/p&gt;

&lt;h2 id=&#34;例外が-推奨され-ない&#34;&gt;例外が(推奨され)ない&lt;/h2&gt;

&lt;p&gt;Java, Python, Rubyなどを書いていた私としても例外がないのは不便なのではと最初は思いましたが、今では err が戻り値で毎回&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if err != nil {
   return err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;と書くほうが、エラーの処理漏れが無いことが明確で安心感を感じます。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://blog.golang.org/errors-are-values&#34;&gt;Errors are values - The Go Blog&lt;/a&gt;のbufioのScannerのようにエラーがチェックする関数が別になっている例もあります。が、個人的には、記事中にある、もしもの例で &lt;code&gt;Scan()&lt;/code&gt; がエラーも返す例のほうがわかりやすいと思います。&lt;/p&gt;

&lt;p&gt;というのも、初めて &lt;code&gt;bufio.Scanner&lt;/code&gt; のドキュメントを見た時は &lt;code&gt;Err()&lt;/code&gt; の存在に気づいて無かったです。ただし、 &lt;a href=&#34;https://golang.org/pkg/bufio/#Scanner&#34;&gt;https://golang.org/pkg/bufio/#Scanner&lt;/a&gt; の Example (Lines) とかを見れば &lt;code&gt;Err()&lt;/code&gt; を使ったサンプルコードが書いてあるんですけどね。&lt;/p&gt;

&lt;p&gt;余談ですけど、APIドキュメントに Example でサンプルコードがついているときは必ず見たほうが良いです。関数のシグネチャ見ただけでは気づかない使い方が説明されていることが多いので。&lt;/p&gt;

&lt;p&gt;エラー処理は&lt;a href=&#34;https://blog.golang.org/error-handling-and-go&#34;&gt;Error handling and Go - The Go Blog&lt;/a&gt;のブログ記事も読みましょう。&lt;/p&gt;

&lt;p&gt;あと &lt;code&gt;panic&lt;/code&gt; と &lt;code&gt;recover&lt;/code&gt; で例外もどきを実現しようとするのも止めましょう。私は &lt;code&gt;recover&lt;/code&gt; は一度足りとも使ったことが無いです。&lt;/p&gt;

&lt;p&gt;panic はエラーがほぼ起きないケースでerrorをreturnして呼び出し側で処理したくないケースは使うこともあります。panicすると標準エラー出力にエラーメッセージとスタックトレースが出力されて異常終了します。&lt;/p&gt;

&lt;p&gt;Goのアプリケーションをsystemdから起動する場合は、panicするとjournalctlでログが見られてそちらで発生日時もわかるので、それでチェックしています。&lt;/p&gt;

&lt;h2 id=&#34;繰り返す-if-err-nil-return-err&#34;&gt;繰り返す if err != nil {return err}&lt;/h2&gt;

&lt;p&gt;ひとつ前の「例外が(推奨され)ない」にまとめて書きました。
個人的には同じパターンで繰り返すほうが、ケースバイケースで書き方が違うより、読みやすいです。&lt;/p&gt;

&lt;h2 id=&#34;return-nil-err-このerrorどこで発生したの&#34;&gt;return nil, err → このerrorどこで発生したの？&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;if err != nil {
  return nil, fmt.Errorf(&amp;quot;Some context: %v&amp;quot;, err)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;でコンテキストを追加するのがGo流らしいです。&lt;/p&gt;

&lt;p&gt;でも個人的にはスタックトレースのほうが楽だと感じます。あと個人的にはエラーが起きた地点での関連する変数もログ出力したいので、自作のログライブラリでは
&lt;a href=&#34;https://godoc.org/github.com/hnakamur/ltsvlog#LTSVLogger.ErrorWithStack&#34;&gt;func (l *LTSVLogger) ErrorWithStack(lv &amp;hellip;LV)&lt;/a&gt; というメソッドを用意して、エラーが起きた箇所でメッセージと変数の値とスタックトレースを出力するようにしています。&lt;/p&gt;

&lt;h2 id=&#34;関数より狭いスコープで-defer&#34;&gt;関数より狭いスコープで defer&lt;/h2&gt;

&lt;p&gt;わかりやすい名前がつけられるケースならprivateの関数に切り出してそちらでdeferするようにします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func myFuncHelper(filename string) (*dataType, error) {
  r, err := os.Open(filename)
  if err != nil {
    return err
  }
  defer r.Close()
  data, err := readDataFromReader(r)  // 実際にはもう少し複雑な処理
  if err != nil {
    return nil, err
  }
  return data, nil
}

func myFunc() error {
  data, err := myFunHelper(filename)
  if err != nil {
    return err
  }
  // その後の他の処理
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;あとエラーで抜けるケースが少なければdeferを使わずに &lt;code&gt;Close()&lt;/code&gt; を呼べば良いと思います。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func myFunc() error {
  // ...
  r, err := os.Open(filename)
  if err != nil {
    return err
  }
  data, err := readDataFromReader(r)  // 実際にはもう少し複雑な処理
  if err != nil {
    r.Close()
    return err
  }
  r.Close()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;「実際にはもう少し複雑な処理」と書いているので、&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  if err != nil {
    r.Close()
    return err
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;が何回も出てくるのでしょうが、多すぎと感じたら別の方法を考える感じで。&lt;/p&gt;

&lt;h2 id=&#34;structとc-javaのクラスとの違い&#34;&gt;structとC++/Javaのクラスとの違い&lt;/h2&gt;

&lt;h3 id=&#34;コンストラクタがない&#34;&gt;コンストラクタがない&lt;/h3&gt;

&lt;p&gt;コンストラクタは無いので &lt;code&gt;NewSomething&lt;/code&gt; とか &lt;code&gt;somepackage.New&lt;/code&gt; のような関数を定義する習慣というのはその通りです。&lt;/p&gt;

&lt;h3 id=&#34;ゼロ初期化が避けられない&#34;&gt;ゼロ初期化が避けられない&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;structが外部に公開されるのならばstructは全てがゼロ初期化された場合にも正しく動くように常に設計しなくてはならないのです。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;これは現実には無理だと思います。例えばファイル名のフィールドのstringが空文字だった時にはどのファイルを処理すれば良いかはわかりっこないです。zero valueでも構わないフィールドについては、zero valueだとどう解釈されるかをAPIドキュメントに書いておけば良い話です。それ以外は呼び出し側が設定する責任があるということで。&lt;/p&gt;

&lt;h3 id=&#34;コピーされるのが避けられない&#34;&gt;コピーされるのが避けられない&lt;/h3&gt;

&lt;p&gt;Go言語自体にコピー防止の仕組みを入れる議論はあったようです。&lt;a href=&#34;https://github.com/golang/go/issues/8005&#34;&gt;runtime: add NoCopy documentation struct type? · Issue #8005 · golang/go&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;このスレッドの&lt;a href=&#34;https://github.com/golang/go/issues/8005#issuecomment-190753527&#34;&gt;コメント&lt;/a&gt;で実現する方法が紹介されています。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/valyala/fasthttp&#34;&gt;valyala/fasthttp&lt;/a&gt;ではこの技を使っていて
&lt;a href=&#34;https://github.com/valyala/fasthttp/blob/master/nocopy.go&#34;&gt;fasthttp/nocopy.go&lt;/a&gt;に &lt;code&gt;noCopy&lt;/code&gt; の定義があり、 &lt;a href=&#34;https://github.com/valyala/fasthttp/blob/45697fe30a130ec6a54426a069c82f3abe76b63d/http.go#L16-L45&#34;&gt;https://github.com/valyala/fasthttp/blob/45697fe30a130ec6a54426a069c82f3abe76b63d/http.go#L16-L45&lt;/a&gt; に使用例があります。&lt;/p&gt;

&lt;h2 id=&#34;型が後置&#34;&gt;型が後置&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://blog.golang.org/gos-declaration-syntax&#34;&gt;Go&amp;rsquo;s Declaration Syntax - The Go Blog&lt;/a&gt; で理由が説明されています。&lt;/p&gt;

&lt;h2 id=&#34;1-0-が浮動小数点型にならない-時がある&#34;&gt;1.0 が浮動小数点型にならない(時がある)&lt;/h2&gt;

&lt;p&gt;これは知りませんでした。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;e := float64(a / 3.0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;と書けば回避できました。 &lt;a href=&#34;https://play.golang.org/p/Y7_LUdQeeq&#34;&gt;https://play.golang.org/p/Y7_LUdQeeq&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;名前が&#34;&gt;名前が…&lt;/h2&gt;

&lt;p&gt;golang で検索すればOKです。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LXDコンテナでPostgreSQLの非同期リプリケーションを試してみた</title>
      <link>https://hnakamur.github.io/blog/2016/07/23/tried-postgresql-async-replication-in-lxd-containers/</link>
      <pubDate>Sat, 23 Jul 2016 21:13:52 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2016/07/23/tried-postgresql-async-replication-in-lxd-containers/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://lets.postgresql.jp/documents/technical/replication/1/&#34;&gt;ストリーミング・レプリケーションの構築 — Let&amp;rsquo;s Postgres&lt;/a&gt; と &lt;a href=&#34;http://d.hatena.ne.jp/hiroe_orz17/20111113/1321180635&#34;&gt;PostgreSQL9.1ためしてみた【非同期レプリケーション編】 - ごろねこ日記&lt;/a&gt; を読んで、2台のLXDコンテナを使ってPostgreSQLの非同期リプリケーションを試してみたのでメモです。&lt;/p&gt;

&lt;p&gt;また&lt;a href=&#34;https://www.packtpub.com/big-data-and-business-intelligence/postgresql-replication-second-edition&#34;&gt;PostgreSQL Replication - Second Edition | PACKT Books&lt;/a&gt;が $10 と安かったので、買って非同期レプリケーションの章まで読みました。&lt;/p&gt;

&lt;p&gt;手順はAnsible playbookとしてまとめました。 &lt;a href=&#34;https://github.com/hnakamur/postgresql-async-replication-example-playbook&#34;&gt;hnakamur/postgresql-async-replication-example-playbook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ansible.cfg&lt;/code&gt; で &lt;code&gt;ask_vault_pass = True&lt;/code&gt; と指定しているので、プレイブック実行時に &lt;code&gt;Vault password:&lt;/code&gt; と聞かれます。パスワードは &lt;code&gt;password&lt;/code&gt; です。サンプルなので単純なパスワードにしていますが、実案件でのプレイブックはきちんとしたパスワードをつけています。&lt;/p&gt;

&lt;h2 id=&#34;テスト環境構築&#34;&gt;テスト環境構築&lt;/h2&gt;

&lt;p&gt;ホストマシンのディストリビューションはUbuntu 16.04でLXD 2.0.3, curl, jqをインストール済みの状態で試しました。&lt;/p&gt;

&lt;p&gt;作業ディレクトリを作って、そこに移動し上記のプレイブックを取得します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/hnakamur/postgresql-async-replication-example-playbook
cd postgresql-async-replication-example-playbook
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;lxd_container&lt;/code&gt; モジュールを使うため、 github から最新のAnsibleをインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;virtualenv venv
source venv/bin/activate
pip install git+https://github.com/ansible/ansible
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;masterとstandbyのコンテナを作成&#34;&gt;masterとstandbyのコンテナを作成&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;ansible-playbook launch_containers.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;実行すると &lt;code&gt;development&lt;/code&gt; というインベントリファイルを生成します。初期状態ではコンテナ &lt;code&gt;pgsql1&lt;/code&gt; が master, コンテナ &lt;code&gt;pgsql2&lt;/code&gt; が standby になります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[development]
pgsql1 postgresql_peer_ipaddr=10.155.92.234 postgressql_master_standby_type=master
pgsql2 postgresql_peer_ipaddr=10.155.92.202 postgressql_master_standby_type=standby

[development:vars]
ansible_connection=lxd
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;コンテナ内にpostgresqlの非同期レプリケーションの環境設定&#34;&gt;コンテナ内にPostgreSQLの非同期レプリケーションの環境設定&lt;/h2&gt;

&lt;p&gt;以下のコマンドを実行してセットアップを実行します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ansible-playbook initial_setup.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;完了したら、2つ端末を開いて片方で&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lxc exec pgsql1 bash
sudo -u postgres -i
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;を実行し、もう片方で&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lxc exec pgsql2 bash
sudo -u postgres -i
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;を実行し、データベースを作ったり pgbench を動かしたりして変更が同期されるのを確認します。&lt;/p&gt;

&lt;p&gt;test というデータベースを作ってpgbenchを実行する手順は以下の通りです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;createdb test
/usr/pgsql-9.5/bin/pgbench -i test
/usr/pgsql-9.5/bin/pgbench -T 180 test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上記の手順を1歩ずつ試し、 test データベースを作る前は pgsql2 では &lt;code&gt;psql test&lt;/code&gt; が失敗しますが作った後は成功するなどで同期が確認できます。&lt;/p&gt;

&lt;h2 id=&#34;レプリケーションの状態確認&#34;&gt;レプリケーションの状態確認&lt;/h2&gt;

&lt;h3 id=&#34;master側での確認&#34;&gt;master側での確認&lt;/h3&gt;

&lt;p&gt;以下のコマンドを実行します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;watch -n 0.5 &#39;psql -x -c &amp;quot;SELECT * FROM pg_stat_replication&amp;quot;&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;こんな感じで確認できます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Every 0.5s: psql -x -c &amp;quot;SELECT * FROM pg_stat_replication&amp;quot;         Sat Jul 23 12:47:27 2016

-[ RECORD 1 ]----+------------------------------
pid              | 2160
usesysid         | 16384
usename          | repl_user
application_name | walreceiver
client_addr      | 10.155.92.234
client_hostname  |
client_port      | 44822
backend_start    | 2016-07-23 08:34:43.696331+00
backend_xmin     |
state            | streaming
sent_location    | 0/30031E0
write_location   | 0/30031E0
flush_location   | 0/30031E0
replay_location  | 0/30031E0
sync_priority    | 0
sync_state       | async
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;standby側での確認&#34;&gt;standby側での確認&lt;/h3&gt;

&lt;p&gt;以下のコマンドを実行します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;watch -n 0.5 &#39;ps auxww | grep &amp;quot;[p]ostgres:&amp;quot;&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;こんな感じで確認できます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Every 0.5s: ps auxww | grep &amp;quot;[p]ostgres:&amp;quot;                                   Sat Jul 23 12:49:30 2016
ailabl
postgres  2051  0.0  0.0  86736  3420 ?        Ss   08:34   0:00 postgres: logger process
postgres  2052  0.0  0.0 233948  5996 ?        Ss   08:34   0:00 postgres: startup process   recover
ing 000000010000000000000003
postgres  2071  0.0  0.0 234012  7016 ?        Ss   08:34   0:00 postgres: checkpointer process
postgres  2072  0.0  0.0 233912  5916 ?        Ss   08:34   0:00 postgres: writer processl
postgres  2073  0.0  0.0  88856  3444 ?        Ss   08:34   0:00 postgres: stats collector process

postgres  2078  0.0  0.0 240632  7016 ?        Ss   08:34   0:05 postgres: wal receiver process   st
reaming 0/30031E0
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;フェイルオーバー&#34;&gt;フェイルオーバー&lt;/h2&gt;

&lt;p&gt;masterのPostgreSQLを停止し、 standbyをmasterにpromote (昇格)させます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ansible-playbook failover.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;旧masterを新standbyとして稼働再開&#34;&gt;旧masterを新standbyとして稼働再開&lt;/h2&gt;

&lt;p&gt;ここでインベントリファイル &lt;code&gt;development&lt;/code&gt; 内の &lt;code&gt;postgressql_master_standby_type&lt;/code&gt; 変数の &lt;code&gt;master&lt;/code&gt; と &lt;code&gt;standby&lt;/code&gt; を入れ替えます。&lt;/p&gt;

&lt;p&gt;その後、新standbyのPostgreSQLを起動します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ansible-playbook start_new_standby.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;もし復旧できない自体になった場合は、今のstandbyであるpgsql1 のデータディレクトリを退避して一からリプリケーション環境を構築します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lxc exec pgsql1 -- mv /var/lib/pgsql/9.5/data /var/lib/pgsql/9.5/data.bak
ansible-playbook initial_setup.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;フェイルバック&#34;&gt;フェイルバック&lt;/h2&gt;

&lt;p&gt;masterとstandbyを入れ替えているので、フェイルバックの手順はフェイルオーバーと同じです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ansible-playbook failover.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;コンテナの削除&#34;&gt;コンテナの削除&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;ansible-playbook delete_containers.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;ansible-vaultを使う際の変数命名規則のtips&#34;&gt;Ansible vaultを使う際の変数命名規則のtips&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;ansible-vault encrypt&lt;/code&gt; で暗号化したファイルの内容を確認するには &lt;code&gt;ansible-vault decrypt&lt;/code&gt; で復号化する必要があります。どんな変数があったかを確認する度に行うのは面倒なので、以下のように暗号化するファイル内で定義する変数を一旦別の変数で受け取ってplaybookではそれを参照するようにしました。&lt;/p&gt;

&lt;p&gt;playbookの構成として環境ごとに development, production のようにグループを分けるようにしています（このサンプルでは development だけです）。暗号化するファイルとしないファイルを以下のような配置で作っています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;group_vars/development/secrets.yml
group_vars/development/vars.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;group_vars/development/secrets.yml&lt;/code&gt; では&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;development:
  secrets:
    postgresql_replication_password: _YOUR_PASSWORD_HERE_
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;のように定義します。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;group_vars/development/vars.yml&lt;/code&gt; では&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;postgresql_replication_password: &amp;quot;{{ development.secrets.postgresql_replication_password }}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;のようにその変数を参照するようにするという具合です。&lt;/p&gt;

&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;

&lt;p&gt;LXDを使えば複数サーバ構成のテスト環境も簡単に作れてとても便利です！&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>私のソースコードの書き方</title>
      <link>https://hnakamur.github.io/blog/2016/07/16/my-way-of-writing-source-codes/</link>
      <pubDate>Sat, 16 Jul 2016 01:37:48 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2016/07/16/my-way-of-writing-source-codes/</guid>
      <description>

&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://note.mu/ruiu/n/n1083b2a5d547&#34;&gt;ソースコードって実際のところどういうふうに書いていますか？｜Rui Ueyama｜note&lt;/a&gt; を読んで参考になるなーと思ったのですが、はてブ見ても、みんなだいたい同じですみたいなコメントばかりで面白くないので、「上手い人」では無いかもしれませんが、私の書き方をまとめてみました。&lt;/p&gt;

&lt;h2 id=&#34;ボトムアップアプローチ&#34;&gt;ボトムアップアプローチ&lt;/h2&gt;

&lt;p&gt;私はわりと新規プロジェクトで一からコードを書くことが多いです。人が書いたコードを引き継いで保守した経験はほとんど無いような気がします。&lt;/p&gt;

&lt;p&gt;私はトップダウン型で書くのが徹底的に苦手なので、常にボトムアップで書いています。
大規模なプログラムを汎用的に設計するとかは私には無理です。&lt;a href=&#34;https://ja.wikipedia.org/wiki/YAGNI&#34;&gt;YAGNI&lt;/a&gt;大好き。&lt;/p&gt;

&lt;p&gt;具体的なコードの書き方ですが、最初はドキュメントに書かれているサンプルコードをコピペして動かします。すんなり動くこともあれば、環境やライブラリのバージョン違いなどで多少手直しが必要なこともあります。&lt;/p&gt;

&lt;p&gt;次に、やりたいことに向けて1つずつ機能を追加していきます。ちょっと書いたらコンパイルしてエラーを解消します。で、すかさず動作確認します。自分で書き足すだけではなくて複数のサンプルコードをコピペして、つなぎ合わせる部分を書くこともあります。例えば&lt;a href=&#34;http://hnakamur.github.io/blog/2016/06/12/wrote_remoteworkers_go_pacakge/&#34;&gt;Goで複数のリモートのワーカーにジョブを実行させるremoteworkersというパッケージを書いた&lt;/a&gt;です。ちなみにこのパッケージは実案件では出番がなくて使ってないです。&lt;/p&gt;

&lt;p&gt;私は頭の中で全体を把握したり、先に設計してから実装とかが出来ないので、まず書いて動かしてみてから考えます。書くときもなんとなく雰囲気で書いてみることも多いです。で、動かしてみて、あ、こうなるのか、じゃあこう変えよ、という感じで理解を深めながら、コードを書き足していく感じです。&lt;/p&gt;

&lt;h2 id=&#34;gitでのバージョン管理&#34;&gt;gitでのバージョン管理&lt;/h2&gt;

&lt;p&gt;コンパイルが通ったところや、ちゃんと動くようになったところでGitでコミットしていますが、ついつい他のこともついでにやっているのでコミットの単位はきちんと分かれていないことが日常茶飯事です。きちんと整理したほうが良いだろうなと思いつつ、新規開発の時はさっさと開発をすすめるほうが優先と思って妥協しています。&lt;/p&gt;

&lt;p&gt;実際のところ、新規開発時のコミットを後で見直すことは無いので困ったことは特に無いです。一度リリースした後に保守するときは、修正ごとに極力コミットを分けるようにしています。&lt;/p&gt;

&lt;p&gt;私も一直線にゴールに向かうわけではないです。開発初期の時はmasterブランチにどんどんコミットしていますが、横道にそれてなにか試すときはブランチを切って試しています。で結局使わなかった時はmasterに戻って作業を続けて、将来だいぶ立ってから流石にもう使うことはないかと思った時点で試した時のブランチは破棄しています。&lt;/p&gt;

&lt;p&gt;ちょっとした変更を試すときはmasterのままコミットして、結局不採用だけど後でみたいかもと思うときはrevertすることもあります。&lt;/p&gt;

&lt;p&gt;masterにどんどんコミットというのは1人で開発している時で、複数人で開発するようになったらプルリクエストベースに切り替えています。一方で1人で開発している時でも一連のコミットをまとめておきたいときはプルリクエストを送って自分でマージしています。&lt;/p&gt;

&lt;h2 id=&#34;テスト&#34;&gt;テスト&lt;/h2&gt;

&lt;p&gt;動いている状態を維持しつつ機能追加していきますが、私はテストはめったに書かないです。私は3歩歩けば忘れる鳥頭なのでソースコードも書いた瞬間から忘れていきますが、再度コードを読んで何やっているか理解できればOKです。コード読んだだけでは、どうなるかよくわからないところについてはテストを書きます。&lt;/p&gt;

&lt;p&gt;例えば&lt;a href=&#34;https://github.com/hnakamur/ltsvlog&#34;&gt;hnakamur/ltsvlog&lt;/a&gt;というLTSV形式のログラブラリではログが出力されるかといったテストは一切書いてなくて、いろんな型の値がどう文字列化されるかという箇所だけテストを書いています。&lt;/p&gt;

&lt;h2 id=&#34;デバッグはログ出力派&#34;&gt;デバッグはログ出力派&lt;/h2&gt;

&lt;p&gt;デバッガはほとんど使わないです。サードパーティのアプリケーションやライブラリのコードをどこから読んでよいかわからないときに、気になる関数でブレークしてコールスタックを見たりするときとか、デバッグでたまに使う程度です。&lt;/p&gt;

&lt;p&gt;自分のプログラムでもサードパーティのプログラムでも、想定外の動きになったり、挙動がよくわからない時はデバッグログ出力のコードを追加してビルドして再度実行して、呼び出し関係や変数の値を確認します。呼び出し関係は、気になるポイントでスタックトレースを出力するデバッグログを追加します。Pythonとかでは例外を投げてすぐキャッチしてスタックトレースを出力したり、Goだと&lt;a href=&#34;https://golang.org/pkg/runtime/#Stack&#34;&gt;runtime.Stack&lt;/a&gt;でスタックトレースを出力します。&lt;/p&gt;

&lt;p&gt;今だと拙作のltsvlogパッケージの&lt;a href=&#34;https://godoc.org/github.com/hnakamur/ltsvlog#LTSVLogger.ErrorWithStack&#34;&gt;func (l *LTSVLogger) ErrorWithStack(lv &amp;hellip;LV)&lt;/a&gt;を一時的に埋め込むという手もあります。この関数はエラーを出力するためのものですけど、あくまで一時的に埋め込んで動作させて、確認したらすぐ消す感じで。&lt;/p&gt;

&lt;p&gt;デバッガだとプログラム修正後に再起動してブレークポイント指定してそこまで進んでという手間が面倒ですし、変数の中から見たい情報を見たい形に加工するのも面倒だと感じます。デバッグログなら一回仕込んでビルドすれば後はプログラム実行するだけで毎回必要な情報を出力してくれるので楽です。&lt;/p&gt;

&lt;h2 id=&#34;リファクタリング&#34;&gt;リファクタリング&lt;/h2&gt;

&lt;p&gt;リファクタリングするときに互換レイヤを作るという発想はありませんでした。1つのコミットの変更内容が大きくなり過ぎないように、変更をいくつかのステップに分けて、それぞれコミットしておくようにはしています。&lt;/p&gt;

&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;

&lt;p&gt;ということで正直ベースで書いてみました。&lt;/p&gt;

&lt;p&gt;Rui Ueyamaさんのような凄い方と比べるとレベルが低いですが、こんな人もいますよということで。この記事が他の方の話の呼び水になれば嬉しいです。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>1台のサーバに異なる設定でApache Traffic Serverを複数立ち上げるためのビルド設定</title>
      <link>https://hnakamur.github.io/blog/2016/07/02/config-for-multiple-installations-of-apache-traffic-server/</link>
      <pubDate>Sat, 02 Jul 2016 01:00:00 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2016/07/02/config-for-multiple-installations-of-apache-traffic-server/</guid>
      <description>

&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;

&lt;p&gt;Apache Traffic Serverには&lt;a href=&#34;https://docs.trafficserver.apache.org/en/latest/admin-guide/configuration/hierachical-caching.en.html&#34;&gt;Hierarchical Caching&lt;/a&gt;という機能があって、キャッシュを親と子の2階層にすることが出来ます。&lt;/p&gt;

&lt;p&gt;CentOSで1つのサーバに親と子の2つのTraffic Server 6.1.1を異なる設定で起動するような構成にしたかったのですが、本家のrpmでは出来ないようでした。
ソースを見ていたらconfigureオプションをうまく指定すれば可能だとわかり、カスタムrpmを作りました。&lt;/p&gt;

&lt;p&gt;rpmのspecファイルは&lt;a href=&#34;https://github.com/hnakamur/apache-traffic-server-rpm/blob/d1688aec09f6761841bbc638938577cae49beccd/SPECS/trafficserver.spec&#34;&gt;apache-traffic-server-rpm/trafficserver.spec&lt;/a&gt;、ビルドしたrpmは &lt;a href=&#34;https://copr.fedorainfracloud.org/coprs/hnakamur/apache-traffic-server-6/&#34;&gt;hnakamur/apache-traffic-server-6 Copr&lt;/a&gt; で公開しています。&lt;/p&gt;

&lt;h2 id=&#34;起動オプションではやりたいことは出来なさそうでした&#34;&gt;起動オプションではやりたいことは出来なさそうでした&lt;/h2&gt;

&lt;p&gt;カスタムrpmを作る前に、本家のrpmを使いつつコマンドラインオプションや環境変数の設定によってやりたいことが実現できないか調べてみたのですが、出来なさそうでした。&lt;/p&gt;

&lt;p&gt;バージョン6.1.1のソースを見た時のメモです。&lt;/p&gt;

&lt;p&gt;まず、 &lt;code&gt;traffic_server&lt;/code&gt; コマンドには &lt;code&gt;-conf_dir&lt;/code&gt; というオプションがあります。ソースは &lt;a href=&#34;https://github.com/apache/trafficserver/blob/6.1.1/proxy/Main.cc#L206&#34;&gt;proxy/Main.cc&lt;/a&gt; です。&lt;a href=&#34;https://docs.trafficserver.apache.org/en/6.1.x/appendices/command-line/traffic_server.en.html&#34;&gt;traffic_serverのドキュメント&lt;/a&gt;には記載がありません。&lt;/p&gt;

&lt;p&gt;一方、 &lt;code&gt;traffic_manager&lt;/code&gt; コマンドには &lt;code&gt;-tsArgs&lt;/code&gt; というオプションがあります。 ソースは &lt;a href=&#34;https://github.com/apache/trafficserver/blob/6.1.1/cmd/traffic_manager/traffic_manager.cc#L453&#34;&gt;cmd/traffic_manager/traffic_manager.cc&lt;/a&gt; で &lt;a href=&#34;https://docs.trafficserver.apache.org/en/6.1.x/appendices/command-line/traffic_manager.en.html#cmdoption-traffic_manager--tsArgs&#34;&gt;traffic_managerのドキュメント&lt;/a&gt; にも説明はありませんが載っています。&lt;/p&gt;

&lt;p&gt;しかし、 &lt;code&gt;traffic_cop&lt;/code&gt; コマンドが &lt;code&gt;traffic_manager&lt;/code&gt; コマンドを起動する際には &lt;code&gt;-tsArgs&lt;/code&gt; オプションは指定していません。ソースは &lt;a href=&#34;https://github.com/apache/trafficserver/blob/6.1.1/cmd/traffic_cop/traffic_cop.cc#L758&#34;&gt;cmd/traffic_cop/traffic_cop.cc&lt;/a&gt; です。 &lt;a href=&#34;https://docs.trafficserver.apache.org/en/6.1.x/appendices/command-line/traffic_cop.en.html&#34;&gt;traffic_cop&lt;/a&gt; のドキュメントを見ても traffic_manager にオプションを渡すためのオプションは無いようです。&lt;/p&gt;

&lt;p&gt;rpmでインストールされるサービス起動スクリプトだと &lt;code&gt;traffic_cop&lt;/code&gt; →　&lt;code&gt;traffic_manger&lt;/code&gt; →　&lt;code&gt;traffic_sever&lt;/code&gt; という呼び出し関係になるので、こ &lt;code&gt;traffic_server&lt;/code&gt;   に &lt;code&gt;-conf_dir&lt;/code&gt; オプションを渡すことは出来なさそうです。&lt;/p&gt;

&lt;h2 id=&#34;ts-rootという環境変数を発見&#34;&gt;TS_ROOTという環境変数を発見&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/apache/trafficserver/blob/6.1.1/lib/ts/Layout.cc#L146-L187&#34;&gt;lib/ts/Layout.cc&lt;/a&gt; で &lt;code&gt;TS_ROOT&lt;/code&gt; という環境変数を参照しているのを見つけました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Layout::Layout(const char *_prefix)
{
  if (_prefix) {
    prefix = ats_strdup(_prefix);
  } else {
    char *env_path;
    char path[PATH_NAME_MAX];
    int len;

    if ((env_path = getenv(&amp;quot;TS_ROOT&amp;quot;))) {
      len = strlen(env_path);
      if ((len + 1) &amp;gt; PATH_NAME_MAX) {
        ink_error(&amp;quot;TS_ROOT environment variable is too big: %d, max %d\n&amp;quot;, len, PATH_NAME_MAX - 1);
        return;
      }
      ink_strlcpy(path, env_path, sizeof(path));
      while (len &amp;gt; 1 &amp;amp;&amp;amp; path[len - 1] == &#39;/&#39;) {
        path[len - 1] = &#39;\0&#39;;
        --len;
      }
    } else {
      // Use compile time --prefix
      ink_strlcpy(path, TS_BUILD_PREFIX, sizeof(path));
    }

    prefix = ats_strdup(path);
  }
  exec_prefix = layout_relative(prefix, TS_BUILD_EXEC_PREFIX);
  bindir = layout_relative(prefix, TS_BUILD_BINDIR);
  sbindir = layout_relative(prefix, TS_BUILD_SBINDIR);
  sysconfdir = layout_relative(prefix, TS_BUILD_SYSCONFDIR);
  datadir = layout_relative(prefix, TS_BUILD_DATADIR);
  includedir = layout_relative(prefix, TS_BUILD_INCLUDEDIR);
  libdir = layout_relative(prefix, TS_BUILD_LIBDIR);
  libexecdir = layout_relative(prefix, TS_BUILD_LIBEXECDIR);
  localstatedir = layout_relative(prefix, TS_BUILD_LOCALSTATEDIR);
  runtimedir = layout_relative(prefix, TS_BUILD_RUNTIMEDIR);
  logdir = layout_relative(prefix, TS_BUILD_LOGDIR);
  mandir = layout_relative(prefix, TS_BUILD_MANDIR);
  infodir = layout_relative(prefix, TS_BUILD_INFODIR);
  cachedir = layout_relative(prefix, TS_BUILD_CACHEDIR);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/apache/trafficserver/blob/6.1.1/lib/ts/Layout.cc#L51-L70&#34;&gt;layout_relative関数の定義&lt;/a&gt; と &lt;a href=&#34;https://github.com/apache/trafficserver/blob/d6906e2a59858005d09018994262562b03ca24e9/lib/ts/ink_file.cc#L132-L323&#34;&gt;ink_filepath_merge関数の定義&lt;/a&gt; を見ると、 layout_relative の第2引数が &lt;code&gt;/&lt;/code&gt; で始まっていると第2引数がそのまま使われ、 &lt;code&gt;/&lt;/code&gt; で始まっていないと第1引数と第2引数を必要に応じて &lt;code&gt;/&lt;/code&gt; を挟んで連結した値になることがわかりました。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;TS_BUILD_SYSCONFDIR&lt;/code&gt; などは&lt;a href=&#34;https://github.com/apache/trafficserver/blob/6.1.1/lib/ts/ink_config.h.in#L110-L125&#34;&gt;trafficserver/ink_config.h.in&lt;/a&gt; で定義されていました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/* Various &amp;quot;build&amp;quot; defines */
#define TS_BUILD_PREFIX &amp;quot;@prefix@&amp;quot;
#define TS_BUILD_EXEC_PREFIX &amp;quot;@rel_exec_prefix@&amp;quot;
#define TS_BUILD_BINDIR &amp;quot;@rel_bindir@&amp;quot;
#define TS_BUILD_SBINDIR &amp;quot;@rel_sbindir@&amp;quot;
#define TS_BUILD_SYSCONFDIR &amp;quot;@rel_sysconfdir@&amp;quot;
#define TS_BUILD_DATADIR &amp;quot;@rel_datadir@&amp;quot;
#define TS_BUILD_INCLUDEDIR &amp;quot;@rel_includedir@&amp;quot;
#define TS_BUILD_LIBDIR &amp;quot;@rel_libdir@&amp;quot;
#define TS_BUILD_LIBEXECDIR &amp;quot;@rel_libexecdir@&amp;quot;
#define TS_BUILD_LOCALSTATEDIR &amp;quot;@rel_localstatedir@&amp;quot;
#define TS_BUILD_RUNTIMEDIR &amp;quot;@rel_runtimedir@&amp;quot;
#define TS_BUILD_LOGDIR &amp;quot;@rel_logdir@&amp;quot;
#define TS_BUILD_MANDIR &amp;quot;@rel_mandir@&amp;quot;
#define TS_BUILD_CACHEDIR &amp;quot;@rel_cachedir@&amp;quot;
#define TS_BUILD_INFODIR &amp;quot;@rel_infodir@&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;rel_*&lt;/code&gt; という値は &lt;code&gt;configure&lt;/code&gt; 実行時にbuild/common.m4の &lt;a href=&#34;https://github.com/apache/trafficserver/blob/5a0952b01d01ef927a65fc44bac5f68c345747aa/build/common.m4#L252-L263&#34;&gt;TS_SUBST_LAYOUT_PATH&lt;/a&gt; で設定されるようです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dnl
dnl TS_SUBST_LAYOUT_PATH
dnl Export (via TS_SUBST) the various path-related variables that
dnl trafficserver will use while generating scripts and
dnl the default config file.
AC_DEFUN([TS_SUBST_LAYOUT_PATH], [
  TS_EXPAND_VAR(exp_$1, [$]$1)
  TS_PATH_RELATIVE(rel_$1, [$]exp_$1, ${prefix})
  TS_SUBST(exp_$1)
  TS_SUBST(rel_$1)
  TS_SUBST($1)
])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ここから呼ばれる &lt;a href=&#34;https://github.com/apache/trafficserver/blob/5a0952b01d01ef927a65fc44bac5f68c345747aa/build/common.m4#L223-L241&#34;&gt;TS_PATH_RELATIVE&lt;/a&gt; で実際の値が作られます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dnl
dnl Removes the value of $3 from the string in $2, strips of any leading
dnl slashes, and returns the value in $1.
dnl
dnl Example:
dnl orig_path=&amp;quot;${prefix}/bar&amp;quot;
dnl TS_PATH_RELATIVE(final_path, $orig_path, $prefix)
dnl    $final_path now contains &amp;quot;bar&amp;quot;
AC_DEFUN([TS_PATH_RELATIVE], [
ats_stripped=`echo $2 | sed -e &amp;quot;s#^$3##&amp;quot;`
# check if the stripping was successful
if test &amp;quot;x$2&amp;quot; != &amp;quot;x${ats_stripped}&amp;quot;; then
# it was, so strip of any leading slashes
    $1=&amp;quot;`echo ${ats_stripped} | sed -e &#39;s#^/*##&#39;`&amp;quot;
else
# it wasn&#39;t so return the original
    $1=&amp;quot;$2&amp;quot;
fi
])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ということで、例えば &lt;code&gt;sysconfdir&lt;/code&gt; の値が &lt;code&gt;prefix&lt;/code&gt; の値で始まっていれば &lt;code&gt;rel_sysconfdir&lt;/code&gt; は &lt;code&gt;prefix&lt;/code&gt; からの相対パスになり、そうでなければ &lt;code&gt;sysconfdir&lt;/code&gt; そのままになるということがわかりました。&lt;/p&gt;

&lt;h2 id=&#34;configureオプションの指定方法&#34;&gt;configureオプションの指定方法&lt;/h2&gt;

&lt;p&gt;上記を踏まえて、私が作成した &lt;a href=&#34;https://github.com/hnakamur/apache-traffic-server-rpm/blob/d1688aec09f6761841bbc638938577cae49beccd/SPECS/trafficserver.spec&#34;&gt;/trafficserver.spec&lt;/a&gt; では &lt;a href=&#34;https://github.com/hnakamur/apache-traffic-server-rpm/blob/d1688aec09f6761841bbc638938577cae49beccd/SPECS/trafficserver.spec#L1&#34;&gt;1行目&lt;/a&gt;で&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;%define _prefix /opt/trafficserver
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;と設定し、 &lt;a href=&#34;https://github.com/hnakamur/apache-traffic-server-rpm/blob/d1688aec09f6761841bbc638938577cae49beccd/SPECS/trafficserver.spec#L85-L94&#34;&gt;85〜94行目&lt;/a&gt; で以下のような configure オプションを指定しています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;%configure \
  --enable-layout=opt \
  --sysconfdir=%{_prefix}%{_sysconfdir} \
  --localstatedir=%{_prefix}%{_localstatedir} \
  --libexecdir=%{_prefix}/%{_lib}/plugins \
  --with-tcl=/usr/%{_lib} \
  --enable-luajit \
  --with-user=ats --with-group=ats \
  --disable-silent-rules \
  --enable-experimental-plugins
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これでビルドしたtrafficserverを実行する際に、環境変数TS_ROOTを設定することで以下のようなディレクトリを参照することが出来ました。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;sysconfdir: ${TS_ROOT}/etc&lt;/li&gt;
&lt;li&gt;localstatedir: ${TS_ROOT}/var/run&lt;/li&gt;
&lt;li&gt;libexecdir: ${TS_ROOT}/lib64/plugins&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;私が使っているディレクトリ構成&#34;&gt;私が使っているディレクトリ構成&lt;/h2&gt;

&lt;p&gt;実際には以下のようなシンボリックリンクを貼って使っています。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1段目

&lt;ul&gt;
&lt;li&gt;/opt/trafficserver-first/etc -&amp;gt; /etc/trafficserver-first&lt;/li&gt;
&lt;li&gt;/opt/trafficserver-first/bin -&amp;gt; /opt/trafficserver/bin&lt;/li&gt;
&lt;li&gt;/opt/trafficserver-first/lib64 -&amp;gt; /opt/trafficserver/lib64&lt;/li&gt;
&lt;li&gt;/opt/trafficserver-first/var/cache -&amp;gt; /var/cache/trafficserver-first&lt;/li&gt;
&lt;li&gt;/opt/trafficserver-first/var/logs -&amp;gt; /var/log/trafficserver-first&lt;/li&gt;
&lt;li&gt;/opt/trafficserver-first/var/run -&amp;gt; /var/run/trafficserver-first&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2段目

&lt;ul&gt;
&lt;li&gt;/opt/trafficserver-second/etc -&amp;gt; /etc/trafficserver-second&lt;/li&gt;
&lt;li&gt;/opt/trafficserver-second/bin -&amp;gt; /opt/trafficserver/bin&lt;/li&gt;
&lt;li&gt;/opt/trafficserver-second/lib64 -&amp;gt; /opt/trafficserver/lib64&lt;/li&gt;
&lt;li&gt;/opt/trafficserver-second/var/cache -&amp;gt; /var/cache/trafficserver-second&lt;/li&gt;
&lt;li&gt;/opt/trafficserver-second/var/logs -&amp;gt; /var/log/trafficserver-second&lt;/li&gt;
&lt;li&gt;/opt/trafficserver-second/var/run -&amp;gt; /var/run/trafficserver-second&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;コマンド実行時の環境変数指定&#34;&gt;コマンド実行時の環境変数指定&lt;/h2&gt;

&lt;p&gt;コマンドを実行するときはPATHを通すかフルパスで指定するだけではなく、 TS_ROOT 環境変数も指定する必要があります。&lt;/p&gt;

&lt;p&gt;例えば、1段目のキャッシュを全クリアするときは &lt;a href=&#34;https://docs.trafficserver.apache.org/en/6.1.x/admin-guide/storage/index.en.html#clearing-the-cache&#34;&gt;Clearing the Cache&lt;/a&gt; の説明では &lt;code&gt;traffic_server -Cclear&lt;/code&gt; ですが、このrpmの場合は&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TS_ROOT=/opt/trafficserver-first /opt/trafficserver-first/bin/traffic_server -Cclear
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;と実行する必要があります。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>lxd_containerというAnsibleモジュールを書いたときに学んだtips</title>
      <link>https://hnakamur.github.io/blog/2016/07/01/tips_for_writing_ansible_module/</link>
      <pubDate>Fri, 01 Jul 2016 22:44:12 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2016/07/01/tips_for_writing_ansible_module/</guid>
      <description>

&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;

&lt;p&gt;lxd_containerというAnsibleのモジュールを書いたときに学んだtipsのメモです。&lt;/p&gt;

&lt;h2 id=&#34;モジュールでデバッグ出力は出来ないのでデバッグ情報は戻り値のjsonに入れる&#34;&gt;モジュールでデバッグ出力は出来ないのでデバッグ情報は戻り値のJSONに入れる&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://groups.google.com/d/msg/ansible-devel/s0iSb7phnqY/UB9vaLFJAwAJ&#34;&gt;ansible-dev MLでの投稿&lt;/a&gt;によるとモジュールは何も出力できないとのことなので、デバッグ情報は戻り値のJSONに入れる必要があります。&lt;/p&gt;

&lt;p&gt;Ansible 2.1からはAnsibleModuleクラスでは &lt;code&gt;_verbosity&lt;/code&gt;、それ以外では &lt;code&gt;_ansible_verbosity&lt;/code&gt; で &lt;code&gt;-v&lt;/code&gt;, &lt;code&gt;-vv&lt;/code&gt;, &lt;code&gt;-vvv&lt;/code&gt;, &lt;code&gt;-vvvv&lt;/code&gt; を指定した場合の &lt;code&gt;v&lt;/code&gt; の個数が取得できるので、それに応じて戻り値のJSONにデバッグ情報を含めるかどうか制御することが出来ます。値は &lt;code&gt;-v&lt;/code&gt; を指定しない場合は 0 で、 &lt;code&gt;-vvvv&lt;/code&gt; だと4という感じです。&lt;/p&gt;

&lt;h2 id=&#34;コードフォーマットのチェック&#34;&gt;コードフォーマットのチェック&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/ansible/ansible-modules-extras/pull/2208#discussion_r62996064&#34;&gt;Ansibleのコミッタの方からのコメント&lt;/a&gt; で &lt;code&gt;pep8&lt;/code&gt; というツールでコードフォーマットのチェックを行っているということを知りました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pep8 -r --ignore=E501,E221,W291,W391,E302,E251,E203,W293,E231,E303,E201,E225,E261,E241,E402 *.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;という感じで使います。 pep8はUbuntu 16.04 では &lt;code&gt;sudo apt install pep8&lt;/code&gt; でインストールできました。&lt;/p&gt;

&lt;h2 id=&#34;ansibleモジュールのチェック&#34;&gt;Ansibleモジュールのチェック&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/ansible/ansible-modules-extras&#34;&gt;ansible/ansible-modules-extras&lt;/a&gt; にプルリクエストを送ると Travis CI でチェックが走るのですが、そのチェックの1つで &lt;code&gt;ansible-validate-modules&lt;/code&gt; というコマンドが使われていました。&lt;/p&gt;

&lt;p&gt;いろいろチェックしているようなのですが、例えばモジュール内にYAMLで書いたドキュメントの書式が間違っていると &lt;code&gt;ansible-validate-modules&lt;/code&gt; エラーになりました。コミットをプッシュしてからエラーになると面倒なのでローカルで先にチェックしておくのが良いです。&lt;/p&gt;

&lt;p&gt;私はPythonのvirtualenv環境内で &lt;code&gt;pip install ansible-testing&lt;/code&gt; でインストールしました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ansible-validate-modules 対象ディレクトリ
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;でチェックできます。&lt;/p&gt;

&lt;h2 id=&#34;サードパーティのrequestsを使うとansible-module-utils-urlsを使うべきというエラーが出る&#34;&gt;サードパーティのrequestsを使うとansible.module_utils.urlsを使うべきというエラーが出る&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://docs.python-requests.org/en/master/&#34;&gt;Requests: HTTP for Humans&lt;/a&gt;を使っているとansible-validate-modulesが &lt;code&gt;ansible.module_utils.urls&lt;/code&gt; を使うべきという&lt;a href=&#34;https://github.com/ansible/ansible-modules-extras/pull/2208#issuecomment-228027653&#34;&gt;エラーを出してきます&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;今回書いたlxd_containerモジュールは&lt;a href=&#34;https://github.com/lxc/lxd/blob/master/doc/rest-api.md&#34;&gt;LXD REST API&lt;/a&gt;を使うのですが (1) Unixドメインソケットでの通信、(2) クライアント証明書を使ったhttps通信の2つが必要です。が &lt;code&gt;ansible.module_utils.urls&lt;/code&gt; での実現方法がわからなかったので、今回はPython2標準ライブラリのhttplibを使って実装しました。&lt;/p&gt;

&lt;p&gt;サードパーティのライブラリを使わず標準ライブラリを使うことで、lxd_containerモジュールを使うときに依存するライブラリを入れる手間が発生しないので結果的には良かったと思います。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>