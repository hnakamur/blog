<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on hnakamur&#39;s blog at github</title>
    <link>https://hnakamur.github.io/blog/post/index.xml</link>
    <description>Recent content in Posts on hnakamur&#39;s blog at github</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja-JP</language>
    <lastBuildDate>Sun, 12 Feb 2017 23:00:03 +0900</lastBuildDate>
    <atom:link href="https://hnakamur.github.io/blog/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Facebookの時系列データベースGorillaのデータ圧縮方式を試してみた</title>
      <link>https://hnakamur.github.io/blog/2017/02/12/tried-facebook-gorilla-time-series-database-compression/</link>
      <pubDate>Sun, 12 Feb 2017 23:00:03 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2017/02/12/tried-facebook-gorilla-time-series-database-compression/</guid>
      <description>

&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://code.facebook.com/posts/952820474848503/beringei-a-high-performance-time-series-storage-engine/&#34;&gt;Beringei: A high-performance time series storage engine | Engineering Blog | Facebook Code&lt;/a&gt; という記事を読んで、Facebookが2015年に &lt;a href=&#34;http://www.vldb.org/pvldb/vol8/p1816-teller.pdf&#34;&gt;&amp;ldquo;Gorilla: A Fast, Scalable, In-Memory Time Series Database&amp;rdquo;&lt;/a&gt; という論文でGorillaという時系列データベースについて発表したものを&lt;a href=&#34;https://github.com/facebookincubator/beringei&#34;&gt;Beringei&lt;/a&gt;としてオープンソースで公開したのを知りました。&lt;/p&gt;

&lt;p&gt;この論文は読んだことがなかったので読んでみたのですが、時系列データベースのデータの特徴をうまく活かした独自の圧縮方法が興味深かったので、自分でも試してみたのでメモです。&lt;/p&gt;

&lt;p&gt;Gorillaでは高い圧縮率によってデータをオンメモリで扱うことができるようになり、書き込みと問い合わせの速度がそれまで使っていたディスクベースの時系列と比べて飛躍的に改善したそうです。&lt;/p&gt;

&lt;p&gt;Gorillaもディスクに書き出して永続化は行うのですが、RDBのようなACIDのトランザクションは持たず障害発生時には数秒程度のデータは消失するおそれがあるという割り切った設計にしているそうです。その代わり書き込みが高速に行えるというのが利点です。&lt;/p&gt;

&lt;h2 id=&#34;サードパーティの実装&#34;&gt;サードパーティの実装&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/facebookincubator/beringei&#34;&gt;Beringei&lt;/a&gt;は C++ で書かれていて、ライセンスは3項BSDですが、最近のFacebookのOSSでは定番の&lt;a href=&#34;https://github.com/facebookincubator/beringei/blob/master/PATENTS&#34;&gt;PATENTS&lt;/a&gt;ファイルがあります。&lt;/p&gt;

&lt;p&gt;Goの実装はないかと調べてみると、&lt;a href=&#34;https://github.com/dgryski/go-tsz&#34;&gt;dgryski/go-tsz: Time series compression algorithm from Facebook&amp;rsquo;s Gorilla paper&lt;/a&gt; というサードパーティの実装がありました。が、ライセンスが明記されていないので、私のポリシーとしてはソースコードを参照するわけにはいきません。 &lt;a href=&#34;https://github.com/dgryski/go-tsz/issues/18&#34;&gt;Add a license · Issue #18 · dgryski/go-tsz&lt;/a&gt; というイシューはあるのですが昨年9月から放置状態になっています。私もコメントしてみたのですがまだ反応はないです。また、 &lt;a href=&#34;https://godoc.org/github.com/dgryski/go-tsz&#34;&gt;dgryski/go-tszのGoDoc&lt;/a&gt;は見てみたのですが、私が期待するAPIとはちょっと違う感じでした。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;[]byte&lt;/code&gt; とデータを相互変換するMarshal, Unmarshalとか、ストリームと相互変換するEncoder, Decoderが欲しいところです。&lt;/p&gt;

&lt;p&gt;さらに調べてみると &lt;a href=&#34;https://github.com/burmanm/gorilla-tsc/&#34;&gt;burmanm/gorilla-tsc: Implementation of time series compression method from the Facebook&amp;rsquo;s Gorilla paper&lt;/a&gt; というJavaのサードパーティの実装がありました。こちらはありがたいことにApache 2ライセンスです。ということで、このコードを参考にして、自分で実装してみました。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/hnakamur/timeseries&#34;&gt;hnakamur/timeseries: a Go package for encoding and decoding time-series data point in similar way to Facebook Gorilla time-series database&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;例によって雰囲気で実装しているので、uint32とuint64などに入れたビット列をとint64などに相互変換しているあたりなどは特にバグがある可能性があります。むやみに信用せず疑いの目で見てください。&lt;/p&gt;

&lt;p&gt;ビットストリームは、&lt;a href=&#34;https://github.com/dgryski/go-tsz&#34;&gt;dgryski/go-tsz&lt;/a&gt;と同じ作者の方の &lt;a href=&#34;https://github.com/dgryski/go-bitstream&#34;&gt;dgryski/go-bitstream: go-bitstream: read and write bits from io.Reader and io.Writer&lt;/a&gt; を使わせていただいています。こちらはMITライセンスです。&lt;/p&gt;

&lt;h2 id=&#34;試してみて気づいたこと&#34;&gt;試してみて気づいたこと&lt;/h2&gt;

&lt;h3 id=&#34;高い圧縮率を保つためには時刻の精度はミリ秒ではなく秒が良い&#34;&gt;高い圧縮率を保つためには時刻の精度はミリ秒ではなく秒が良い&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://www.vldb.org/pvldb/vol8/p1816-teller.pdf&#34;&gt;&amp;ldquo;Gorilla: A Fast, Scalable, In-Memory Time Series Database&amp;rdquo;&lt;/a&gt; の &amp;ldquo;4.1.1 Compressing time stamps&amp;rdquo; でデータポイントの時刻の圧縮について説明されています。&lt;/p&gt;

&lt;p&gt;時刻の差分の差分 (delta of delta) をなるべくビット数が少なくなるような独自の方式でエンコードするようになっています。&lt;/p&gt;

&lt;p&gt;モニタリングは60秒毎のように一定の間隔で行うことが多いので、差分の差分であれば、ほぼ常に0になります。Gorillaのエンコード方式では0は1ビットの0で表すので、0が多いとデータサイズが小さくて済みます。&lt;/p&gt;

&lt;p&gt;多少ずれて間隔が 59秒, 61秒のようになったとしても、差分の差分は-1, 1と絶対値が小さい数値になり、0のように1ビットとは行きませんが、絶対値が大きい数値よりは少ないビット数で済みます。&lt;/p&gt;

&lt;p&gt;一方 &lt;a href=&#34;https://github.com/burmanm/gorilla-tsc/blob/fb984aefffb63c7b4d48c526f69db53813df2f28/src/main/java/fi/iki/yak/ts/compression/gorilla/Compressor.java#L90&#34;&gt;https://github.com/burmanm/gorilla-tsc/blob/fb984aefffb63c7b4d48c526f69db53813df2f28/src/main/java/fi/iki/yak/ts/compression/gorilla/Compressor.java#L90&lt;/a&gt; のコメントにあるように時刻をミリ秒の精度にすると圧縮には良くないです。ミリ秒にすると各データ点の時刻のミリ秒部分はばらつきがあり等間隔にならないので、差分の差分の数値の桁数が増え、エンコードしてもビット数が多くなってしまうからです。&lt;/p&gt;

&lt;h3 id=&#34;小数の値が増えると圧縮率は下がる&#34;&gt;小数の値が増えると圧縮率は下がる&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://www.vldb.org/pvldb/vol8/p1816-teller.pdf&#34;&gt;&amp;ldquo;Gorilla: A Fast, Scalable, In-Memory Time Series Database&amp;rdquo;&lt;/a&gt; の &amp;ldquo;4.1.2 Compressing values&amp;rdquo; でデータポイントの値の圧縮について説明されています。&lt;/p&gt;

&lt;p&gt;各値を浮動小数点数の64ビット列に変換して1つ前のデータ点とのXORをとるようにしています。全く同じ値の場合は0になるので、エンコードすると1ビットの0で済みます。&lt;/p&gt;

&lt;p&gt;またXORの結果を毎回64ビットで記録するのではなく、先頭からのビットで0が続く部分 (LeadingZeros) と終端からのビットで0が続く部分 (TrailingZeros) は、それらのビット数をエンコードし、残りのビット列を記録するようにしています。&lt;/p&gt;

&lt;p&gt;さらに、1つ前の値の LeadingZeros と TrailingZeros の桁数よりも多い場合は、そのままにして残りのビット列のみ記録するようになっています。&lt;/p&gt;

&lt;p&gt;そうでない場合は新しい LeadingZeros と TrailingZeros の値と残りのビット列を記録します。&lt;/p&gt;

&lt;p&gt;このエンコーディング方式は、値が12.0や12.5など浮動小数点数の仮数部の途中から最後まで0のビットが多く続く場合は、少ないビット数で済みます。が、0.1 のような数だと仮数部の多くのビットが0でないため、LeadingZerosとTrailingZerosの値が小さくなり、残りのビット列を記録するのに、多くのビット数を消費してしまいます。&lt;/p&gt;

&lt;h3 id=&#34;時刻が等間隔で-同じ値が続く場合は高圧縮率になる&#34;&gt;時刻が等間隔で、同じ値が続く場合は高圧縮率になる&lt;/h3&gt;

&lt;p&gt;上に書いたように、圧縮率が悪くなるケースもあります。ですが、時刻が等間隔で、同じ値が続く場合は1つのデータポイントで時刻で1ビット、値で1ビットの2ビットで済むというのは凄いと思いました。&lt;/p&gt;

&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;

&lt;p&gt;時系列データベースの特性を考慮して、典型的なデータで高圧縮率を実現していることがわかりました。一方で、圧縮率が悪くなるケースについても理解できました。&lt;/p&gt;

&lt;p&gt;また、エンコード方式以外にも&lt;a href=&#34;http://www.vldb.org/pvldb/vol8/p1816-teller.pdf&#34;&gt;&amp;ldquo;Gorilla: A Fast, Scalable, In-Memory Time Series Database&amp;rdquo;&lt;/a&gt; の &amp;ldquo;4.3 On disk structures&amp;rdquo; にはディスク上のレイアウトについて、 &amp;ldquo;4.4 Handling failures&amp;rdquo; には障害発生時に対応についてそれぞれ書かれていて、こちらも興味深いです。時系列データベースに興味のある方は、一読をお勧めします。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>さくらのクラウドでPackerとTerraformを使ってContainer Linuxの環境構築をしてみた</title>
      <link>https://hnakamur.github.io/blog/2017/01/02/use-container-linux-on-sakura-cloud-using-packer-and-terraform/</link>
      <pubDate>Mon, 02 Jan 2017 15:34:23 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2017/01/02/use-container-linux-on-sakura-cloud-using-packer-and-terraform/</guid>
      <description>

&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;

&lt;p&gt;さくらのクラウドでPackerとTerraformを使って&lt;a href=&#34;https://coreos.com/os/docs/latest/&#34;&gt;CoreOS Container Linux&lt;/a&gt;の環境構築をしてみたのでメモです。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://cloud-news.sakura.ad.jp/public_archive_iso/&#34;&gt;パブリックアーカイブ・ISOイメージ&lt;/a&gt;にCoreOSはあるのですが、現状では残念ながらバージョンが 367.1.0 (stable) とかなり古い状態です。&lt;/p&gt;

&lt;p&gt;そこで &lt;a href=&#34;https://stable.release.core-os.net/amd64-usr/&#34;&gt;https://stable.release.core-os.net/amd64-usr/&lt;/a&gt; 以下にある安定版公式ISOイメージの現時点の最新版である 1185.5.0 を使ってPackerでさくらのクラウド上にマイアーカイブを作成し、それを元にサーバで使用するディスクとサーバを作成します。&lt;/p&gt;

&lt;p&gt;さくらのクラウドには&lt;a href=&#34;http://cloud-news.sakura.ad.jp/startup-script/&#34;&gt;スタートアップスクリプト&lt;/a&gt;という機能がありサーバの起動時に設定を行うことができるのですが、これが使えるのはCentOS、Debian、Ubuntuに限定されるようでCoreOSでは使えませんでした。&lt;/p&gt;

&lt;p&gt;これだと構成はほぼ同じで静的IPアドレスだけが異なる複数のサーバを作りたい場合も、サーバ1台毎にPackerでマイアーカイブを作ってそこからサーバを作る必要があり、実用には厳しいなと思って一度は断念していました。&lt;/p&gt;

&lt;p&gt;ルータを使わない構成であれば、まずはDHCPで起動してアドレスをもらってからプロビジョニング時に静的IPアドレスに切り替えるという手はあります。ですがルータを使う場合はDHCPサーバがいないのでこの手は使えません。&lt;/p&gt;

&lt;p&gt;そんな時、&lt;a href=&#34;https://github.com/sacloud/packer-builder-sakuracloud&#34;&gt;さくらのクラウド用Packerプラグイン&lt;/a&gt;、&lt;a href=&#34;https://github.com/yamamoto-febc/terraform-provider-sakuracloud&#34;&gt;Terraform for さくらのクラウド&lt;/a&gt;、&lt;a href=&#34;https://github.com/yamamoto-febc/sacloud-upload-image&#34;&gt;Upload ISO image to SAKURA CLOUD&lt;/a&gt;などの便利なツールを作ってくださっている山本さんのツイートでContainer Linuxの&lt;a href=&#34;https://coreos.com/os/docs/latest/config-drive.html&#34;&gt;Customize with Config-Drive&lt;/a&gt;という機能を知りました。便利なツールに加えて有用な情報、いつもありがとうございます！&lt;/p&gt;

&lt;p&gt;この記事はこの機能と上記の3つのツールを使ってContainer Linuxの環境構築をしてみたメモです。&lt;/p&gt;

&lt;h2 id=&#34;参考資料&#34;&gt;参考資料&lt;/h2&gt;

&lt;h3 id=&#34;packer-for-さくらのクラウド&#34;&gt;Packer for さくらのクラウド&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/sacloud/packer-builder-sakuracloud&#34;&gt;さくらのクラウド用Packerプラグイン&lt;/a&gt;の作者の山本さんによる詳しくてわかりやすい記事が
&lt;a href=&#34;http://knowledge.sakura.ad.jp/knowledge/6790/&#34;&gt;さくらのクラウド上でマシンイメージを自動構築 〜「Packer for さくらのクラウド」 - さくらのナレッジ&lt;/a&gt; にありますので、ぜひご参照ください。&lt;/p&gt;

&lt;h3 id=&#34;terraform-for-さくらのクラウド&#34;&gt;Terraform for さくらのクラウド&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/yamamoto-febc/terraform-provider-sakuracloud&#34;&gt;Terraform for さくらのクラウド&lt;/a&gt;については作者の山本さんによる詳しくてわかりやすい連載記事がありますので、是非そちらをご参照ください。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://qiita.com/yamamoto-febc/items/ae92cd258cf040957487&#34;&gt;Terraform for さくらのクラウド スタートガイド(第1回) - Qiita&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://qiita.com/yamamoto-febc/items/2480b11c9e6a8b64f78d&#34;&gt;Terraform for さくらのクラウド スタートガイド(第2回) - Qiita&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://qiita.com/yamamoto-febc/items/fe954e2d4a92b864cfef&#34;&gt;Terraform for さくらのクラウド スタートガイド(第3回) - Qiita&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://qiita.com/yamamoto-febc/items/a9795cb909bd9b69f729&#34;&gt;Terraform for さくらのクラウド スタートガイド(第4回) - Qiita&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://qiita.com/yamamoto-febc/items/4b774404e041fa05688a&#34;&gt;Terraform for さくらのクラウド スタートガイド(第5回) - Qiita&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;container-linuxのisoイメージ作成&#34;&gt;Container LinuxのISOイメージ作成&lt;/h2&gt;

&lt;h3 id=&#34;packerとさくらのクラウド用packerプラグインの事前準備&#34;&gt;Packerとさくらのクラウド用Packerプラグインの事前準備&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.packer.io/&#34;&gt;Packer by HashiCorp&lt;/a&gt;と&lt;a href=&#34;https://github.com/sacloud/packer-builder-sakuracloud&#34;&gt;さくらのクラウド用Packerプラグイン&lt;/a&gt;をインストールしていない場合はそれぞれのドキュメントに従ってインストールしてください。&lt;/p&gt;

&lt;p&gt;また&lt;a href=&#34;https://github.com/yamamoto-febc/terraform-provider-sakuracloud/blob/master/docs/installation.md#%E3%81%95%E3%81%8F%E3%82%89%E3%81%AE%E3%82%AF%E3%83%A9%E3%82%A6%E3%83%89api%E3%82%AD%E3%83%BC%E3%81%AE%E5%8F%96%E5%BE%97&#34;&gt;APIキーの取得&lt;/a&gt;と&lt;a href=&#34;https://github.com/sacloud/packer-builder-sakuracloud#apiキーの設定&#34;&gt;APIキーの設定&lt;/a&gt;も行っておいてください。&lt;/p&gt;

&lt;h3 id=&#34;packerでさくらのクラウドにcontainer-linuxのマイアーカイブを作成&#34;&gt;PackerでさくらのクラウドにContainer Linuxのマイアーカイブを作成&lt;/h3&gt;

&lt;p&gt;以下の内容を &lt;code&gt;containerlinux.json&lt;/code&gt; というファイルに保存します。
「ここにパスワードを設定」にはContainer Linuxで予め用意されている &lt;code&gt;core&lt;/code&gt; ユーザに設定するパスワードを設定します。
「ここにパスワードのハッシュを設定」には &lt;a href=&#34;https://github.com/coreos/coreos-cloudinit/blob/master/Documentation/cloud-config.md#generating-a-password-hash&#34;&gt;Generating a password hash&lt;/a&gt; の手順で生成したパスワードのハッシュを設定します。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;sakuracloud_zone&lt;/code&gt; は&lt;a href=&#34;http://developer.sakura.ad.jp/cloud/api/1.1/&#34;&gt;さくらのクラウド API v1.1 ドキュメント&lt;/a&gt;の一般注記事項のAPI URLに書いてあるゾーンのうち、自分が利用したいゾーンを指定します。以下の例では &lt;code&gt;is1b&lt;/code&gt; (石狩第2ゾーン)としています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;variables&amp;quot;: {
    &amp;quot;sakuracloud_zone&amp;quot;: &amp;quot;is1b&amp;quot;,
    &amp;quot;archive_name&amp;quot;: &amp;quot;CoreOS 1185.5.0&amp;quot;,
    &amp;quot;iso_url&amp;quot;: &amp;quot;https://stable.release.core-os.net/amd64-usr/1185.5.0/coreos_production_iso_image.iso&amp;quot;,
    &amp;quot;iso_checksum&amp;quot;: &amp;quot;1c8e7948bdc54980df87a9a2b08fa744104f977950002f1605b60bf44d2021b9&amp;quot;,
    &amp;quot;iso_checksum_type&amp;quot;: &amp;quot;sha256&amp;quot;,
    &amp;quot;install_disk_device&amp;quot;: &amp;quot;/dev/vda&amp;quot;,
    &amp;quot;tmp_password&amp;quot;: &amp;quot;ここにパスワードを設定&amp;quot;,
    &amp;quot;tmp_password_hash&amp;quot;: &amp;quot;ここにパスワードのハッシュを設定&amp;quot;
  },
  &amp;quot;builders&amp;quot;: [{
    &amp;quot;type&amp;quot;: &amp;quot;sakuracloud&amp;quot;,
    &amp;quot;zone&amp;quot;: &amp;quot;{{user `sakuracloud_zone`}}&amp;quot;,
    &amp;quot;os_type&amp;quot;: &amp;quot;iso&amp;quot;,
    &amp;quot;iso_url&amp;quot;: &amp;quot;{{user `iso_url`}}&amp;quot;,
    &amp;quot;iso_checksum&amp;quot;: &amp;quot;{{user `iso_checksum`}}&amp;quot;,
    &amp;quot;iso_checksum_type&amp;quot;: &amp;quot;{{user `iso_checksum_type`}}&amp;quot;,
    &amp;quot;us_keyboard&amp;quot;: true,
    &amp;quot;boot_wait&amp;quot;: &amp;quot;20s&amp;quot;,
    &amp;quot;boot_command&amp;quot;: [
      &amp;quot;cat &amp;lt;&amp;lt;&#39;EOF&#39; &amp;gt; /tmp/cloud-config.yml&amp;lt;enter&amp;gt;&amp;quot;,
      &amp;quot;#cloud-config&amp;lt;enter&amp;gt;&amp;quot;,
      &amp;quot;users:&amp;lt;enter&amp;gt;&amp;quot;,
      &amp;quot;  - name: core&amp;lt;enter&amp;gt;&amp;quot;,
      &amp;quot;    passwd: {{user `tmp_password_hash`}}&amp;lt;enter&amp;gt;&amp;quot;,
      &amp;quot;EOF&amp;lt;enter&amp;gt;&amp;lt;wait&amp;gt;&amp;quot;,
      &amp;quot;sudo coreos-install -c /tmp/cloud-config.yml -d {{user `install_disk_device`}}&amp;lt;enter&amp;gt;&amp;lt;wait&amp;gt;&amp;quot;,
      &amp;quot;&amp;lt;wait10&amp;gt;&amp;lt;wait10&amp;gt;&amp;lt;wait10&amp;gt;&amp;lt;wait10&amp;gt;&amp;lt;wait10&amp;gt;&amp;lt;wait10&amp;gt;&amp;quot;,
      &amp;quot;&amp;lt;wait10&amp;gt;&amp;lt;wait10&amp;gt;&amp;lt;wait10&amp;gt;&amp;quot;,
      &amp;quot;reboot&amp;lt;enter&amp;gt;&amp;lt;wait&amp;gt;&amp;quot;
    ],
    &amp;quot;user_name&amp;quot;: &amp;quot;core&amp;quot;,
    &amp;quot;password&amp;quot;: &amp;quot;{{user `tmp_password`}}&amp;quot;,
    &amp;quot;archive_name&amp;quot;: &amp;quot;{{user `archive_name`}}&amp;quot;,
    &amp;quot;archive_tags&amp;quot;: [&amp;quot;@size-extendable&amp;quot;, &amp;quot;current-stable&amp;quot;, &amp;quot;arch-64bit&amp;quot;, &amp;quot;distro-containerlinux&amp;quot;]
  }],
  &amp;quot;provisioners&amp;quot;: [{
    &amp;quot;type&amp;quot;: &amp;quot;shell&amp;quot;,
    &amp;quot;inline&amp;quot;: [
      &amp;quot;sudo passwd -d core&amp;quot;
    ],
    &amp;quot;pause_before&amp;quot;: &amp;quot;20s&amp;quot;
  }]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下のコマンドを実行すると、一時的にサーバを作ってISOイメージからインストールし、その後シャットダウンしてマイアーカイブを作るという一連の処理を行ってくれます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;packer build containerlinux.json
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;terraform-for-さくらのクラウドでまずルータだけ作成&#34;&gt;Terraform for さくらのクラウドでまずルータだけ作成&lt;/h2&gt;

&lt;h3 id=&#34;terraform-for-さくらのクラウドの事前準備&#34;&gt;Terraform for さくらのクラウドの事前準備&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.terraform.io/&#34;&gt;Terraform&lt;/a&gt;と&lt;a href=&#34;https://github.com/yamamoto-febc/terraform-provider-sakuracloud&#34;&gt;Terraform for さくらのクラウド&lt;/a&gt;をインストールしていない場合は、それぞれのドキュメントに従ってインストールしてください。&lt;/p&gt;

&lt;p&gt;またAPIキーと利用したいゾーンの設定も必要です。&lt;/p&gt;

&lt;p&gt;APIキーの設定は&lt;a href=&#34;https://github.com/sacloud/packer-builder-sakuracloud&#34;&gt;さくらのクラウド用Packerプラグイン&lt;/a&gt;で行ったものと同じなので、ゾーンの設定を追加で行う必要があります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export SAKURACLOUD_ACCESS_TOKEN=[APIトークン]
$ export SAKURACLOUD_ACCESS_TOKEN_SECRET=[APIシークレット]
$ export SAKURACLOUD_ZONE=is1b
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;terraform-for-さくらのクラウドでルータを作成&#34;&gt;Terraform for さくらのクラウドでルータを作成&lt;/h3&gt;

&lt;p&gt;Terraformを使うなら本来は1つのtfファイルでルータとサーバを一気に作成したいところなのですが、サーバ1台毎の設定ファイルを含むISOイメージを作る部分をTerraform外のスクリプトで作成する都合上、2ステップに分ける必要があります。&lt;/p&gt;

&lt;p&gt;まずは以下の内容を &lt;code&gt;server.tf&lt;/code&gt; というファイルに保存します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;resource &amp;quot;sakuracloud_internet&amp;quot; &amp;quot;router01&amp;quot; {
    name = &amp;quot;router01&amp;quot;
    description = &amp;quot;by Terraform&amp;quot;
    tags = [&amp;quot;Terraform&amp;quot;]
    nw_mask_len = 28
    band_width = 100
}

output &amp;quot;router01_ipaddress&amp;quot; {
    value = &amp;quot;${sakuracloud_internet.router01.nw_address}&amp;quot;
}

output &amp;quot;router01_gateway&amp;quot; {
    value = &amp;quot;${sakuracloud_internet.router01.nw_gateway}&amp;quot;
}

output &amp;quot;router01_min_ipaddress&amp;quot; {
    value = &amp;quot;${sakuracloud_internet.router01.nw_min_ipaddress}&amp;quot;
}

output &amp;quot;router01_max_ipaddress&amp;quot; {
    value = &amp;quot;${sakuracloud_internet.router01.nw_max_ipaddress}&amp;quot;
}

output &amp;quot;router01_ipaddresses&amp;quot; {
    value = [&amp;quot;${sakuracloud_internet.router01.nw_ipaddresses}&amp;quot;]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;nw_mask_len&lt;/code&gt; は&lt;a href=&#34;https://github.com/yamamoto-febc/terraform-provider-sakuracloud/blob/master/docs/configuration/resources/internet.md&#34;&gt;Terraform for さくらのクラウドのルーター&lt;/a&gt;のドキュメントのパラメーターの項を参考に、必要なIPアドレスの数に応じて &lt;code&gt;/28&lt;/code&gt;, &lt;code&gt;/27&lt;/code&gt;, &lt;code&gt;/26&lt;/code&gt; から選択してください。設定する値は &lt;code&gt;/&lt;/code&gt; 無しの数値です。&lt;/p&gt;

&lt;p&gt;なお、&lt;a href=&#34;http://cloud-news.sakura.ad.jp/2015/03/31/ipaddr24-25/&#34;&gt;「ルータ＋スイッチ」 一部の追加IPアドレス個数でのお申込み方法変更のお知らせ | さくらのクラウドニュース&lt;/a&gt; を見ると &lt;code&gt;/25&lt;/code&gt;, &lt;code&gt;/24&lt;/code&gt; も利用可能ですが営業に問い合わせが必要なため、APIからは利用不可となっています。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;name&lt;/code&gt; や &lt;code&gt;description&lt;/code&gt; はお好みで変更してください。&lt;/p&gt;

&lt;p&gt;ルーターに付与されるIPアドレスの範囲はルーター作成後に確定し&lt;a href=&#34;https://github.com/yamamoto-febc/terraform-provider-sakuracloud/blob/master/docs/configuration/resources/internet.md&#34;&gt;Terraform for さくらのクラウドのルーター&lt;/a&gt;のドキュメントの属性 &lt;code&gt;nw_address&lt;/code&gt; などに設定されます。&lt;/p&gt;

&lt;p&gt;上記の &lt;code&gt;server.tf&lt;/code&gt; ではTerraformの&lt;a href=&#34;https://www.terraform.io/docs/configuration/outputs.html&#34;&gt;Configuring Outputs&lt;/a&gt;の機能を使ってこれらの属性を出力するようにしています。&lt;/p&gt;

&lt;p&gt;Terraformの使い方自体は通常通りです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;terraform plan
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;でプランを確認し、&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;terraform apply
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;で適用します。&lt;/p&gt;

&lt;p&gt;すると以下のように出力が出ます。以下ではIPアドレスを伏せています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Outputs:

router01_gateway = xxx.yyy.zzz.145
router01_ipaddress = xxx.yyy.zzz.144
router01_ipaddresses = [
    xxx.yyy.zzz.148,
    xxx.yyy.zzz.149,
    xxx.yyy.zzz.150,
    xxx.yyy.zzz.151,
    xxx.yyy.zzz.152,
    xxx.yyy.zzz.153,
    xxx.yyy.zzz.154,
    xxx.yyy.zzz.155,
    xxx.yyy.zzz.156,
    xxx.yyy.zzz.157,
    xxx.yyy.zzz.158
]
router01_max_ipaddress = xxx.yyy.zzz.158
router01_min_ipaddress = xxx.yyy.zzz.148
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;作成したいサーバ1台毎にcontainer-linuxのconfig-driveのisoイメージを作成&#34;&gt;作成したいサーバ1台毎にContainer LinuxのConfig DriveのISOイメージを作成&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://coreos.com/os/docs/latest/config-drive.html&#34;&gt;Customize with Config-Drive&lt;/a&gt;の手順に従ってConfig DriveのISOイメージを作成し、&lt;a href=&#34;https://github.com/yamamoto-febc/sacloud-upload-image&#34;&gt;Upload ISO image to SAKURA CLOUD&lt;/a&gt;を使ってさくらのクラウドにアップロードします。&lt;/p&gt;

&lt;h3 id=&#34;事前準備&#34;&gt;事前準備&lt;/h3&gt;

&lt;p&gt;私はCentOSで作業したので、ISOイメージの作成に使う &lt;code&gt;mkisofs&lt;/code&gt; を以下のコマンドでインストールしました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo yum install -y mkisofs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;macOSをお使いの場合は &lt;code&gt;mkisofs&lt;/code&gt; は不要ですが、次項の &lt;code&gt;mkupload.sh&lt;/code&gt; で &lt;code&gt;mkisofs&lt;/code&gt; を呼び出しているところを&lt;a href=&#34;https://coreos.com/os/docs/latest/config-drive.html&#34;&gt;Customize with Config-Drive&lt;/a&gt;を参考に書き変えてください。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/yamamoto-febc/sacloud-upload-image&#34;&gt;Upload ISO image to SAKURA CLOUD&lt;/a&gt;をインストールしていない場合はインストールしてください。&lt;/p&gt;

&lt;p&gt;APIキーの取得とAPIキー及びゾーンの環境変数設定は上記のTerraform for さくらのクラウドのときと同じなので既に行っていれば不要です。&lt;/p&gt;

&lt;h3 id=&#34;サーバを作成するリージョンの推奨ネームサーバのipアドレスを調べる&#34;&gt;サーバを作成するリージョンの推奨ネームサーバのIPアドレスを調べる&lt;/h3&gt;

&lt;p&gt;この記事を書いた当初はネームサーバの調べ方がわからなくて、GoogleのDNS 8.8.8.8 を指定していましたが、サーバを作成した後さくらのクラウドのコントロールパネルでサーバの詳細情報のNICタブを選択すると「このリージョンの推奨ネームサーバ: 133.242.0.3, 133.242.0.4」のように表示されていることに気づきました。&lt;/p&gt;

&lt;p&gt;リージョン毎のネームサーバ一覧はさくらのクラウドのドキュメントでは見つけられなかったのですが、&lt;a href=&#34;http://developer.sakura.ad.jp/cloud/api/1.1/facility/&#34;&gt;設備関連API - さくらのクラウド API v1.1 ドキュメント&lt;/a&gt;のリージョン一覧を取得のレスポンスにリージョン毎のネームサーバのIPアドレスが含まれていました。&lt;/p&gt;

&lt;p&gt;実際に試した結果は以下の通りです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -su $SAKURACLOUD_ACCESS_TOKEN:$SAKURACLOUD_ACCESS_TOKEN_SECRET \
  https://secure.sakura.ad.jp/cloud/zone/$SAKURACLOUD_ZONE/api/cloud/1.1/region \
  | jq .
{
  &amp;quot;From&amp;quot;: 0,
  &amp;quot;Count&amp;quot;: 3,
  &amp;quot;Total&amp;quot;: 3,
  &amp;quot;Regions&amp;quot;: [
    {
      &amp;quot;Index&amp;quot;: 0,
      &amp;quot;ID&amp;quot;: 210,
      &amp;quot;Name&amp;quot;: &amp;quot;東京&amp;quot;,
      &amp;quot;Description&amp;quot;: &amp;quot;東京&amp;quot;,
      &amp;quot;NameServers&amp;quot;: [
        &amp;quot;210.188.224.10&amp;quot;,
        &amp;quot;210.188.224.11&amp;quot;
      ]
    },
    {
      &amp;quot;Index&amp;quot;: 1,
      &amp;quot;ID&amp;quot;: 290,
      &amp;quot;Name&amp;quot;: &amp;quot;Sandbox&amp;quot;,
      &amp;quot;Description&amp;quot;: &amp;quot;Sandbox&amp;quot;,
      &amp;quot;NameServers&amp;quot;: [
        &amp;quot;133.242.0.3&amp;quot;,
        &amp;quot;133.242.0.4&amp;quot;
      ]
    },
    {
      &amp;quot;Index&amp;quot;: 2,
      &amp;quot;ID&amp;quot;: 310,
      &amp;quot;Name&amp;quot;: &amp;quot;石狩&amp;quot;,
      &amp;quot;Description&amp;quot;: &amp;quot;石狩&amp;quot;,
      &amp;quot;NameServers&amp;quot;: [
        &amp;quot;133.242.0.3&amp;quot;,
        &amp;quot;133.242.0.4&amp;quot;
      ]
    }
  ],
  &amp;quot;is_ok&amp;quot;: true
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;config-driveのisoイメージを作成-アップロード&#34;&gt;Config DriveのISOイメージを作成・アップロード&lt;/h3&gt;

&lt;p&gt;以下のシェルスクリプトを &lt;code&gt;mkupload.sh&lt;/code&gt; という名前で保存し、 &lt;code&gt;chmod +x mkupload.sh&lt;/code&gt; で実行パーミションを付けます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/sh
set -eu
basedir=/tmp/configdrive.$$
server=&amp;quot;$SERVER&amp;quot;
ssh_pub_key=&amp;quot;$SSH_PUB_KEY&amp;quot;
dns1=&amp;quot;$DNS1&amp;quot;
dns2=&amp;quot;$DNS2&amp;quot;
address=&amp;quot;$ADDRESS&amp;quot;
gateway=&amp;quot;$GATEWAY&amp;quot;
priv_address=&amp;quot;$PRIV_ADDRESS&amp;quot;

mkdir -p &amp;quot;$basedir/openstack/latest&amp;quot;

cat &amp;lt;&amp;lt;EOF &amp;gt; &amp;quot;$basedir/openstack/latest/user_data&amp;quot;
#cloud-config

users:
  - name: &amp;quot;core&amp;quot;
    ssh-authorized-keys:
      - &amp;quot;${ssh_pub_key}&amp;quot;
coreos:
  units:
    - name: 00-eth0.network
      runtime: true
      content: |
        [Match]
        Name=eth0

        [Network]
        DNS=${dns1}
        DNS=${dns2}
        Address=${address}
        Gateway=${gateway}
    - name: 01-eth1.network
      runtime: true
      content: |
        [Match]
        Name=eth1

        [Network]
        Address=${priv_address}
EOF

config_name=&amp;quot;${server}-config&amp;quot;
mkisofs -R -V config-2 -o &amp;quot;${config_name}.iso&amp;quot; &amp;quot;${basedir}&amp;quot;
sacloud-upload-image -f &amp;quot;${config_name}.iso&amp;quot; &amp;quot;${config_name}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://coreos.com/os/docs/latest/config-drive.html&#34;&gt;Customize with Config-Drive&lt;/a&gt;では &lt;code&gt;DNS=&lt;/code&gt; の行は1つだけですが、&lt;a href=&#34;https://www.freedesktop.org/software/systemd/man/systemd.network.html#DNS=&#34;&gt;systemd.network&lt;/a&gt;のドキュメントによると複数指定可能なので2つ指定するようにしました。&lt;/p&gt;

&lt;p&gt;以下のように実行します。公開鍵のパスはとIPアドレスは適宜変更してください。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SERVER=server01 \
SSH_PUB_KEY=&amp;quot;`cat ~/.ssh/id_rsa.pub`&amp;quot; \
DNS1=133.242.0.3 \
DNS2=133.242.0.4 \
ADDRESS=xxx.yyy.zzz.148/28 \
GATEWAY=xxx.yyy.zzz.145 \
PRIV_ADDRESS=192.168.0.101/24 \
./mkuploadconfig.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;SERVER=server02 \
SSH_PUB_KEY=&amp;quot;`cat ~/.ssh/id_rsa.pub`&amp;quot; \
DNS1=133.242.0.3 \
DNS2=133.242.0.4 \
ADDRESS=xxx.yyy.zzz.149/28 \
GATEWAY=xxx.yyy.zzz.145 \
PRIV_ADDRESS=192.168.0.102/24 \
./mkuploadconfig.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;ADDRESS&lt;/code&gt; の値は上記で出力された &lt;code&gt;router01_ipaddresses&lt;/code&gt; の値を上から順番に使い、ネットワークマスク付きで指定しています。&lt;/p&gt;

&lt;p&gt;作成されるISOイメージの名前は &lt;code&gt;${SERVER}-config&lt;/code&gt; となります。上記の例だと &lt;code&gt;server01-config&lt;/code&gt; と &lt;code&gt;server02-config&lt;/code&gt; です。&lt;/p&gt;

&lt;h2 id=&#34;terraform-for-さくらのクラウドでルータに繋がったサーバを作成&#34;&gt;Terraform for さくらのクラウドでルータに繋がったサーバを作成&lt;/h2&gt;

&lt;p&gt;上記で作成していた &lt;code&gt;server.tf&lt;/code&gt; にローカルネットワーク用のスイッチ、サーバ、ディスクのリソースを追記します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;resource &amp;quot;sakuracloud_internet&amp;quot; &amp;quot;router01&amp;quot; {
    name = &amp;quot;router01&amp;quot;
    description = &amp;quot;by Terraform&amp;quot;
    tags = [&amp;quot;Terraform&amp;quot;]
    nw_mask_len = 28
    band_width = 100
}

resource &amp;quot;sakuracloud_switch&amp;quot; &amp;quot;switch01&amp;quot; {
    name = &amp;quot;switch01&amp;quot;
    description = &amp;quot;by Terraform&amp;quot;
    tags = [&amp;quot;Terraform&amp;quot;]
}

resource &amp;quot;sakuracloud_server&amp;quot; &amp;quot;server01&amp;quot; {
    name = &amp;quot;server01&amp;quot;
    disks = [&amp;quot;${sakuracloud_disk.disk01.id}&amp;quot;]
    cdrom_id = &amp;quot;${data.sakuracloud_cdrom.server01_config.id}&amp;quot;
    tags = [&amp;quot;@virtio-net-pci&amp;quot;, &amp;quot;Terraform&amp;quot;]
    description = &amp;quot;by Terraform&amp;quot;
    core = &amp;quot;1&amp;quot;
    memory = &amp;quot;1&amp;quot;
    base_interface = &amp;quot;${sakuracloud_internet.router01.switch_id}&amp;quot;
    additional_interfaces = [&amp;quot;${sakuracloud_switch.switch01.id}&amp;quot;]
}
resource &amp;quot;sakuracloud_disk&amp;quot; &amp;quot;disk01&amp;quot; {
    name = &amp;quot;disk01&amp;quot;
    source_archive_id = &amp;quot;${data.sakuracloud_archive.containerlinux.id}&amp;quot;
    size = &amp;quot;40&amp;quot;
    description = &amp;quot;by Terraform&amp;quot;
}
data &amp;quot;sakuracloud_cdrom&amp;quot; &amp;quot;server01_config&amp;quot; {
    filter = {
        name   = &amp;quot;Name&amp;quot;
        values = [&amp;quot;server01-config&amp;quot;]
    }
}

resource &amp;quot;sakuracloud_server&amp;quot; &amp;quot;server02&amp;quot; {
    name = &amp;quot;server02&amp;quot;
    disks = [&amp;quot;${sakuracloud_disk.disk02.id}&amp;quot;]
    cdrom_id = &amp;quot;${data.sakuracloud_cdrom.server02_config.id}&amp;quot;
    tags = [&amp;quot;@virtio-net-pci&amp;quot;, &amp;quot;Terraform&amp;quot;]
    description = &amp;quot;by Terraform&amp;quot;
    core = &amp;quot;1&amp;quot;
    memory = &amp;quot;1&amp;quot;
    base_interface = &amp;quot;${sakuracloud_internet.router01.switch_id}&amp;quot;
    additional_interfaces = [&amp;quot;${sakuracloud_switch.switch01.id}&amp;quot;]
}
resource &amp;quot;sakuracloud_disk&amp;quot; &amp;quot;disk02&amp;quot; {
    name = &amp;quot;disk02&amp;quot;
    source_archive_id = &amp;quot;${data.sakuracloud_archive.containerlinux.id}&amp;quot;
    size = &amp;quot;40&amp;quot;
    description = &amp;quot;by Terraform&amp;quot;
}
data &amp;quot;sakuracloud_cdrom&amp;quot; &amp;quot;server02_config&amp;quot; {
    filter = {
        name   = &amp;quot;Name&amp;quot;
        values = [&amp;quot;server02-config&amp;quot;]
    }
}

data &amp;quot;sakuracloud_archive&amp;quot; &amp;quot;containerlinux&amp;quot; {
    filter = {
        name   = &amp;quot;Tags&amp;quot;
        values = [&amp;quot;current-stable&amp;quot;, &amp;quot;arch-64bit&amp;quot;, &amp;quot;distro-containerlinux&amp;quot;]
    }
}

output &amp;quot;router01_ipaddress&amp;quot; {
    value = &amp;quot;${sakuracloud_internet.router01.nw_address}&amp;quot;
}

output &amp;quot;router01_gateway&amp;quot; {
    value = &amp;quot;${sakuracloud_internet.router01.nw_gateway}&amp;quot;
}

output &amp;quot;router01_min_ipaddress&amp;quot; {
    value = &amp;quot;${sakuracloud_internet.router01.nw_min_ipaddress}&amp;quot;
}

output &amp;quot;router01_max_ipaddress&amp;quot; {
    value = &amp;quot;${sakuracloud_internet.router01.nw_max_ipaddress}&amp;quot;
}

output &amp;quot;router01_ipaddresses&amp;quot; {
    value = [&amp;quot;${sakuracloud_internet.router01.nw_ipaddresses}&amp;quot;]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;serverの &lt;code&gt;core&lt;/code&gt;, &lt;code&gt;memory&lt;/code&gt; やdiskの &lt;code&gt;size&lt;/code&gt; などはお好みで変更してください。
設定可能な値の一覧は&lt;a href=&#34;http://cloud.sakura.ad.jp/specification/server-disk/&#34;&gt;サーバー/ディスク機能の仕様・料金| さくらのクラウド&lt;/a&gt;を参照してください。&lt;/p&gt;

&lt;p&gt;あとは通常通りTerraformを実行するだけです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;terraform plan
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;でプランを確認し、&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;terraform apply
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;で適用します。&lt;/p&gt;

&lt;p&gt;これでルーターに繋がったContainer Linuxのサーバを静的IPアドレス設定で作成できました！&lt;/p&gt;

&lt;h2 id=&#34;気になった点&#34;&gt;気になった点&lt;/h2&gt;

&lt;h3 id=&#34;作成したサーバをコンパネでみるとipアドレスが表示されていない&#34;&gt;作成したサーバをコンパネでみるとIPアドレスが表示されていない&lt;/h3&gt;

&lt;p&gt;コンパネのサーバ詳細の「NIC」タブのルータ＋スイッチの行の「IPv4アドレス」がハイフンになっていました。またコンパネの「マップ」で見てもIPアドレスが表示されていませんでした。&lt;/p&gt;

&lt;p&gt;まあこれはディスクの修正機能を使っていないので仕方ない気もします。
が、&lt;a href=&#34;http://cloud-news.sakura.ad.jp/2014/09/19/map-ipaddr-modifying/&#34;&gt;マップ画面に表示されるIPアドレス編集機能を追加しました | さくらのクラウドニュース&lt;/a&gt;の手順で設定すれば大丈夫でした。&lt;/p&gt;

&lt;p&gt;実現可能かどうかまだよくわかっていないのですが&lt;a href=&#34;https://github.com/yamamoto-febc/terraform-provider-sakuracloud&#34;&gt;Terraform for さくらのクラウド&lt;/a&gt;でのサーバ作成時にこのIPアドレスを設定できると理想的だなあと思います。&lt;/p&gt;

&lt;h3 id=&#34;container-linuxのconfig-driveをterraformで作成できたらさらに理想的&#34;&gt;Container LinuxのConfig DriveをTerraformで作成できたらさらに理想的&lt;/h3&gt;

&lt;p&gt;現状だとこの記事で書いたように一旦ルーターだけ作って、IPアドレスを調べてから、サーバを作るという手順を踏む必要があります。このため、Terraformの設定ファイルを書き変えて2回適用する必要があります。&lt;/p&gt;

&lt;p&gt;もしContainer LinuxのConfig DriveをTerraformで作成できたら、Terraformの設定ファイルを最初からサーバ込みで記述して1回の適用でルータとサーバを一気に作成できることになるので、こうなれば最高だなーと思います。が、どういう仕様にするかと実装を推測してみるとこれはかなり難しそうな気がします。&lt;/p&gt;

&lt;h3 id=&#34;packerで作ったマイアーカイブでcoreユーザのパスワードを消せていない&#34;&gt;Packerで作ったマイアーカイブでcoreユーザのパスワードを消せていない&lt;/h3&gt;

&lt;p&gt;Container Linuxはcoreユーザでssh鍵認証でログインすることが前提となっていて、パスワードは元々設定されていません。が、Packerでは &lt;code&gt;boot_command&lt;/code&gt; でセットアップした後パスワード認証でssh (Windowsの場合はwinrm)で接続してprovisionerを動かすようになっています。&lt;/p&gt;

&lt;p&gt;そこで上記の &lt;code&gt;containerlinux.json&lt;/code&gt; では &lt;code&gt;boot_command&lt;/code&gt; 内でcoreユーザにパスワードを設定し、ssh接続した後にshell provisionerで &lt;code&gt;sudo passwd -d core&lt;/code&gt; というコマンドを実行してパスワードを消そうとしています。&lt;/p&gt;

&lt;p&gt;Packerの実行結果は以下のようになり、 &lt;code&gt;passwd: password expiry information changed.&lt;/code&gt; の出力でパスワード削除がうまく行っているように見えます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ packer build containerlinux.json
sakuracloud output will be in this color.

==&amp;gt; sakuracloud: Downloading or copying ISO
    sakuracloud: Downloading or copying: https://stable.release.core-os.net/amd64-usr/1185.5.0/coreos_production_iso_image.iso
==&amp;gt; sakuracloud: Creating temporary SSH key for instance...
==&amp;gt; sakuracloud: Creating server...
==&amp;gt; sakuracloud: Waiting 20s for boot...
==&amp;gt; sakuracloud: Waiting for server to become active...
==&amp;gt; sakuracloud: Connecting to VM via VNC
==&amp;gt; sakuracloud: Typing the boot command over VNC...
==&amp;gt; sakuracloud: Waiting for SSH to become available...
==&amp;gt; sakuracloud: Connected to SSH!
==&amp;gt; sakuracloud: Pausing 20s before the next provisioner...
==&amp;gt; sakuracloud: Provisioning with shell script: /tmp/packer-shell837255893
    sakuracloud: passwd: password expiry information changed.
==&amp;gt; sakuracloud: Gracefully shutting down server...
==&amp;gt; sakuracloud: Creating archive: CoreOS 1185.5.0
==&amp;gt; sakuracloud: Destroying server...
Build &#39;sakuracloud&#39; finished.

==&amp;gt; Builds finished. The artifacts of successful builds are:
--&amp;gt; sakuracloud: A archive was created: &#39;CoreOS 1185.5.0&#39; (ID: 112900007545) in zone &#39;is1b&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;が、実際に作成したアーカイブをコピーしてディスクとサーバを作成して、sshを試してみるとパスワードでログインできてしまいます。sshでログインした後 &lt;code&gt;sudo passwd -d core&lt;/code&gt; でパスワードを消すとその後はパスワード認証は失敗し鍵認証だけ成功するようになります。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;sudo passwd -d core&lt;/code&gt; と実行したときにはPackerで実行したときと同じ &lt;code&gt;sakuracloud: passwd: password expiry information changed.&lt;/code&gt; というメッセージが表示されていました。何が原因かわかりませんが、現状は今書いたとおりです。&lt;/p&gt;

&lt;p&gt;ということでパスワード認証をしたく無い場合は、サーバ起動後に &lt;code&gt;sudo passwd -d core&lt;/code&gt; してください。&lt;/p&gt;

&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;

&lt;p&gt;ということで少々不便な点はありますが、さくらのクラウドでContainer Linuxの最新版を使うことが出来ました！&lt;/p&gt;

&lt;h3 id=&#34;サーバ1台毎にconfig-driveのisoイメージを作成することの利点&#34;&gt;サーバ1台毎にConfig DriveのISOイメージを作成することの利点&lt;/h3&gt;

&lt;p&gt;サーバ1台毎にConfig DriveのISOイメージを作成するのは面倒な気もしますが、実際に使ってみるとそれを上回る利点がありました。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;利点1: サーバ1台毎にマイアーカイブを作るよりは早くて手軽

&lt;ul&gt;
&lt;li&gt;Packerで &lt;code&gt;boot_command&lt;/code&gt; と provisioners でプロビジョニングした後サーバを停止してディスクのアーカイブを作成するのですが、この最後の工程が結構時間がかかる時があります。早いときは5分もかからないのですが、混んでいる時は遅くなるらしく1時間弱待たされることもありました。&lt;/li&gt;
&lt;li&gt;一方、Config DriveのISOイメージのアップロードはいつも数十秒程度でサクッと終わりました。&lt;/li&gt;
&lt;li&gt;この記事の方式だと利用したいディストリビューションのバージョン1つに対してマイアーカイブを作成するのは1回で済むので、時間が節約できます。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;利点2: 一度ルーターを作ってIPアドレスが確定した後はConfig DriveのISOイメージは割と使いまわせる

&lt;ul&gt;
&lt;li&gt;こちらは普通の使い方ではあまり関係ないかもしれませんが、今回Packerでマイアーカイブを作るときに &lt;code&gt;boot_command&lt;/code&gt; や provisioners の設定を変えて何度も試行錯誤して作り直しました。その度にサーバと付随するディスクも作り直したのですが、Config DriveのISOイメージはそのまま残しておいて使いまわすことが出来ました。&lt;/li&gt;
&lt;li&gt;このようにベースのアーカイブを何度も変えてサーバを作り直すような試行錯誤をするケースではサーバ1台毎の設定を別出しにしておけるConfig DriveのISOイメージはなかなか便利だなと思いました。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;編集履歴&#34;&gt;編集履歴&lt;/h2&gt;

&lt;h3 id=&#34;2017-01-02-21-16頃&#34;&gt;2017-01-02 21:16頃&lt;/h3&gt;

&lt;p&gt;ISOイメージは&lt;a href=&#34;https://github.com/yamamoto-febc/terraform-provider-sakuracloud/blob/master/docs/configuration/resources/data_resource.md&#34;&gt;データソース&lt;/a&gt;の機能ですでに参照可能とのご指摘を山本さんから頂きました。
すみません、私のドキュメントの読み込み不足でした。
検証してみたら無事使えました！元記事に打ち消し線いれて追記しようかと思ったのですが、わかりにくくなるので直接書き換えました。&lt;/p&gt;

&lt;p&gt;変更内容が気になる方は&lt;a href=&#34;https://github.com/hnakamur/blog/commit/20170102_2116&#34;&gt;gitの差分&lt;/a&gt;を参照してください。&lt;/p&gt;

&lt;h3 id=&#34;2017-01-02-21-40頃&#34;&gt;2017-01-02 21:40頃&lt;/h3&gt;

&lt;p&gt;「気になった点」に「Packerで作ったマイアーカイブでcoreユーザのパスワードを消せていない」を追記しました。&lt;/p&gt;

&lt;h3 id=&#34;2017-01-02-22-30頃&#34;&gt;2017-01-02 22:30頃&lt;/h3&gt;

&lt;p&gt;さくらのクラウドのリージョンごとの推奨ネームサーバを使うように改良しました。
変更内容は&lt;a href=&#34;https://github.com/hnakamur/blog/commit/20170102_2230&#34;&gt;gitの差分&lt;/a&gt;を参照してください。&lt;/p&gt;

&lt;h3 id=&#34;2017-01-02-22-50頃&#34;&gt;2017-01-02 22:50頃&lt;/h3&gt;

&lt;p&gt;「おわりに」に「サーバ1台毎にConfig DriveのISOイメージを作成することの利点」を追記しました。&lt;/p&gt;

&lt;h3 id=&#34;2017-01-03-11-10頃&#34;&gt;2017-01-03 11:10頃&lt;/h3&gt;

&lt;p&gt;ルーターとは別にスイッチを追加してローカルネットワークも作成するようにしました。
また、
変更内容は&lt;a href=&#34;https://github.com/hnakamur/blog/commit/20170103_1110&#34;&gt;gitの差分&lt;/a&gt;を参照してください。&lt;/p&gt;

&lt;h3 id=&#34;2017-01-03-11-15頃&#34;&gt;2017-01-03 11:15頃&lt;/h3&gt;

&lt;p&gt;「参考資料」を追加しました。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>KubernetesのSecrets機能を試してみた</title>
      <link>https://hnakamur.github.io/blog/2017/01/01/tried-kubernetes-secrets/</link>
      <pubDate>Sun, 01 Jan 2017 16:31:08 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2017/01/01/tried-kubernetes-secrets/</guid>
      <description>

&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://kubernetes.io/docs/tutorials/stateful-application/run-stateful-application/&#34;&gt;Running a Single-Instance Stateful Application - Kubernetes&lt;/a&gt; ではMySQLのrootユーザのパスワードを設定のyamlファイルに直接書いていましたが、 安全に管理するためには&lt;a href=&#34;http://kubernetes.io/docs/user-guide/secrets/&#34;&gt;Secrets - Kubernetes&lt;/a&gt; を使うべきとのことなので試してみました。&lt;/p&gt;

&lt;h2 id=&#34;パスワードをsecretとして作成&#34;&gt;パスワードをsecretとして作成&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://kubernetes.io/docs/user-guide/secrets/&#34;&gt;Secrets - Kubernetes&lt;/a&gt; ではユーザ名とパスワードを作っていますが、ここではrootユーザのパスワードだけにしてみました。&lt;/p&gt;

&lt;p&gt;以下のような内容でmysql-secrets.ymlというファイルを作成します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Secret
metadata:
  name: mysql-secret
type: Opaque
data:
  rootPassword: MWYyZDFlMmU2N2Rm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;data.rootPasswordの値は指定したいパスワードをbase64エンコードした値を書いています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ echo -n &amp;quot;1f2d1e2e67df&amp;quot; | base64
MWYyZDFlMmU2N2Rm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下のコマンドでsecretを作成します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ k create -f mysql-secrets.yml
secret &amp;quot;mysql-secret&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;作成したsecretを確認&#34;&gt;作成したsecretを確認&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get secrets mysql-secret -o yaml
apiVersion: v1
data:
  rootPassword: MWYyZDFlMmU2N2Rm
kind: Secret
metadata:
  creationTimestamp: 2017-01-01T07:56:48Z
  name: mysql-secret
  namespace: default
  resourceVersion: &amp;quot;70478&amp;quot;
  selfLink: /api/v1/namespaces/default/secrets/mysql-secret
  uid: d8fe8b5f-cff7-11e6-8be9-aece81f30d69
type: Opaque
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;secretをpodから利用する&#34;&gt;secretをPodから利用する&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://hnakamur.github.io/blog/blog/2017/01/01/tried-mysql-and-nfs-on-kubernetes/&#34;&gt;Kuberntesでデータ領域をNFSマウントしてMySQLを動かしてみた · hnakamur&amp;rsquo;s blog at github&lt;/a&gt;のmysql-deploy.ymlを以下のように変更しました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: mysql
spec:
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: mysql
        ver: &amp;quot;5.6&amp;quot;
    spec:
      containers:
      - image: mysql:5.6
        name: mysql
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: rootPassword
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: mysql-persistent-storage
          mountPath: /var/lib/mysql
      volumes:
        - name: mysql-persistent-storage
          persistentVolumeClaim:
            claimName: mysql-pvc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;serviceとdeployを一旦削除し、mac上のデータディレクトリも一旦消してから、イカのコマンドで作り直しました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f mysql-deploy.yml
deployment &amp;quot;mysql&amp;quot; created
$ kubectl create -f mysql-svc.yml
service &amp;quot;mysql&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;作成したpodの詳細情報を確認&#34;&gt;作成したPodの詳細情報を確認&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe po -l app=mysql
Name:           mysql-1289358488-80g5n
Namespace:      default
Node:           minikube/192.168.99.100
Start Time:     Sun, 01 Jan 2017 17:07:51 +0900
Labels:         app=mysql
                pod-template-hash=1289358488
                ver=5.6
Status:         Running
IP:             172.17.0.5
Controllers:    ReplicaSet/mysql-1289358488
Containers:
  mysql:
    Container ID:       docker://928c7f98bdc8241830ec564d3fb31656647bc2c1e020b257bb1364de1d4e9435
    Image:              mysql:5.6
    Image ID:           docker://sha256:e1406e1f7c42c7e664e138c2cedfcd4c09eef6d4859df1f93fd54d87ed3ba1a1
    Port:               3306/TCP
    State:              Running
      Started:          Sun, 01 Jan 2017 17:07:52 +0900
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/lib/mysql from mysql-persistent-storage (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qqsb7 (ro)
    Environment Variables:
      MYSQL_ROOT_PASSWORD:      &amp;lt;set to the key &#39;rootPassword&#39; in secret &#39;mysql-secret&#39;&amp;gt;
Conditions:
  Type          Status
  Initialized   True
  Ready         True
  PodScheduled  True
Volumes:
  mysql-persistent-storage:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  mysql-pvc
    ReadOnly:   false
  default-token-qqsb7:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-qqsb7
QoS Class:      BestEffort
Tolerations:    &amp;lt;none&amp;gt;
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath           Type            Reason          Message
  ---------     --------        -----   ----                    -------------           --------        ------          -------
  1m            1m              1       {default-scheduler }                            Normal          Scheduled       Successfully as
signed mysql-1289358488-80g5n to minikube
  1m            1m              1       {kubelet minikube}      spec.containers{mysql}  Normal          Pulled          Container image
 &amp;quot;mysql:5.6&amp;quot; already present on machine
  1m            1m              1       {kubelet minikube}      spec.containers{mysql}  Normal          Created         Created contain
er with docker id 928c7f98bdc8; Security:[seccomp=unconfined]
  1m            1m              1       {kubelet minikube}      spec.containers{mysql}  Normal          Started         Started contain
er with docker id 928c7f98bdc8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;MYSQL_ROOT_PASSWORDの説明が &lt;code&gt;&amp;lt;set to the key &#39;rootPassword&#39; in secret &#39;mysql-secret&#39;&amp;gt;&lt;/code&gt; となっていて問題なく使えているようです。&lt;/p&gt;

&lt;h3 id=&#34;mysqlクライアントで接続&#34;&gt;mysqlクライアントで接続&lt;/h3&gt;

&lt;p&gt;mysqlのクライアントで指定したパスワードで接続できることが確認できました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl run -it --rm --image=mysql:5.6 mysql-client -- mysql -h 172.17.0.5 -p1f2d1e2e67df
Waiting for pod default/mysql-client-992258208-c9wm3 to be running, status is Pending, pod ready: false
If you don&#39;t see a command prompt, try pressing enter.

mysql&amp;gt; create database db1;
Query OK, 1 row affected (0.01 sec)

mysql&amp;gt; exit
Bye
Session ended, resume using &#39;kubectl attach mysql-client-992258208-c9wm3 -c mysql-client -i -t&#39; command when the pod is running
deployment &amp;quot;mysql-client&amp;quot; deleted
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Kuberntesでデータ領域をNFSマウントしてMySQLを動かしてみた</title>
      <link>https://hnakamur.github.io/blog/2017/01/01/tried-mysql-and-nfs-on-kubernetes/</link>
      <pubDate>Sun, 01 Jan 2017 12:38:24 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2017/01/01/tried-mysql-and-nfs-on-kubernetes/</guid>
      <description>

&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://hnakamur.github.io/blog/blog/2017/01/01/use-nfs-persistent-volume-on-minikube-virtualbox/&#34;&gt;minikubeとVirtualBoxでNFSのpersistent volumeを試してみた · hnakamur&amp;rsquo;s blog at github&lt;/a&gt;の結果を踏まえて、 &lt;a href=&#34;http://kubernetes.io/docs/tutorials/stateful-application/run-stateful-application/&#34;&gt;Running a Single-Instance Stateful Application - Kubernetes&lt;/a&gt; のチュートリアルを試してみたのでメモです。&lt;/p&gt;

&lt;h2 id=&#34;設定ファイル&#34;&gt;設定ファイル&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ cat persistent-volume-nfs.yml
kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv-nfs
  labels:
    type: nfs
spec:
  capacity:
    storage: 30Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  nfs:
    # TODO: modify path and server appropriately
    path: /Users/hnakamur/kube-data/mysql
    server: 192.168.99.1
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;$ cat mysql-pvc.yml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: mysql-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 15Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;$ cat mysql-deploy.yml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: mysql
spec:
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - image: mysql:5.6
        name: mysql
        env:
          # Use secret in real usage
        - name: MYSQL_ROOT_PASSWORD
          value: password
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: mysql-persistent-storage
          mountPath: /var/lib/mysql
      volumes:
      - name: mysql-persistent-storage
        persistentVolumeClaim:
          claimName: mysql-pvc
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;$ cat mysql-svc.yml
apiVersion: v1
kind: Service
metadata:
  name: mysql
spec:
  ports:
    - port: 3306
  selector:
    app: mysql
  clusterIP: None
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ハマった点としては、persistent volumeとpersistent volume claimのaccessModesは合わせないとうまく行きませんでした。具体的には &lt;code&gt;kubectl describe pvc mysql-pvc&lt;/code&gt; で確認したときにStatusがPendingになっていました。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://kubernetes.io/docs/user-guide/persistent-volumes/#access-modes&#34;&gt;Access Modes&lt;/a&gt;にアクセスモードについての説明があります。&lt;/p&gt;

&lt;h2 id=&#34;サービスの作成と公開&#34;&gt;サービスの作成と公開&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f persistent-volume-nfs.yml
kubectl create -f mysql-pvc.yml
kubectl create -f mysql-deploy.yml
kubectl create -f mysql-svc.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;mysqlに接続してみる&#34;&gt;MySQLに接続してみる&lt;/h2&gt;

&lt;p&gt;まずMySQLコンテナのIPアドレスを調べます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods -o wide -l app=mysql
NAME                     READY     STATUS    RESTARTS   AGE       IP           NODE
mysql-4160924354-c5x2l   1/1       Running   0          3m        172.17.0.5   minikube
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;mysqlクライアントのイメージを使ってmysqlに接続します。
&lt;code&gt;If you don&#39;t see a command prompt, try pressing enter.&lt;/code&gt; とある通り、そのままではプロンプトが表示されなかったのでエンターキーを押すと表示されました。&lt;/p&gt;

&lt;p&gt;試しにデータベースをテーブルを作成してみます。その後 Control-D を押してmysqlクライアントを抜けます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl run -it --rm --image=mysql:5.6 mysql-client -- mysql -h 172.17.0.5 -ppassword
Waiting for pod default/mysql-client-1703061864-g1p4s to be running, status is Pending, pod ready: false
If you don&#39;t see a command prompt, try pressing enter.

mysql&amp;gt; create database test1;
Query OK, 1 row affected (0.00 sec)

mysql&amp;gt; use test1;
Database changed
mysql&amp;gt; create table table1 (id integer);
Query OK, 0 rows affected (0.02 sec)

mysql&amp;gt; ^DBye
Session ended, resume using &#39;kubectl attach mysql-client-1703061864-g1p4s -c mysql-client -i -t&#39; command when the pod is running
deployment &amp;quot;mysql-client&amp;quot; deleted
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;mysqlサーバのpodを更新してみる&#34;&gt;MySQLサーバのPodを更新してみる&lt;/h2&gt;

&lt;p&gt;mysql-deploy.yml を書き変えて &lt;code&gt;kubectl apply&lt;/code&gt; で更新してみます。&lt;/p&gt;

&lt;p&gt;最初MYSQL_ROOT_PASSWORDの値を変えてみようかと思って試したのですが、よく考えるとこれはデータベース作成時に設定されてNFSでマウントしたデータ領域はそのまま残るので、この値を変えても更新できませんでした。&lt;/p&gt;

&lt;p&gt;そこで、ラベルに &lt;code&gt;ver=5.6&lt;/code&gt; というのを追加してみました。
最初5.6はダブルクォートで囲まずにyamlファイルに書いてみたのですが、エラーになったのでダブルクォートで囲んでいます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat mysql-deploy.yml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: mysql
spec:
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: mysql
        ver: &amp;quot;5.6&amp;quot;
    spec:
      containers:
      - image: mysql:5.6
        name: mysql
        env:
          # Use secret in real usage
        - name: MYSQL_ROOT_PASSWORD
          value: password
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: mysql-persistent-storage
          mountPath: /var/lib/mysql
      volumes:
      - name: mysql-persistent-storage
        persistentVolumeClaim:
          claimName: mysql-pvc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下のコマンドで反映しました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f mysql-deploy.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再度mysqlに接続して、先ほど作成したデータベースとテーブルがあるかを確認します。&lt;/p&gt;

&lt;p&gt;まずMySQLコンテナのIPアドレスを調べます。
spec.strategyがRecreateなので、コンテナが作り直されてIPアドレスも先程とは変わっています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods -o wide -l app=mysql
NAME                     READY     STATUS    RESTARTS   AGE       IP           NODE
mysql-1726459224-c5gps   1/1       Running   0          10s       172.17.0.6   minikube
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;kubectl describe po -l app=mysql&lt;/code&gt; を実行してStateのStartedやEventsの時刻を見るとコンテナが作り直されたことがわかります。&lt;/p&gt;

&lt;p&gt;mysqlに接続して、データベースとテーブルを確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl run -it --rm --image=mysql:5.6 mysql-client -- mysql -h 172.17.0.6 -pmypassword
Waiting for pod default/mysql-client-1775806825-vxjgf to be running, status is Pending, pod ready: false
If you don&#39;t see a command prompt, try pressing enter.

mysql&amp;gt; show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| test1              |
+--------------------+
4 rows in set (0.01 sec)

mysql&amp;gt; use test1;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
mysql&amp;gt; show tables;
+-----------------+
| Tables_in_test1 |
+-----------------+
| table1          |
+-----------------+
1 row in set (0.00 sec)

mysql&amp;gt; exit
Bye
Session ended, resume using &#39;kubectl attach mysql-client-1775806825-vxjgf -c mysql-client -i -t&#39; command when the pod is running
deployment &amp;quot;mysql-client&amp;quot; deleted
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>minikubeとVirtualBoxでNFSのpersistent volumeを試してみた</title>
      <link>https://hnakamur.github.io/blog/2017/01/01/use-nfs-persistent-volume-on-minikube-virtualbox/</link>
      <pubDate>Sun, 01 Jan 2017 09:40:35 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2017/01/01/use-nfs-persistent-volume-on-minikube-virtualbox/</guid>
      <description>

&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://kubernetes.io/docs/tutorials/&#34;&gt;Tutorials - Kubernetes&lt;/a&gt;のStateful Applicationsを試そうと思って少し読んだ所、 persistent volume というものを用意する必要があることがわかりました。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://kubernetes.io/docs/user-guide/persistent-volumes/#types-of-persistent-volumes&#34;&gt;Types of Persistent Volumes&lt;/a&gt; を見るとさまざまなタイプの persistent volume がありますが、Mac上での開発環境としてkubernetesを使うならNFSが手軽そうなので、これを試してみることにしました。&lt;/p&gt;

&lt;p&gt;このページを見てもよくわからなかったので、検索して見つけた以下の情報を参考にして試行錯誤して、とりあえず動くようになったのでメモです。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/minikube/issues/2#issuecomment-233629375&#34;&gt;Support mounting host directories into pods · Issue #2 · kubernetes/minikube&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/TheNewNormal/kube-solo-osx/blob/master/examples/pv/nfs-pv-mount-on-pod.md&#34;&gt;kube-solo-osx/nfs-pv-mount-on-pod.md at master · TheNewNormal/kube-solo-osx&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/master/examples/volumes/nfs&#34;&gt;kubernetes/examples/volumes/nfs at master · kubernetes/kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;minikubeからmacのディスクをnfsマウントする&#34;&gt;minikubeからmacのディスクをNFSマウントする&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/minikube/issues/2#issuecomment-233629375&#34;&gt;Support mounting host directories into pods · Issue #2 · kubernetes/minikube&lt;/a&gt;のコメントに従って以下のコマンドをmacで実行しました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo &amp;quot;/Users -network 192.168.99.0 -mask 255.255.255.0 -alldirs -maproot=root:wheel&amp;quot; | sudo tee -a /etc/exports
sudo nfsd restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;IPアドレスは &lt;code&gt;minikube ip&lt;/code&gt; の結果に合わせて調整します。私の環境では 192.168.99.100 だったので、それにあわせて &lt;code&gt;-network&lt;/code&gt; は 192.168.99.0、 &lt;code&gt;-mask&lt;/code&gt; は 255.255.255.0 としています。&lt;/p&gt;

&lt;p&gt;以下の手順で、手動で一度マウントしてみました。 &lt;code&gt;minikube start&lt;/code&gt; は既に起動済みなら不要です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;minikube start
minikube ssh -- sudo umount /Users
minikube ssh -- sudo /usr/local/etc/init.d/nfs-client start
minikube ssh -- sudo mount 192.168.99.1:/Users /Users -o rw,async,noatime,rsize=32768,wsize=32768,proto=tcp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;IPアドレスは &lt;code&gt;minikube ip&lt;/code&gt; の結果に合わせて調整します。私の環境では 192.168.99.100 だったので、minikubeからmacへは 192.168.99.1 で参照できるということでmountの引数にはこのアドレスを指定しています。&lt;/p&gt;

&lt;p&gt;マウントポイントの /Users は適宜変更変更します。&lt;/p&gt;

&lt;p&gt;無事マウントできたら &lt;code&gt;minikube ssh&lt;/code&gt; でssh接続して &lt;code&gt;df -h&lt;/code&gt; などでマウントされたことを確認し、minikube内からとmac側からファイルを作ったり削除して相互に見えることを確認しました。&lt;/p&gt;

&lt;p&gt;一通り確認したらminikube内から &lt;code&gt;sudo umount /Users&lt;/code&gt; でアンマウントしておきます。&lt;/p&gt;

&lt;h2 id=&#34;podからnfsのpersistent-volumeを使ってみる&#34;&gt;PodからNFSのpersistent volumeを使ってみる&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/TheNewNormal/kube-solo-osx/blob/master/examples/pv/nfs-pv-mount-on-pod.md&#34;&gt;kube-solo-osx/nfs-pv-mount-on-pod.md at master · TheNewNormal/kube-solo-osx&lt;/a&gt;と&lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/master/examples/volumes/nfs&#34;&gt;kubernetes/examples/volumes/nfs at master · kubernetes/kubernetes&lt;/a&gt;を参考にして試行錯誤しました。&lt;/p&gt;

&lt;p&gt;後者の &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/f5d9c430e9168cf5c41197b8a4e457981cb031df/examples/volumes/nfs/nfs-web-rc.yaml&#34;&gt;nfs-web-rc.yaml&lt;/a&gt;では ReplicationController というものを作っているのですが、&lt;a href=&#34;http://stackoverflow.com/questions/37423117/replication-controller-vs-deployment-in-kubernetes/37423281#37423281&#34;&gt;google compute engine - Replication Controller VS Deployment in Kubernetes - Stack Overflow&lt;/a&gt;というコメントによると、ReplicatioControllerはDeploymentsにとって変わられるものだそうです。ただし、 &lt;a href=&#34;http://stackoverflow.com/questions/37423117/replication-controller-vs-deployment-in-kubernetes/37423217#37423217&#34;&gt;google compute engine - Replication Controller VS Deployment in Kubernetes - Stack Overflow&lt;/a&gt;によるとDeploymentはまだベータです。&lt;/p&gt;

&lt;h2 id=&#34;サービス公開用の設定ファイル&#34;&gt;サービス公開用の設定ファイル&lt;/h2&gt;

&lt;p&gt;試行錯誤した結果の設定ファイルは以下の通りです。&lt;/p&gt;

&lt;p&gt;persistent-volume-nfs.ymlのspec.nfs.pathに対応するディレクトリはmacで &lt;code&gt;mkdir -p /Users/hnakamur/kube-data&lt;/code&gt; で作成しておきます。spec.nfs.serverはminikubeから見たmacのIPアドレスを指定します。&lt;/p&gt;

&lt;p&gt;spec.persistentVolumeReclaimPolicyは&lt;a href=&#34;https://github.com/TheNewNormal/kube-solo-osx/blob/252b46b4837efc41e7c85c7c3171518e23520866/examples/pv/nfs-pv-mount-on-pod.md&#34;&gt;kube-solo-osx/nfs-pv-mount-on-pod.md at 252b46b4837efc41e7c85c7c3171518e23520866 · TheNewNormal/kube-solo-osx&lt;/a&gt;ではRetainedとなっていたのですが、動かしてみるとエラーメッセージが出たのでそこに書いてあった選択肢の1つのRetainに変えました。&lt;/p&gt;

&lt;p&gt;persistemt volumeとpersistent volume claimについては&lt;a href=&#34;http://kubernetes.io/docs/user-guide/persistent-volumes/&#34;&gt;Persistent Volumes - Kubernetes&lt;/a&gt;に説明があります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat persistent-volume-nfs.yml
kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv-nfs
  labels:
    type: nfs
spec:
  capacity:
    storage: 30Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  nfs:
    # TODO: modify path and server appropriately
    path: /Users/hnakamur/kube-data
    server: 192.168.99.1
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;$ cat persistent-volume-claim.yml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 15Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Deploymentsについては&lt;a href=&#34;http://kubernetes.io/docs/user-guide/deployments/&#34;&gt;Deployments - Kubernetes&lt;/a&gt;を参照してください。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat nginx-deployment.yml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:1.11.8
          ports:
            - containerPort: 80
          volumeMounts:
            - mountPath: &amp;quot;/usr/share/nginx/html&amp;quot;
              name: nginx-data
      volumes:
        - name: nginx-data
          persistentVolumeClaim:
            claimName: my-pvc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Servicesについては&lt;a href=&#34;http://kubernetes.io/docs/user-guide/services/#type-nodeport&#34;&gt;Services - Kubernetes&lt;/a&gt;を参照してください。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat nginx-service.yml
kind: Service
apiVersion: v1
metadata:
  name: nginx
spec:
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: NodePort
  selector:
    app: nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;サービス作成と公開&#34;&gt;サービス作成と公開&lt;/h2&gt;

&lt;p&gt;上記の設定ファイルを用意しておけば、サービス作成と公開は以下のように実行するだけです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f persistent-volume-nfs.yml
kubectl create -f persistent-volume-claim.yml
kubectl create -f nginx-deployment.yml
kubectl create -f nginx-service.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;mac上で以下のコマンドでnginxで表示するHTMLファイルを作成します。HTMLファイルと言いつつ手抜きで単なるテキストです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo &#39;Hello Kubernetes NFS volume!&#39; &amp;gt; ~/kube-data/index.html
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;macからcurlでサービスのnginxにアクセスしてみる&#34;&gt;macからcurlでサービスのnginxにアクセスしてみる&lt;/h2&gt;

&lt;p&gt;ノードのIPとポートを取得して環境変数に設定。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export NODE_IP=$(minikube ip)
$ echo NODE_IP=$NODE_IP
NODE_IP=192.168.99.100
$ export NODE_PORT=$(kubectl get services/nginx -o go-template=&#39;{{(index .spec.ports 0).nodePort}}&#39;)
$ echo NODE_PORT=$NODE_PORT
NODE_PORT=32252
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;curlでアクセスすると、上記で作成したファイルの内容が表示されることを確認できました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes NFS volume!
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;サービス公開停止と削除&#34;&gt;サービス公開停止と削除&lt;/h2&gt;

&lt;p&gt;作成時とは逆の順番に &lt;code&gt;kubectl delete -f&lt;/code&gt; で設定ファイルを指定して削除すればOKでした。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl delete -f nginx-service.yml
kubectl delete -f nginx-deployment.yml
kubectl delete -f persistent-volume-claim.yml
kubectl delete -f persistent-volume-nfs.yml
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>minikubeでKubernetesのチュートリアルをやってみた</title>
      <link>https://hnakamur.github.io/blog/2016/12/31/tried-kubernetes-tutorial-with-minikube/</link>
      <pubDate>Sat, 31 Dec 2016 16:24:33 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2016/12/31/tried-kubernetes-tutorial-with-minikube/</guid>
      <description>

&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;

&lt;p&gt;検索してたら &lt;a href=&#34;https://news.ycombinator.com/item?id=12462261&#34;&gt;Why Kubernetes is winning the container war | Hacker News&lt;/a&gt; というHacker Newsのスレッドを見つけました。&lt;/p&gt;

&lt;p&gt;実際に勝つどうかはともかく、実際に使っている人やMesosphereやRed Hatの人のコメントがあり、非常に参考になりそうです。このブログ記事を書くまで私は Kubernetes はろくに触ったことが無かったので内容はよくわからないですが、後日また見直してみたいところです。&lt;/p&gt;

&lt;p&gt;上記のHacker Newsのコメントで以下の2つのチュートリアルが紹介されていました。このブログ記事はこのうち1つめのほうを試してみたメモです。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetesbootcamp.github.io/kubernetes-bootcamp/&#34;&gt;Kubernetes Bootcamp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kelseyhightower/kubernetes-the-hard-way&#34;&gt;kelseyhightower/kubernetes-the-hard-way: Bootstrap Kubernetes the hard way on Google Cloud Platform or Amazon EC2. No scripts.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;試してから気づいたのですが、全く同じ内容が Kubernetes の公式ドキュメントの &lt;a href=&#34;http://kubernetes.io/docs/tutorials/kubernetes-basics/&#34;&gt;Kubernetes Basics&lt;/a&gt; にありました。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://kubernetes.io/docs/tutorials/kubernetes-basics/&#34;&gt;Kubernetes Basics&lt;/a&gt; はいくつかの章（このチュートリアルでは Module と呼ばれています）に分かれていて、まず図解付きのわかりやすい概念説明があり、その後ブラウザ上のターミナルでコマンドを入力すると結果が表示されるというインタラクティブなチュートリアルになっています。&lt;/p&gt;

&lt;p&gt;各章末にクイズがあり、概念を理解したか確認できるのも良い感じです。&lt;/p&gt;

&lt;p&gt;ターミナルの左に説明文があり、入力する各コマンドをマウスでクリックすると、右側のターミナルに入力してくれるので手軽に試せます。&lt;/p&gt;

&lt;p&gt;とはいえ、手元の環境でも試してみたかったので、macOS上に環境構築してブラウザのインタラクティブチュートリアルとともに試してみました。&lt;/p&gt;

&lt;h2 id=&#34;macos-sierraでの事前準備&#34;&gt;macOS Sierraでの事前準備&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;minikube start --help&lt;/code&gt; の &lt;code&gt;--vm-driver&lt;/code&gt; の説明によると仮想マシンドライバは virtualbox xhyve vmwarefusion のいずれかでデフォルトは virtualbox です。
ということでVirtualBoxをインストールしておきます。私の環境ではバージョンは 5.1.12 でした。&lt;/p&gt;

&lt;p&gt;minikubeとKubernetesはGitHubのプロジェクトにリリースページがあってそこからバイナリをダウンロードできます。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/minikube/releases/tag/v0.14.0&#34;&gt;Release v0.14.0 · kubernetes/minikube&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/kubernetes/releases/tag/v1.5.1&#34;&gt;Release v1.5.1 · kubernetes/kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;が、Homebrewでパッケージが用意されていてバージョンも上記と同じで最新だったのでHomebrewでインストールしました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;brew install Caskroom/cask/minikube
brew install kubectl
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;module-1-kubernetsクラスタを作成する&#34;&gt;Module 1: Kubernetsクラスタを作成する&lt;/h2&gt;

&lt;p&gt;概念の説明は &lt;a href=&#34;https://kubernetesbootcamp.github.io/kubernetes-bootcamp/1-1.html&#34;&gt;Introduction to Kubernetes cluster&lt;/a&gt; を参照してください。&lt;/p&gt;

&lt;h3 id=&#34;minikubeのバージョン確認&#34;&gt;minikubeのバージョン確認&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ minikube version       
minikube version: v0.14.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;minikube起動&#34;&gt;minikube起動&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ minikube start
Starting local Kubernetes cluster...
Kubectl is now configured to use the cluster.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;kubectlのバージョン確認&#34;&gt;kubectlのバージョン確認&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl version
Client Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;5&amp;quot;, GitVersion:&amp;quot;v1.5.1&amp;quot;, GitCommit:&amp;quot;82450d03cb057bab0950214ef122b67c83fb11df&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;2016-12-22T13:56:59Z&amp;quot;, GoVersion:&amp;quot;go1.7.4&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;darwin/amd64&amp;quot;}
Server Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;5&amp;quot;, GitVersion:&amp;quot;v1.5.1&amp;quot;, GitCommit:&amp;quot;82450d03cb057bab0950214ef122b67c83fb11df&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;1970-01-01T00:00:00Z&amp;quot;, GoVersion:&amp;quot;go1.7.1&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;linux/amd64&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;クラスタの情報表示&#34;&gt;クラスタの情報表示&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl cluster-info
Kubernetes master is running at https://192.168.99.100:8443
kubernetes-dashboard is running at https://192.168.99.100:8443/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard

To further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;ノード一覧&#34;&gt;ノード一覧&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get nodes
NAME       STATUS    AGE
minikube   Ready     11h
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;module-2-アプリをデプロイ&#34;&gt;Module 2: アプリをデプロイ&lt;/h2&gt;

&lt;p&gt;概念の説明は &lt;a href=&#34;https://kubernetesbootcamp.github.io/kubernetes-bootcamp/2-1.html&#34;&gt;Your first application deployment&lt;/a&gt; を参照してください。&lt;/p&gt;

&lt;h3 id=&#34;kubernetes-bootcampアプリをデプロイ&#34;&gt;kubernetes-bootcampアプリをデプロイ&lt;/h3&gt;

&lt;p&gt;チュートリアルのために用意されたkubernetes-bootcampアプリをデプロイしてみます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl run kubernetes-bootcamp --image=docker.io/jocatalin/kubernetes-bootcamp:v1 --port=8080
deployment &amp;quot;kubernetes-bootcamp&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;デプロイされたアプリ一覧&#34;&gt;デプロイされたアプリ一覧&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get deployments
NAME                  DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kubernetes-bootcamp   1         1         1            1           57s
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;プロキシ起動&#34;&gt;プロキシ起動&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl proxy
Starting to serve on 127.0.0.1:8001
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;プロキシを起動したらプロンプトには戻ってこないので、以降のコマンドは別のターミナルで実行します。&lt;/p&gt;

&lt;h3 id=&#34;podの名前を取得&#34;&gt;Podの名前を取得&lt;/h3&gt;

&lt;p&gt;この後参照するため、Podの名前を取得して環境変数POD_NAMEに設定しておきます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export POD_NAME=$(kubectl get pods -o go-template --template &#39;{{range .items}}{{.metadata.name}}{{&amp;quot;\n&amp;quot;}}{{end}}&#39;)
$ echo Name of the Pod: $POD_NAME
Name of the Pod: kubernetes-bootcamp-390780338-6j8fn
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;見た感じ &lt;code&gt;--template&lt;/code&gt; の引数の書式はGo言語の &lt;a href=&#34;https://golang.org/pkg/text/template/&#34;&gt;text/template&lt;/a&gt;パッケージのテンプレート言語をそのまま使っているようです。&lt;/p&gt;

&lt;p&gt;Pod名の &lt;code&gt;390780338-6j8fn&lt;/code&gt; の部分はデプロイの度に生成されるランダムな文字列となっています。&lt;/p&gt;

&lt;h3 id=&#34;プロキシ経由でアプリにアクセス&#34;&gt;プロキシ経由でアプリにアクセス&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ curl http://localhost:8001/api/v1/proxy/namespaces/default/pods/$POD_NAME/
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-6j8fn | v=1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;module-3-デプロイしたアプリを詳しく見てみる&#34;&gt;Module 3: デプロイしたアプリを詳しく見てみる&lt;/h2&gt;

&lt;p&gt;概念の説明は &lt;a href=&#34;https://kubernetesbootcamp.github.io/kubernetes-bootcamp/3-1.html&#34;&gt;Pods and Nodes&lt;/a&gt; を参照してください。&lt;/p&gt;

&lt;h3 id=&#34;pod一覧表示&#34;&gt;Pod一覧表示&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods
NAME                                  READY     STATUS    RESTARTS   AGE
kubernetes-bootcamp-390780338-6j8fn   1/1       Running   0          13m
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;pod詳細表示&#34;&gt;Pod詳細表示&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe pods
Name:           kubernetes-bootcamp-390780338-6j8fn
Namespace:      default
Node:           minikube/192.168.99.100
Start Time:     Sat, 31 Dec 2016 17:15:41 +0900
Labels:         pod-template-hash=390780338
                run=kubernetes-bootcamp
Status:         Running
IP:             172.17.0.4
Controllers:    ReplicaSet/kubernetes-bootcamp-390780338
Containers:
  kubernetes-bootcamp:
    Container ID:       docker://f3d04d91e8f27b2b537c20d82253376993483f9bb9c0d1196ba50ecc3a69ff7c
    Image:              docker.io/jocatalin/kubernetes-bootcamp:v1
    Image ID:           docker://sha256:8fafd8af70e9aa7c3ab40222ca4fd58050cf3e49cb14a4e7c0f460cd4f78e9fe
    Port:               8080/TCP
    State:              Running
      Started:          Sat, 31 Dec 2016 17:15:42 +0900
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qqsb7 (ro)
    Environment Variables:      &amp;lt;none&amp;gt;
Conditions:
  Type          Status
  Initialized   True 
  Ready         True 
  PodScheduled  True 
Volumes:
  default-token-qqsb7:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-qqsb7
QoS Class:      BestEffort
Tolerations:    &amp;lt;none&amp;gt;
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath                           Type            Reason          Message
  ---------     --------        -----   ----                    -------------                           --------        ------          -------
  15m           15m             1       {default-scheduler }                                            Normal          Scheduled       Successfully assigned kubernetes-bootcamp-390780338-6j8fn to minikube
  15m           15m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Pulled          Container image &amp;quot;docker.io/jocatalin/kubernetes-bootcamp:v1&amp;quot; already present on machine
  15m           15m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Created         Created container with docker id f3d04d91e8f2; Security:[seccomp=unconfined]
  15m           15m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Started         Started container with docker id f3d04d91e8f2
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;podのログ表示&#34;&gt;Podのログ表示&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl logs $POD_NAME
Kubernetes Bootcamp App Started At: 2016-12-31T08:15:42.728Z | Running On:  kubernetes-bootcamp-390780338-6j8fn 

Running On: kubernetes-bootcamp-390780338-6j8fn | Total Requests: 1 | App Uptime: 580.532 seconds | Log Time: 2016-12-31T08:25:23.260Z
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;pod内でコマンド実行&#34;&gt;Pod内でコマンド実行&lt;/h3&gt;

&lt;p&gt;envコマンドを実行してみます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl exec $POD_NAME env
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=kubernetes-bootcamp-390780338-6j8fn
KUBERNETES_SERVICE_HOST=10.0.0.1
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBERNETES_PORT=tcp://10.0.0.1:443
KUBERNETES_PORT_443_TCP=tcp://10.0.0.1:443
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_PORT_443_TCP_PORT=443
KUBERNETES_PORT_443_TCP_ADDR=10.0.0.1
NPM_CONFIG_LOGLEVEL=info
NODE_VERSION=6.3.1
HOME=/root
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;hostname&lt;/code&gt; コマンドを実行してみます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl exec $POD_NAME hostname
kubernetes-bootcamp-390780338-6j8fn
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;ip a&lt;/code&gt; を実行してみます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl exec $POD_NAME ip a
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
11: eth0@if12: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default 
    link/ether 02:42:ac:11:00:04 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.4/16 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::42:acff:fe11:4/64 scope link tentative dadfailed 
       valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;pod内でbash実行&#34;&gt;Pod内でbash実行&lt;/h3&gt;

&lt;p&gt;以下のコマンドでbashを実行するとプロンプトが表示されます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl exec -ti $POD_NAME bash
root@kubernetes-bootcamp-390780338-6j8fn:/# 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;チュートリアルのために用意されたkubernetes-bootcampアプリに含まれるファイル &lt;code&gt;server.js&lt;/code&gt; の内容を表示してみます。このアプリは Node.js で書かれていることがわかります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@kubernetes-bootcamp-390780338-6j8fn:/# cat server.js
var http = require(&#39;http&#39;);
var requests=0;
var podname= process.env.HOSTNAME;
var startTime;
var host;
var handleRequest = function(request, response) {
  response.setHeader(&#39;Content-Type&#39;, &#39;text/plain&#39;);
  response.writeHead(200);
  response.write(&amp;quot;Hello Kubernetes bootcamp! | Running on: &amp;quot;);
  response.write(host);
  response.end(&amp;quot; | v=1\n&amp;quot;);
  console.log(&amp;quot;Running On:&amp;quot; ,host, &amp;quot;| Total Requests:&amp;quot;, ++requests,&amp;quot;| App Uptime:&amp;quot;, (new Date() - startTime)/1000 , &amp;quot;seconds&amp;quot;, &amp;quot;| Log Time:&amp;quot;,new Date());
}
var www = http.createServer(handleRequest);
www.listen(8080,function () {
    startTime = new Date();;
    host = process.env.HOSTNAME;
    console.log (&amp;quot;Kubernetes Bootcamp App Started At:&amp;quot;,startTime, &amp;quot;| Running On: &amp;quot; ,host, &amp;quot;\n&amp;quot; );
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pod内からcurlで直接アプリにアクセスしてみます。 Node.js コンテナ内でbashを実行しているのでホスト名には localhost を指定しています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@kubernetes-bootcamp-390780338-6j8fn:/# curl localhost:8080
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-6j8fn | v=1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;exit&lt;/code&gt; を入力してPod内のbashを抜けます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@kubernetes-bootcamp-390780338-6j8fn:/# exit
exit
$ 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;module-4-アプリをkubernetes外に公開する&#34;&gt;Module 4: アプリをKubernetes外に公開する&lt;/h2&gt;

&lt;p&gt;概念の説明は &lt;a href=&#34;https://kubernetesbootcamp.github.io/kubernetes-bootcamp/4-1.html&#34;&gt;Services&lt;/a&gt; を参照してください。&lt;/p&gt;

&lt;p&gt;Module 1では &lt;code&gt;minikube proxy&lt;/code&gt; を実行してMacの8001番ポートでリッスンしておいて、Macから localhost:8001 でアクセスしました。&lt;/p&gt;

&lt;p&gt;ここではKubernetesのノード上のポートでリッスンして、Macからminikubeのproxyを経由せずに直接アクセスします。&lt;/p&gt;

&lt;h3 id=&#34;サービス一覧表示&#34;&gt;サービス一覧表示&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get services
NAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   10.0.0.1     &amp;lt;none&amp;gt;        443/TCP   12h
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;kubernetes-bootcampアプリを公開&#34;&gt;kubernetes-bootcampアプリを公開&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl expose deployment/kubernetes-bootcamp --type=&amp;quot;NodePort&amp;quot; --port 8080
service &amp;quot;kubernetes-bootcamp&amp;quot; exposed
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;再度サービス一覧表示&#34;&gt;再度サービス一覧表示&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get services
NAME                  CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE
kubernetes            10.0.0.1     &amp;lt;none&amp;gt;        443/TCP          12h
kubernetes-bootcamp   10.0.0.228   &amp;lt;nodes&amp;gt;       8080:31123/TCP   40s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上で &lt;code&gt;kubectl expose&lt;/code&gt; コマンドでサービスを公開したので、一覧にkubernetes-bootcampが含まれるようになりました。&lt;/p&gt;

&lt;h3 id=&#34;サービス詳細表示&#34;&gt;サービス詳細表示&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe services/kubernetes-bootcamp
Name:                   kubernetes-bootcamp
Namespace:              default
Labels:                 run=kubernetes-bootcamp
Selector:               run=kubernetes-bootcamp
Type:                   NodePort
IP:                     10.0.0.228
Port:                   &amp;lt;unset&amp;gt; 8080/TCP
NodePort:               &amp;lt;unset&amp;gt; 31123/TCP
Endpoints:              172.17.0.4:8080
Session Affinity:       None
No events.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;ノードのポート取得&#34;&gt;ノードのポート取得&lt;/h3&gt;

&lt;p&gt;この後参照するため、ノードのポートを取得して環境変数 NODE_PORT に設定しておきます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template=&#39;{{(index .spec.ports 0).nodePort}}&#39;)
$ echo NODE_PORT=$NODE_PORT
NODE_PORT=31123
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetesbootcamp.github.io/kubernetes-bootcamp/4-2.html&#34;&gt;Module 4のインタラクティブチュートリアル&lt;/a&gt;ではこの後 &lt;code&gt;curl host01:$NODE_PORT&lt;/code&gt; でアクセスしているのですが、手元の環境では &lt;code&gt;host01&lt;/code&gt; というホスト名ではアクセスできません。&lt;/p&gt;

&lt;p&gt;そこで、以下のコマンドを実行してノードのIPアドレスを取得し、環境変数 NODE_IP に設定します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export NODE_IP=$(minikube ip)
$ echo NODE_IP=$NODE_IP
NODE_IP=192.168.99.100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下のコマンドでMacからKubernetesのノードに直接アクセスします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-6j8fn | v=1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Module 4のStep 2でラベルを付けて、Step 3でサービス削除するのですが、この記事を書く時は飛ばしてしまったので、Module 6の後に行います。&lt;/p&gt;

&lt;h2 id=&#34;module-5-アプリをスケールアップする&#34;&gt;Module 5: アプリをスケールアップする&lt;/h2&gt;

&lt;p&gt;概念の説明は &lt;a href=&#34;https://kubernetesbootcamp.github.io/kubernetes-bootcamp/5-1.html&#34;&gt;Running multiple instances of an app&lt;/a&gt; を参照してください。&lt;/p&gt;

&lt;h3 id=&#34;デプロイされたアプリのスケールアップ前のレプリカ数を確認&#34;&gt;デプロイされたアプリのスケールアップ前のレプリカ数を確認&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;kubectl get deployments&lt;/code&gt; の結果にはデプロイごとにアプリのレプリカ（複製）の数が表示されます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get deployments
NAME                  DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kubernetes-bootcamp   1         1         1            1           46m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;この結果ではPodの数は1です。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;DESIRED: デプロイ時に指定したレプリカ数。desireの意味は「切望する」なので、デプロイ時に希望した数ということでしょう。&lt;/li&gt;
&lt;li&gt;CURRENT: 現在実行中のレプリカ数。&lt;/li&gt;
&lt;li&gt;UP-TO-DATE: 指定した状態に更新されたレプリカ数。&lt;/li&gt;
&lt;li&gt;AVAILABLE: ユーザが利用可能な（＝ユーザに実際にサービスが提供されている）レプリカ数。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;スケールアップ&#34;&gt;スケールアップ&lt;/h3&gt;

&lt;p&gt;このデプロイのレプリカ数を4に増やしてみます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl scale deployments/kubernetes-bootcamp --replicas=4
deployment &amp;quot;kubernetes-bootcamp&amp;quot; scaled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;デプロイ一覧で再度確認するとレプリカ数が4に増えていました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get deployments
NAME                  DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kubernetes-bootcamp   4         4         4            4           55m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pod一覧を &lt;code&gt;-o wide&lt;/code&gt; を指定して表示するとIPアドレスとノードを確認できます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods -o wide
NAME                                  READY     STATUS    RESTARTS   AGE       IP           NODE
kubernetes-bootcamp-390780338-6j8fn   1/1       Running   0          55m       172.17.0.4   minikube
kubernetes-bootcamp-390780338-jw7cn   1/1       Running   0          7s        172.17.0.5   minikube
kubernetes-bootcamp-390780338-p8jbb   1/1       Running   0          7s        172.17.0.6   minikube
kubernetes-bootcamp-390780338-vq3kx   1/1       Running   0          7s        172.17.0.7   minikube
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ちなみに &lt;code&gt;-o wide&lt;/code&gt; 無しの出力結果は以下の通りです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods
NAME                                  READY     STATUS    RESTARTS   AGE
kubernetes-bootcamp-390780338-6j8fn   1/1       Running   0          56m
kubernetes-bootcamp-390780338-jw7cn   1/1       Running   0          1m
kubernetes-bootcamp-390780338-p8jbb   1/1       Running   0          1m
kubernetes-bootcamp-390780338-vq3kx   1/1       Running   0          1m
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;スケールアップ後のデプロイの詳細表示&#34;&gt;スケールアップ後のデプロイの詳細表示&lt;/h3&gt;

&lt;p&gt;Events欄にスケールアップした記録が残っています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe deployments/kubernetes-bootcamp
Name:                   kubernetes-bootcamp
Namespace:              default
CreationTimestamp:      Sat, 31 Dec 2016 17:15:41 +0900
Labels:                 run=kubernetes-bootcamp
Selector:               run=kubernetes-bootcamp
Replicas:               4 updated | 4 total | 4 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     True    MinimumReplicasAvailable
OldReplicaSets: &amp;lt;none&amp;gt;
NewReplicaSet:  kubernetes-bootcamp-390780338 (4/4 replicas created)
Events:
  FirstSeen     LastSeen        Count   From                            SubObjectPath   Type            Reason                  Message
  ---------     --------        -----   ----                            -------------   --------        ------                  -------
  58m           58m             1       {deployment-controller }                        Normal          ScalingReplicaSet       Scaled up replica set kubernetes-bootcamp-390780338 to 1
  3m            3m              1       {deployment-controller }                        Normal          ScalingReplicaSet       Scaled up replica set kubernetes-bootcamp-390780338 to 4
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;スケールアップ後のアプリにcurlでアクセス&#34;&gt;スケールアップ後のアプリにcurlでアクセス&lt;/h3&gt;

&lt;p&gt;アクセスしてみるとリクエストごとにランダムなPodに振り分けられ、負荷分散されていることが確認できました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-6j8fn | v=1
$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-6j8fn | v=1
$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-jw7cn | v=1
$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-p8jbb | v=1
$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-6j8fn | v=1
$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-6j8fn | v=1
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;スケールダウン&#34;&gt;スケールダウン&lt;/h3&gt;

&lt;p&gt;レプリカ数を2に減らします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl scale deployments/kubernetes-bootcamp --replicas=2
deployment &amp;quot;kubernetes-bootcamp&amp;quot; scaled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;デプロイ一覧で2に減ったことを確認しました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get deployments
NAME                  DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kubernetes-bootcamp   2         2         2            2           1h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;直後のPod一覧では2つのコンテナのSTATUSがTerminating （終了中）となっていました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods -o wide
NAME                                  READY     STATUS        RESTARTS   AGE       IP           NODE
kubernetes-bootcamp-390780338-6j8fn   1/1       Running       0          1h        172.17.0.4   minikube
kubernetes-bootcamp-390780338-jw7cn   1/1       Running       0          12m       172.17.0.5   minikube
kubernetes-bootcamp-390780338-p8jbb   1/1       Terminating   0          12m       172.17.0.6   minikube
kubernetes-bootcamp-390780338-vq3kx   1/1       Terminating   0          12m       172.17.0.7   minikube
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;数十秒程度してから再度Pod一覧を見るとSTATUSがRunningの2つだけになっていました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods -o wide
NAME                                  READY     STATUS    RESTARTS   AGE       IP           NODE
kubernetes-bootcamp-390780338-6j8fn   1/1       Running   0          1h        172.17.0.4   minikube
kubernetes-bootcamp-390780338-jw7cn   1/1       Running   0          13m       172.17.0.5   minikube
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;module-6-アプリをローリングアップデート&#34;&gt;Module 6: アプリをローリングアップデート&lt;/h2&gt;

&lt;p&gt;概念の説明は &lt;a href=&#34;https://kubernetesbootcamp.github.io/kubernetes-bootcamp/6-1.html&#34;&gt;Performing a rolling update for an app&lt;/a&gt; を参照してください。&lt;/p&gt;

&lt;p&gt;ローリングアップデートではアプリのダウンタイムをゼロでアプリを更新できるそうです。&lt;/p&gt;

&lt;h3 id=&#34;アップデート前の状態確認&#34;&gt;アップデート前の状態確認&lt;/h3&gt;

&lt;p&gt;Pod一覧。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods -o wide
NAME                                  READY     STATUS    RESTARTS   AGE       IP           NODE
kubernetes-bootcamp-390780338-6j8fn   1/1       Running   0          1h        172.17.0.4   minikube
kubernetes-bootcamp-390780338-jw7cn   1/1       Running   0          21m       172.17.0.5   minikube
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pod詳細情報。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe pods
Name:           kubernetes-bootcamp-390780338-6j8fn
Namespace:      default
Node:           minikube/192.168.99.100
Start Time:     Sat, 31 Dec 2016 17:15:41 +0900
Labels:         pod-template-hash=390780338
                run=kubernetes-bootcamp
Status:         Running
IP:             172.17.0.4
Controllers:    ReplicaSet/kubernetes-bootcamp-390780338
Containers:
  kubernetes-bootcamp:
    Container ID:       docker://f3d04d91e8f27b2b537c20d82253376993483f9bb9c0d1196ba50ecc3a69ff7c
    Image:              docker.io/jocatalin/kubernetes-bootcamp:v1
    Image ID:           docker://sha256:8fafd8af70e9aa7c3ab40222ca4fd58050cf3e49cb14a4e7c0f460cd4f78e9fe
    Port:               8080/TCP
    State:              Running
      Started:          Sat, 31 Dec 2016 17:15:42 +0900
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qqsb7 (ro)
    Environment Variables:      &amp;lt;none&amp;gt;
Conditions:
  Type          Status
  Initialized   True
  Ready         True
  PodScheduled  True
Volumes:
  default-token-qqsb7:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-qqsb7
QoS Class:      BestEffort
Tolerations:    &amp;lt;none&amp;gt;
No events.


Name:           kubernetes-bootcamp-390780338-jw7cn
Namespace:      default
Node:           minikube/192.168.99.100
Start Time:     Sat, 31 Dec 2016 18:10:47 +0900
Labels:         pod-template-hash=390780338
                run=kubernetes-bootcamp
Status:         Running
IP:             172.17.0.5
Controllers:    ReplicaSet/kubernetes-bootcamp-390780338
Containers:
  kubernetes-bootcamp:
    Container ID:       docker://23e8c9c3c2b64701f88a61033b534a23f7f2e4a540afa019eea20050bfd12a39
    Image:              docker.io/jocatalin/kubernetes-bootcamp:v1
    Image ID:           docker://sha256:8fafd8af70e9aa7c3ab40222ca4fd58050cf3e49cb14a4e7c0f460cd4f78e9fe
    Port:               8080/TCP
    State:              Running
      Started:          Sat, 31 Dec 2016 18:10:48 +0900
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qqsb7 (ro)
    Environment Variables:      &amp;lt;none&amp;gt;
Conditions:
  Type          Status
  Initialized   True
  Ready         True
  PodScheduled  True
Volumes:
  default-token-qqsb7:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-qqsb7
QoS Class:      BestEffort
Tolerations:    &amp;lt;none&amp;gt;
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath                           Type            Reason          Message
  ---------     --------        -----   ----                    -------------                           --------        ------          -------
  21m           21m             1       {default-scheduler }                                            Normal          Scheduled       Successfully assigned kubernetes-bootcamp-390780338-jw7cn to minikube
  21m           21m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Pulled          Container image &amp;quot;docker.io/jocatalin/kubernetes-bootcamp:v1&amp;quot; already present on machine
  21m           21m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Created         Created container with docker id 23e8c9c3c2b6; Security:[seccomp=unconfined]
  21m           21m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Started         Started container with docker id 23e8c9c3c2b6
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;アプリのバージョンアップ&#34;&gt;アプリのバージョンアップ&lt;/h3&gt;

&lt;p&gt;以下のコマンドでアプリをv1からv2にバージョンアップします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2$ kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2
deployment &amp;quot;kubernetes-bootcamp&amp;quot; image updated
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;直後のPods一覧。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods -o wide
NAME                                   READY     STATUS        RESTARTS   AGE       IP           NODE
kubernetes-bootcamp-2100875782-0jd0d   1/1       Running       0          26s       172.17.0.6   minikube
kubernetes-bootcamp-2100875782-vnxk1   1/1       Running       0          26s       172.17.0.7   minikube
kubernetes-bootcamp-390780338-6j8fn    1/1       Terminating   0          1h        172.17.0.4   minikube
kubernetes-bootcamp-390780338-jw7cn    1/1       Terminating   0          24m       172.17.0.5   minikube
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;十秒程度したあとのPods一覧。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods -o wide
NAME                                   READY     STATUS    RESTARTS   AGE       IP           NODE
kubernetes-bootcamp-2100875782-0jd0d   1/1       Running   0          55s       172.17.0.6   minikube
kubernetes-bootcamp-2100875782-vnxk1   1/1       Running   0          55s       172.17.0.7   minikube
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;サービス詳細。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
$ kubectl describe services/kubernetes-bootcamp
Name:                   kubernetes-bootcamp
Namespace:              default
Labels:                 run=kubernetes-bootcamp
Selector:               run=kubernetes-bootcamp
Type:                   NodePort
IP:                     10.0.0.228
Port:                   &amp;lt;unset&amp;gt; 8080/TCP
NodePort:               &amp;lt;unset&amp;gt; 31123/TCP
Endpoints:              172.17.0.6:8080,172.17.0.7:8080
Session Affinity:       None
No events.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;アップデート後のアプリにアクセス&#34;&gt;アップデート後のアプリにアクセス&lt;/h3&gt;

&lt;p&gt;curlでアクセスしてみると出力にv=2と表示され、アップデートされたアプリが利用可能になっていることが確認できました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-2100875782-0jd0d | v=2
$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-2100875782-0jd0d | v=2
$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-2100875782-vnxk1 | v=2
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;ローリングアップデート後の状態確認&#34;&gt;ローリングアップデート後の状態確認&lt;/h3&gt;

&lt;p&gt;ローリングアップデートの状態確認。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl rollout status deployments/kubernetes-bootcamp
deployment &amp;quot;kubernetes-bootcamp&amp;quot; successfully rolled out
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pod詳細表示。Imageの値が &lt;code&gt;jocatalin/kubernetes-bootcamp:v2&lt;/code&gt; と v2になっていることが確認できます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe pods
Name:           kubernetes-bootcamp-2100875782-0jd0d
Namespace:      default
Node:           minikube/192.168.99.100
Start Time:     Sat, 31 Dec 2016 18:34:57 +0900
Labels:         pod-template-hash=2100875782
                run=kubernetes-bootcamp
Status:         Running
IP:             172.17.0.6
Controllers:    ReplicaSet/kubernetes-bootcamp-2100875782
Containers:
  kubernetes-bootcamp:
    Container ID:       docker://7438b24d95242018dae9b4e82b93055d772f14650c688203b80204073d67d84b
    Image:              jocatalin/kubernetes-bootcamp:v2
    Image ID:           docker://sha256:b6556396ebd45c517469c522c3c61ecf5ab708cafe0e59df906278d34c255ef8
    Port:               8080/TCP
    State:              Running
      Started:          Sat, 31 Dec 2016 18:34:58 +0900
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qqsb7 (ro)
    Environment Variables:      &amp;lt;none&amp;gt;
Conditions:
  Type          Status
  Initialized   True 
  Ready         True 
  PodScheduled  True 
Volumes:
  default-token-qqsb7:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-qqsb7
QoS Class:      BestEffort
Tolerations:    &amp;lt;none&amp;gt;
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath                           Type            Reason          Message
  ---------     --------        -----   ----                    -------------                           --------        ------          -------
  5m            5m              1       {default-scheduler }                                            Normal          Scheduled       Successfully assigned kubernetes-bootcamp-2100875782-0jd0d to minikube
  5m            5m              1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Pulled          Container image &amp;quot;jocatalin/kubernetes-bootcamp:v2&amp;quot; already present on machine
  5m            5m              1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Created         Created container with docker id 7438b24d9524; Security:[seccomp=unconfined]
  5m            5m              1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Started         Started container with docker id 7438b24d9524


Name:           kubernetes-bootcamp-2100875782-vnxk1
Namespace:      default
Node:           minikube/192.168.99.100
Start Time:     Sat, 31 Dec 2016 18:34:57 +0900
Labels:         pod-template-hash=2100875782
                run=kubernetes-bootcamp
Status:         Running
IP:             172.17.0.7
Controllers:    ReplicaSet/kubernetes-bootcamp-2100875782
Containers:
  kubernetes-bootcamp:
    Container ID:       docker://54c58841abb18466fb0f79636111ef5ff193226f43a5741d1730efeb4689ba58
    Image:              jocatalin/kubernetes-bootcamp:v2
    Image ID:           docker://sha256:b6556396ebd45c517469c522c3c61ecf5ab708cafe0e59df906278d34c255ef8
    Port:               8080/TCP
    State:              Running
      Started:          Sat, 31 Dec 2016 18:34:58 +0900
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qqsb7 (ro)
    Environment Variables:      &amp;lt;none&amp;gt;
Conditions:
  Type          Status
  Initialized   True 
  Ready         True 
  PodScheduled  True 
Volumes:
  default-token-qqsb7:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-qqsb7
QoS Class:      BestEffort
Tolerations:    &amp;lt;none&amp;gt;
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath                           Type            Reason          Message
  ---------     --------        -----   ----                    -------------                           --------        ------          -------
  5m            5m              1       {default-scheduler }                                            Normal          Scheduled       Successfully assigned kubernetes-bootcamp-2100875782-vnxk1 to minikube
  5m            5m              1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Pulled          Container image &amp;quot;jocatalin/kubernetes-bootcamp:v2&amp;quot; already present on machine
  5m            5m              1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Created         Created container with docker id 54c58841abb1; Security:[seccomp=unconfined]
  5m            5m              1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Started         Started container with docker id 54c58841abb1
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;アプリのバージョンアップ失敗の例&#34;&gt;アプリのバージョンアップ失敗の例&lt;/h3&gt;

&lt;h4 id=&#34;存在しないタグのイメージにアップデート&#34;&gt;存在しないタグのイメージにアップデート&lt;/h4&gt;

&lt;p&gt;次はv10とタグ付けされたイメージにアップデートを試みます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v10
deployment &amp;quot;kubernetes-bootcamp&amp;quot; image updated
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;問題なくアップデートされたように見えますが、実はv10というタグのイメージは存在しないのでエラーになります。&lt;/p&gt;

&lt;p&gt;デプロイ一覧のレプリカ数を見ると、DESIREDが2に対してAVAILABLEが1であり希望した状態になっていないことがわかります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get deployments
NAME                  DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kubernetes-bootcamp   2         3         2            1           1h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pods一覧を見ると一部のPodはSTATUSがImagePullBackOffとなっています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods -o wide
NAME                                   READY     STATUS             RESTARTS   AGE       IP           NODE
kubernetes-bootcamp-1951388213-lpc3k   0/1       ImagePullBackOff   0          26s       172.17.0.4   minikube
kubernetes-bootcamp-1951388213-mwx9v   0/1       ImagePullBackOff   0          25s       172.17.0.5   minikube
kubernetes-bootcamp-2100875782-0jd0d   1/1       Terminating        0          10m       172.17.0.6   minikube
kubernetes-bootcamp-2100875782-vnxk1   1/1       Running            0          10m       172.17.0.7   minikube
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;間を置いて何度か試していると、STATUSがErrImagePullとなっているときもありました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods -o wide
NAME                                   READY     STATUS             RESTARTS   AGE       IP           NODE
kubernetes-bootcamp-1951388213-lpc3k   0/1       ImagePullBackOff   0          1m        172.17.0.4   minikube
kubernetes-bootcamp-1951388213-mwx9v   0/1       ErrImagePull       0          1m        172.17.0.5   minikube
kubernetes-bootcamp-2100875782-vnxk1   1/1       Running            0          11m       172.17.0.7   minikube
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pod詳細。
Imageの値が &lt;code&gt;jocatalin/kubernetes-bootcamp:v10&lt;/code&gt; であるコンテナのEventsを見ると
&lt;code&gt;Failed to pull image &amp;quot;jocatalin/kubernetes-bootcamp:v10&amp;quot;: Tag v10 not found in repository docker.io/jocatalin/kubernetes-bootcamp&lt;/code&gt; というエラーがあり、タグv10はレジストリに無かったことがわかります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe pods
Name:           kubernetes-bootcamp-1951388213-lpc3k
Namespace:      default
Node:           minikube/192.168.99.100
Start Time:     Sat, 31 Dec 2016 18:44:43 +0900
Labels:         pod-template-hash=1951388213
                run=kubernetes-bootcamp
Status:         Pending
IP:             172.17.0.4
Controllers:    ReplicaSet/kubernetes-bootcamp-1951388213
Containers:
  kubernetes-bootcamp:
    Container ID:
    Image:              jocatalin/kubernetes-bootcamp:v10
    Image ID:
    Port:               8080/TCP
    State:              Waiting
      Reason:           ImagePullBackOff
    Ready:              False
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qqsb7 (ro)
    Environment Variables:      &amp;lt;none&amp;gt;
Conditions:
  Type          Status
  Initialized   True 
  Ready         False 
  PodScheduled  True 
Volumes:
  default-token-qqsb7:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-qqsb7
QoS Class:      BestEffort
Tolerations:    &amp;lt;none&amp;gt;
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath                           Type            Reason          Message
  ---------     --------        -----   ----                    -------------                           --------        ------          -------
  6m            6m              1       {default-scheduler }                                            Normal          Scheduled       Successfully assigned kubernetes-bootcamp-1951388213-lpc3k to minikube
  6m            36s             6       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Pulling         pulling image &amp;quot;jocatalin/kubernetes-bootcamp:v10&amp;quot;
  6m            30s             6       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Warning         Failed          Failed to pull image &amp;quot;jocatalin/kubernetes-bootcamp:v10&amp;quot;: Tag v10 not found in repository docker.io/jocatalin/kubernetes-bootcamp
  6m            30s             6       {kubelet minikube}                                              Warning         FailedSync      Error syncing pod, skipping: failed to &amp;quot;StartContainer&amp;quot; for &amp;quot;kubernetes-bootcamp&amp;quot; with ErrImagePull: &amp;quot;Tag v10 not found in reposit
ory docker.io/jocatalin/kubernetes-bootcamp&amp;quot;

  6m    2s      22      {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal  BackOff         Back-off pulling image &amp;quot;jocatalin/kubernetes-bootcamp:v10&amp;quot;
  6m    2s      22      {kubelet minikube}                                              Warning FailedSync      Error syncing pod, skipping: failed to &amp;quot;StartContainer&amp;quot; for &amp;quot;kubernetes-bootcamp&amp;quot; with ImagePullBackOff: &amp;quot;Back-off pulling image \&amp;quot;jocatalin/kubernetes-bo
otcamp:v10\&amp;quot;&amp;quot;



Name:           kubernetes-bootcamp-1951388213-mwx9v
Namespace:      default
Node:           minikube/192.168.99.100
Start Time:     Sat, 31 Dec 2016 18:44:44 +0900
Labels:         pod-template-hash=1951388213
                run=kubernetes-bootcamp
Status:         Pending
IP:             172.17.0.5
Controllers:    ReplicaSet/kubernetes-bootcamp-1951388213
Containers:
  kubernetes-bootcamp:
    Container ID:
    Image:              jocatalin/kubernetes-bootcamp:v10
    Image ID:
    Port:               8080/TCP
    State:              Waiting
      Reason:           ImagePullBackOff
    Ready:              False
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qqsb7 (ro)
    Environment Variables:      &amp;lt;none&amp;gt;
Conditions:
  Type          Status
  Initialized   True 
  Ready         False 
  PodScheduled  True 
Volumes:
  default-token-qqsb7:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-qqsb7
QoS Class:      BestEffort
Tolerations:    &amp;lt;none&amp;gt;
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath                           Type            Reason          Message
  ---------     --------        -----   ----                    -------------                           --------        ------          -------
  6m            6m              1       {default-scheduler }                                            Normal          Scheduled       Successfully assigned kubernetes-bootcamp-1951388213-mwx9v to minikube
  6m            2m              5       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Pulling         pulling image &amp;quot;jocatalin/kubernetes-bootcamp:v10&amp;quot;
  6m            2m              5       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Warning         Failed          Failed to pull image &amp;quot;jocatalin/kubernetes-bootcamp:v10&amp;quot;: Tag v10 not found in repository docker.io/jocatalin/kubernetes-bootcamp
  6m            2m              5       {kubelet minikube}                                              Warning         FailedSync      Error syncing pod, skipping: failed to &amp;quot;StartContainer&amp;quot; for &amp;quot;kubernetes-bootcamp&amp;quot; with ErrImagePull: &amp;quot;Tag v10 not found in repository docker.io/jocatalin/kubernetes-bootcamp&amp;quot;

  6m    14s     23      {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal  BackOff         Back-off pulling image &amp;quot;jocatalin/kubernetes-bootcamp:v10&amp;quot;
  6m    14s     23      {kubelet minikube}                                              Warning FailedSync      Error syncing pod, skipping: failed to &amp;quot;StartContainer&amp;quot; for &amp;quot;kubernetes-bootcamp&amp;quot; with ImagePullBackOff: &amp;quot;Back-off pulling image \&amp;quot;jocatalin/kubernetes-bootcamp:v10\&amp;quot;&amp;quot;



Name:           kubernetes-bootcamp-2100875782-vnxk1
Namespace:      default
Node:           minikube/192.168.99.100
Start Time:     Sat, 31 Dec 2016 18:34:57 +0900
Labels:         pod-template-hash=2100875782
                run=kubernetes-bootcamp
Status:         Running
IP:             172.17.0.7
Controllers:    ReplicaSet/kubernetes-bootcamp-2100875782
Containers:
  kubernetes-bootcamp:
    Container ID:       docker://54c58841abb18466fb0f79636111ef5ff193226f43a5741d1730efeb4689ba58
    Image:              jocatalin/kubernetes-bootcamp:v2
    Image ID:           docker://sha256:b6556396ebd45c517469c522c3c61ecf5ab708cafe0e59df906278d34c255ef8
    Port:               8080/TCP
    State:              Running
      Started:          Sat, 31 Dec 2016 18:34:58 +0900
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qqsb7 (ro)
    Environment Variables:      &amp;lt;none&amp;gt;
Conditions:
  Type          Status
  Initialized   True 
  Ready         True 
  PodScheduled  True 
Volumes:
  default-token-qqsb7:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-qqsb7
QoS Class:      BestEffort
Tolerations:    &amp;lt;none&amp;gt;
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath                           Type            Reason          Message
  ---------     --------        -----   ----                    -------------                           --------        ------          -------
  16m           16m             1       {default-scheduler }                                            Normal          Scheduled       Successfully assigned kubernetes-bootcamp-2100875782-vnxk1 to minikube
  16m           16m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Pulled          Container image &amp;quot;jocatalin/kubernetes-bootcamp:v2&amp;quot; already present on machine
  16m           16m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Created         Created container with docker id 54c58841abb1; Security:[seccomp=unconfined]
  16m           16m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Started         Started container with docker id 54c58841abb1
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;v10へのローリングアップデートを中止&#34;&gt;v10へのローリングアップデートを中止&lt;/h4&gt;

&lt;p&gt;ローリングアップデートをアンドゥします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl rollout undo deployments/kubernetes-bootcamp$ kubectl rollout undo deployments/kubernetes-bootcamp
deployment &amp;quot;kubernetes-bootcamp&amp;quot; rolled back
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pod一覧。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods -o wide
NAME                                   READY     STATUS    RESTARTS   AGE       IP           NODE
kubernetes-bootcamp-2100875782-vnxk1   1/1       Running   0          20m       172.17.0.7   minikube
kubernetes-bootcamp-2100875782-x290l   1/1       Running   0          17s       172.17.0.4   minikube
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pod詳細。&lt;/p&gt;

&lt;p&gt;Podのレプリカ数は以前指定した2で、2つのPodともImageが &lt;code&gt;jocatalin/kubernetes-bootcamp:v2&lt;/code&gt; とアップデート前のバージョンに戻ったことが確認できました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ 
kubectl describe pods$ kubectl describe pods
Name:           kubernetes-bootcamp-2100875782-vnxk1
Namespace:      default
Node:           minikube/192.168.99.100
Start Time:     Sat, 31 Dec 2016 18:34:57 +0900
Labels:         pod-template-hash=2100875782
                run=kubernetes-bootcamp
Status:         Running
IP:             172.17.0.7
Controllers:    ReplicaSet/kubernetes-bootcamp-2100875782
Containers:
  kubernetes-bootcamp:
    Container ID:       docker://54c58841abb18466fb0f79636111ef5ff193226f43a5741d1730efeb4689ba58
    Image:              jocatalin/kubernetes-bootcamp:v2
    Image ID:           docker://sha256:b6556396ebd45c517469c522c3c61ecf5ab708cafe0e59df906278d34c255ef8
    Port:               8080/TCP
    State:              Running
      Started:          Sat, 31 Dec 2016 18:34:58 +0900
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qqsb7 (ro)
    Environment Variables:      &amp;lt;none&amp;gt;
Conditions:
  Type          Status
  Initialized   True 
  Ready         True 
  PodScheduled  True 
Volumes:
  default-token-qqsb7:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-qqsb7
QoS Class:      BestEffort
Tolerations:    &amp;lt;none&amp;gt;
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath                           Type            Reason          Message
  ---------     --------        -----   ----                    -------------                           --------        ------          -------
  21m           21m             1       {default-scheduler }                                            Normal          Scheduled       Successfully assigned kubernetes-bootcamp-2100875782-vnxk1 to minikube
  21m           21m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Pulled          Container image &amp;quot;jocatalin/kubernetes-bootcamp:v2&amp;quot; already present on machine
  21m           21m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Created         Created container with docker id 54c58841abb1; Security:[seccomp=unconfined]
  21m           21m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Started         Started container with docker id 54c58841abb1


Name:           kubernetes-bootcamp-2100875782-x290l
Namespace:      default
Node:           minikube/192.168.99.100
Start Time:     Sat, 31 Dec 2016 18:55:31 +0900
Labels:         pod-template-hash=2100875782
                run=kubernetes-bootcamp
Status:         Running
IP:             172.17.0.4
Controllers:    ReplicaSet/kubernetes-bootcamp-2100875782
Containers:
  kubernetes-bootcamp:
    Container ID:       docker://523dca2d5839e832942af50d52fe8008c16862c19ebed553e50293765f4cf12c
    Image:              jocatalin/kubernetes-bootcamp:v2
    Image ID:           docker://sha256:b6556396ebd45c517469c522c3c61ecf5ab708cafe0e59df906278d34c255ef8
    Port:               8080/TCP
    State:              Running
      Started:          Sat, 31 Dec 2016 18:55:32 +0900
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qqsb7 (ro)
    Environment Variables:      &amp;lt;none&amp;gt;
Conditions:
  Type          Status
  Initialized   True 
  Ready         True 
  PodScheduled  True 
Volumes:
  default-token-qqsb7:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-qqsb7
QoS Class:      BestEffort
Tolerations:    &amp;lt;none&amp;gt;
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath                           Type            Reason          Message
  ---------     --------        -----   ----                    -------------                           --------        ------          -------
  21m           21m             1       {default-scheduler }                                            Normal          Scheduled       Successfully assigned kubernetes-bootcamp-2100875782-vnxk1 to minikube
  21m           21m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Pulled          Container image &amp;quot;jocatalin/kubernetes-bootcamp:v2&amp;quot; already present on machine
  21m           21m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Created         Created container with docker id 54c58841abb1; Security:[seccomp=unconfined]
  21m           21m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Started         Started container with docker id 54c58841abb1


Name:           kubernetes-bootcamp-2100875782-x290l
Namespace:      default
Node:           minikube/192.168.99.100
Start Time:     Sat, 31 Dec 2016 18:55:31 +0900
Labels:         pod-template-hash=2100875782
                run=kubernetes-bootcamp
Status:         Running
IP:             172.17.0.4
Controllers:    ReplicaSet/kubernetes-bootcamp-2100875782
Containers:
  kubernetes-bootcamp:
    Container ID:       docker://523dca2d5839e832942af50d52fe8008c16862c19ebed553e50293765f4cf12c
    Image:              jocatalin/kubernetes-bootcamp:v2
    Image ID:           docker://sha256:b6556396ebd45c517469c522c3c61ecf5ab708cafe0e59df906278d34c255ef8
    Port:               8080/TCP
    State:              Running
      Started:          Sat, 31 Dec 2016 18:55:32 +0900
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qqsb7 (ro)
    Environment Variables:      &amp;lt;none&amp;gt;
Conditions:
  Type          Status
  Initialized   True 
  Ready         True 
  PodScheduled  True 
Volumes:
  default-token-qqsb7:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-qqsb7
QoS Class:      BestEffort
Tolerations:    &amp;lt;none&amp;gt;
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath                           Type            Reason          Message
  ---------     --------        -----   ----                    -------------                           --------        ------          -------
  1m            1m              1       {default-scheduler }                                            Normal          Scheduled       Successfully assigned kubernetes-bootcamp-2100875782-x290l to minikube
  1m            1m              1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Pulled          Container image &amp;quot;jocatalin/kubernetes-bootcamp:v2&amp;quot; already present on machine
  1m            1m              1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Created         Created container with docker id 523dca2d5839; Security:[seccomp=unconfined]
  1m            1m              1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Started         Started container with docker id 523dca2d5839
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;curlでアクセスしてみても v2 と表示されています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-2100875782-vnxk1 | v=2
$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-2100875782-vnxk1 | v=2
$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-2100875782-x290l | v=2
$ curl $NODE_IP:$NODE_PORT
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-2100875782-x290l | v=2
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;module-4のstep-2-ラベルを付ける&#34;&gt;Module 4のStep 2: ラベルを付ける&lt;/h2&gt;

&lt;p&gt;デプロイの詳細表示。
Labelsにrun=kubernetes-bootcampというのがデフォルトで付いていることがわかります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe deployment
Name:                   kubernetes-bootcamp
Namespace:              default
CreationTimestamp:      Sat, 31 Dec 2016 17:15:41 +0900
Labels:                 run=kubernetes-bootcamp
Selector:               run=kubernetes-bootcamp
Replicas:               2 updated | 2 total | 2 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     True    MinimumReplicasAvailable
OldReplicaSets: &amp;lt;none&amp;gt;
NewReplicaSet:  kubernetes-bootcamp-2100875782 (2/2 replicas created)
Events:
  FirstSeen     LastSeen        Count   From                            SubObjectPath   Type            Reason                  Message
  ---------     --------        -----   ----                            -------------   --------        ------                  -------
  55m           55m             1       {deployment-controller }                        Normal          ScalingReplicaSet       Scaled up replica set kubernetes-bootcamp-390780338 to 4
  43m           43m             1       {deployment-controller }                        Normal          ScalingReplicaSet       Scaled down replica set kubernetes-bootcamp-390780338 to 2
  31m           31m             1       {deployment-controller }                        Normal          ScalingReplicaSet       Scaled up replica set kubernetes-bootcamp-2100875782 to 1
  31m           31m             1       {deployment-controller }                        Normal          ScalingReplicaSet       Scaled down replica set kubernetes-bootcamp-390780338 to 1
  31m           31m             1       {deployment-controller }                        Normal          ScalingReplicaSet       Scaled down replica set kubernetes-bootcamp-390780338 to 0
  21m           21m             1       {deployment-controller }                        Normal          ScalingReplicaSet       Scaled up replica set kubernetes-bootcamp-1951388213 to 1
  21m           21m             1       {deployment-controller }                        Normal          ScalingReplicaSet       Scaled down replica set kubernetes-bootcamp-2100875782 to 1
  21m           21m             1       {deployment-controller }                        Normal          ScalingReplicaSet       Scaled up replica set kubernetes-bootcamp-1951388213 to 2
  31m           11m             2       {deployment-controller }                        Normal          ScalingReplicaSet       Scaled up replica set kubernetes-bootcamp-2100875782 to 2
  11m           11m             1       {deployment-controller }                        Normal          DeploymentRollback      Rolled back deployment &amp;quot;kubernetes-bootcamp&amp;quot; to revision 2
  11m           11m             1       {deployment-controller }                        Normal          ScalingReplicaSet       Scaled down replica set kubernetes-bootcamp-1951388213 to 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;指定したラベルを持つPods一覧表示。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods -l run=kubernetes-bootcamp
NAME                                   READY     STATUS    RESTARTS   AGE
kubernetes-bootcamp-2100875782-vnxk1   1/1       Running   0          32m
kubernetes-bootcamp-2100875782-x290l   1/1       Running   0          11m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;指定したラベルを持つサービス一覧表示。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get services -l run=kubernetes-bootcamp
NAME                  CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE
kubernetes-bootcamp   10.0.0.228   &amp;lt;nodes&amp;gt;       8080:31123/TCP   1h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pod名を取得して環境変数POD_NAMEに設定。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export POD_NAME=$(kubectl get pods -o go-template --template &#39;{{range .items}}{{.metadata.name}}{{&amp;quot;\n&amp;quot;}}{{end}}&#39;)
$ echo Name of the Pod: $POD_NAME
Name of the Pod: kubernetes-bootcamp-2100875782-vnxk1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Podにapp=v2というラベルを設定。チュートリアルではapp=v1というラベルを指定していますが、この記事ではバージョンアップ後に実行しているのでapp=v2にします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl label pod $POD_NAME app=v2
pod &amp;quot;kubernetes-bootcamp-2100875782-vnxk1&amp;quot; labeled
pod &amp;quot;kubernetes-bootcamp-2100875782-x290l&amp;quot; labeled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pod詳細表示。
Labelsにapp=v2が付いています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl describe pods $POD_NAME
Name:           kubernetes-bootcamp-2100875782-vnxk1
Namespace:      default
Node:           minikube/192.168.99.100
Start Time:     Sat, 31 Dec 2016 18:34:57 +0900
Labels:         app=v2
                pod-template-hash=2100875782
                run=kubernetes-bootcamp
Status:         Running
IP:             172.17.0.7
Controllers:    ReplicaSet/kubernetes-bootcamp-2100875782
Containers:
  kubernetes-bootcamp:
    Container ID:       docker://54c58841abb18466fb0f79636111ef5ff193226f43a5741d1730efeb4689ba58
    Image:              jocatalin/kubernetes-bootcamp:v2
    Image ID:           docker://sha256:b6556396ebd45c517469c522c3c61ecf5ab708cafe0e59df906278d34c255ef8
    Port:               8080/TCP
    State:              Running
      Started:          Sat, 31 Dec 2016 18:34:58 +0900
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qqsb7 (ro)
    Environment Variables:      &amp;lt;none&amp;gt;
Conditions:
  Type          Status
  Initialized   True 
  Ready         True 
  PodScheduled  True 
Volumes:
  default-token-qqsb7:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-qqsb7
QoS Class:      BestEffort
Tolerations:    &amp;lt;none&amp;gt;
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath                           Type            Reason          Message
  ---------     --------        -----   ----                    -------------                           --------        ------          -------
  36m           36m             1       {default-scheduler }                                            Normal          Scheduled       Successfully assigned kubernetes-bootcamp-2100875782-vnxk1 to minikube
  36m           36m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Pulled          Container image &amp;quot;jocatalin/kubernetes-bootcamp:v2&amp;quot; already present on machine
  36m           36m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Created         Created container with docker id 54c58841abb1; Security:[seccomp=unconfined]
  36m           36m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Started         Started container with docker id 54c58841abb1


Name:           kubernetes-bootcamp-2100875782-x290l
Namespace:      default
Node:           minikube/192.168.99.100
Start Time:     Sat, 31 Dec 2016 18:55:31 +0900
Labels:         app=v2
                pod-template-hash=2100875782
                run=kubernetes-bootcamp
Status:         Running
IP:             172.17.0.4
Controllers:    ReplicaSet/kubernetes-bootcamp-2100875782
Containers:
  kubernetes-bootcamp:
    Container ID:       docker://523dca2d5839e832942af50d52fe8008c16862c19ebed553e50293765f4cf12c
    Image:              jocatalin/kubernetes-bootcamp:v2
    Image ID:           docker://sha256:b6556396ebd45c517469c522c3c61ecf5ab708cafe0e59df906278d34c255ef8
    Port:               8080/TCP
    State:              Running
      Started:          Sat, 31 Dec 2016 18:55:32 +0900
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qqsb7 (ro)
    Environment Variables:      &amp;lt;none&amp;gt;
Conditions:
  Type          Status
  Initialized   True 
  Ready         True 
  PodScheduled  True 
Volumes:
  default-token-qqsb7:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-qqsb7
QoS Class:      BestEffort
Tolerations:    &amp;lt;none&amp;gt;
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath                           Type            Reason          Message
  ---------     --------        -----   ----                    -------------                           --------        ------          -------
  15m           15m             1       {default-scheduler }                                            Normal          Scheduled       Successfully assigned kubernetes-bootcamp-2100875782-x290l to minikube
  15m           15m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Pulled          Container image &amp;quot;jocatalin/kubernetes-bootcamp:v2&amp;quot; already present on machine
  15m           15m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Created         Created container with docker id 523dca2d5839; Security:[seccomp=unconfined]
  15m           15m             1       {kubelet minikube}      spec.containers{kubernetes-bootcamp}    Normal          Started         Started container with docker id 523dca2d5839
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;今つけたラベルを持つPod一覧表示。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods -l app=v2
NAME                                   READY     STATUS    RESTARTS   AGE
kubernetes-bootcamp-2100875782-vnxk1   1/1       Running   0          40m
kubernetes-bootcamp-2100875782-x290l   1/1       Running   0          19m
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;module-4のstep-3-サービスを削除&#34;&gt;Module 4のStep 3: サービスを削除&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl delete service -l run=kubernetes-bootcamp
service &amp;quot;kubernetes-bootcamp&amp;quot; deleted
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;サービス一覧を確認すると kubernetes-bootcamp が消えていることがわかります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get services
NAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   10.0.0.1     &amp;lt;none&amp;gt;        443/TCP   13h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;curlでアクセスすると接続拒否という期待される結果になりました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl $NODE_IP:$NODE_PORT
curl: (7) Failed to connect to 192.168.99.100 port 31123: Connection refused
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pod一覧を見るとPod自体は存在します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods
NAME                                   READY     STATUS    RESTARTS   AGE
kubernetes-bootcamp-2100875782-vnxk1   1/1       Running   0          40m
kubernetes-bootcamp-2100875782-x290l   1/1       Running   0          19m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Podに入ってアクセスするとアプリ自体は引き続き稼働中であることがわかります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export POD_NAME=kubernetes-bootcamp-2100875782-vnxk1
$ kubectl exec -ti $POD_NAME curl localhost:8080
Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-2100875782-vnxk1 | v=2
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;

&lt;p&gt;ローリングアップデートとアップデート失敗時の切り戻しが簡単に行えるのは良いなと思いました。&lt;/p&gt;

&lt;p&gt;このチュートリアルはステートレスなアプリケーションの例でしたが、
&lt;a href=&#34;http://kubernetes.io/docs/tutorials/&#34;&gt;Tutorials - Kubernetes&lt;/a&gt;
には Stateful Applications というチュートリアルもあるので、こちらも後日試してみたいです。　&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LXD で privileged な CentOS 7コンテナを作る</title>
      <link>https://hnakamur.github.io/blog/2016/10/22/lxd-privileged-centos-container/</link>
      <pubDate>Sat, 22 Oct 2016 18:54:49 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2016/10/22/lxd-privileged-centos-container/</guid>
      <description>

&lt;p&gt;小ネタのメモです。&lt;/p&gt;

&lt;p&gt;先日 LXD 2.0.5 で CentOS 7 コンテナを起動して &lt;code&gt;journalctl -xe&lt;/code&gt; を実行すると以下のようなエラーが出ていました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Oct 22 09:53:58 centos systemd-sysctl[36]: Failed to write &#39;16&#39; to &#39;/proc/sys/kernel/sysrq&#39;: Permission denied
Oct 22 09:53:58 centos systemd-sysctl[36]: Failed to write &#39;1&#39; to &#39;/proc/sys/fs/protected_hardlinks&#39;: Permission denied
Oct 22 09:53:58 centos systemd-sysctl[36]: Failed to write &#39;1&#39; to &#39;/proc/sys/kernel/core_uses_pid&#39;: Permission denied
Oct 22 09:53:58 centos systemd-sysctl[36]: Failed to write &#39;1&#39; to &#39;/proc/sys/fs/protected_symlinks&#39;: Permission denied
Oct 22 09:53:58 centos systemd-remount-fs[35]: /bin/mount for / exited with exit status 32.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;コンテナ作成時に以下のように config で &lt;code&gt;security.privileged&lt;/code&gt; を true に設定しておけば出なくなりました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lxc launch -c security.privileged=true images:centos/7/amd64 コンテナ名
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;設定の確認は以下のコマンドで行います。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ lxc config show コンテナ名
name: centos
profiles:
- default
config:
  security.privileged: &amp;quot;true&amp;quot;
  volatile.base_image: d2a0b3cf928778ad1582ee1feb39a0bbcd57edce01a60868f04e78d959886d71
  volatile.eth0.hwaddr: 00:16:3e:b2:dc:5e
  volatile.last_state.idmap: &#39;[]&#39;
devices:
  root:
    path: /
    type: disk
ephemeral: false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;もっと限定した設定でも対応可能かもしれませんが、とりあえずこれで。&lt;/p&gt;

&lt;h2 id=&#34;2016-10-23-追記&#34;&gt;2016-10-23 追記&lt;/h2&gt;

&lt;p&gt;security.privileged を true にするのは良くないと指摘されました。&lt;/p&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/hnakamur2&#34;&gt;@hnakamur2&lt;/a&gt; Don&amp;#39;t do that! This errors are actually bugs ( see &lt;a href=&#34;https://t.co/5IuFQzMI9u&#34;&gt;https://t.co/5IuFQzMI9u&lt;/a&gt; + &lt;a href=&#34;https://t.co/4ypMXS5FTq&#34;&gt;https://t.co/4ypMXS5FTq&lt;/a&gt; ), so report them to CentOS&lt;/p&gt;&amp;mdash; Marqin (@mrMarqin) &lt;a href=&#34;https://twitter.com/mrMarqin/status/789838146083098625&#34;&gt;October 22, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;CentOS にバグ報告というのはよくわからなかったので、LXDにイシューを立ててみました。
&lt;a href=&#34;https://github.com/lxc/lxd/issues/2544&#34;&gt;CentOS 7 container gets errors like systemd-sysctl[36]: Failed to write &amp;lsquo;16&amp;rsquo; to &amp;lsquo;/proc/sys/kernel/sysrq&amp;rsquo;: Permission denied · Issue #2544 · lxc/lxd&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Go言語のos.Chtimesで設定可能な最大日時は 2262-04-11 23:47:16.854775807 &#43;0000 UTC</title>
      <link>https://hnakamur.github.io/blog/2016/10/22/max-time-for-golang-os-chtimes/</link>
      <pubDate>Sat, 22 Oct 2016 18:32:50 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2016/10/22/max-time-for-golang-os-chtimes/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://golang.org/pkg/os/#Chtimes&#34;&gt;os.Chtimes&lt;/a&gt; のソース&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://golang.org/src/os/file_posix.go?s=3693:3758#L123&#34;&gt;src/os/file_posix.go - The Go Programming Language&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/golang/go/blob/go1.7.3/src/os/file_posix.go#L133-L141&#34;&gt;go/file_posix.go at go1.7.3 · golang/go&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;を見ると、引数は &lt;code&gt;time.Time&lt;/code&gt; なのですが、 &lt;code&gt;syscall.Timespec&lt;/code&gt; に変換するときに &lt;code&gt;time&lt;/code&gt; の &lt;code&gt;UnixNano()&lt;/code&gt; を使っています。 &lt;code&gt;UnixNano()&lt;/code&gt; は 1970-01-01T00:00:00Z からの通算ミリ秒です。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;UnixNano()&lt;/code&gt; で int64 の最大値を設定したときと、 &lt;code&gt;time.Time&lt;/code&gt; で表現可能な最大の日時を調べてみました。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://play.golang.org/p/eUj5L-eEkS&#34;&gt;https://play.golang.org/p/eUj5L-eEkS&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
	&amp;quot;fmt&amp;quot;
	&amp;quot;math&amp;quot;
	&amp;quot;time&amp;quot;
)

func main() {
	fmt.Println(time.Unix(int64(math.MaxInt64)/1e9, int64(math.MaxInt64)%1e9).UTC())
	fmt.Println(time.Unix(math.MaxInt64, 1e9-1).UTC())
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;2262-04-11 23:47:16.854775807 +0000 UTC
219250468-12-04 15:30:07.999999999 +0000 UTC
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;となりました。&lt;/p&gt;

&lt;p&gt;Linux amd64 環境だと &lt;a href=&#34;https://github.com/golang/go/blob/go1.7.3/src/syscall/syscall_linux_amd64.go#L91-L95&#34;&gt;NsecToTimespec&lt;/a&gt; と &lt;a href=&#34;https://github.com/golang/go/blob/go1.7.3/src/syscall/ztypes_linux_amd64.go#L24-L27&#34;&gt;Timespec&lt;/a&gt; は&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func NsecToTimespec(nsec int64) (ts Timespec) {
	ts.Sec = nsec / 1e9
	ts.Nsec = nsec % 1e9
	return
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;type Timespec struct {
	Sec  int64
	Nsec int64
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;となっているので、 &lt;code&gt;NsecToTimespec&lt;/code&gt; を使わずに&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Chtimes(name string, atime time.Time, mtime time.Time) error {
	var utimes [2]syscall.Timespec
	utimes[0] = syscall.Timespec(atime.Unix(), atime.Nanosecond())
	utimes[1] = syscall.Timespec(mtime.Unix(), mtime.Nanosecond())
	if e := syscall.UtimesNano(name, utimes[0:]); e != nil {
		return &amp;amp;PathError{&amp;quot;chtimes&amp;quot;, name, e}
	}
	return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;と書けば &lt;code&gt;time.Time&lt;/code&gt; の限界まで渡すことは出来ます。とは言え &lt;code&gt;syscall.UtimesNano&lt;/code&gt; が対応しているかはまた別問題ですが。&lt;/p&gt;

&lt;p&gt;2262 年まで表現できれば個人的には困らないので、メモだけ残しておくということで。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LocaleOverlaySwaggerというgoaプラグインを書いた</title>
      <link>https://hnakamur.github.io/blog/2016/10/22/localeoverlayswagger-goa-plugin/</link>
      <pubDate>Sat, 22 Oct 2016 16:52:02 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2016/10/22/localeoverlayswagger-goa-plugin/</guid>
      <description>

&lt;h2 id=&#34;まず-swagger-仕様を複数ファイル出力する-goa-プラグイン-multiswagger-を試してみました&#34;&gt;まず Swagger 仕様を複数ファイル出力する goa プラグイン Multiswagger を試してみました&lt;/h2&gt;

&lt;p&gt;まずは &lt;a href=&#34;http://tchssk.hatenablog.com/entry/2016/10/18/122215&#34;&gt;Swagger 仕様を複数ファイル出力する goa プラグイン Multiswagger を作った - tchsskのブログ&lt;/a&gt; を読んで試してみました。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/goadesign/goa/&#34;&gt;goadesign/goa: Design-based APIs and microservices in Go&lt;/a&gt; の README からリンクされているサンプル &lt;a href=&#34;https://github.com/goadesign/goa-cellar&#34;&gt;goadesign/goa-cellar: goa winecellar example service&lt;/a&gt; の &lt;code&gt;design.go&lt;/code&gt; の各種項目の &lt;code&gt;Title&lt;/code&gt; や &lt;code&gt;Description&lt;/code&gt; の値に JSON を書いて英語と日本語の説明を書いてみた例が &lt;a href=&#34;https://github.com/hnakamur/goa-getting-started/blob/use_multiswagger/design/design.go&#34;&gt;goa-getting-started/design.go&lt;/a&gt; です。　&lt;/p&gt;

&lt;p&gt;私が試したバージョンの &lt;a href=&#34;https://github.com/tchssk/multiswagger/tree/7ad4f69b2209316035dd222819228f90327cd1f3&#34;&gt;Multiswagger at 7ad4f69b2209316035dd222819228f90327cd1f3&lt;/a&gt; では &lt;a href=&#34;https://github.com/hnakamur/goa-getting-started/blob/4bef7925510700d8797831f3bb665eb87c8ca6b9/design/design.go#L8-L19&#34;&gt;API定義&lt;/a&gt; の &lt;code&gt;Title&lt;/code&gt; や &lt;code&gt;Definition&lt;/code&gt; は非対応だったので、 &lt;a href=&#34;https://github.com/tchssk/multiswagger/compare/master...hnakamur:support_more_fields&#34;&gt;Comparing tchssk:master&amp;hellip;hnakamur:support_more_fields · tchssk/multiswagger&lt;/a&gt; のように変更して試してみました。&lt;/p&gt;

&lt;p&gt;変更に際して以下の点にハマりました。&lt;/p&gt;

&lt;p&gt;ハマった点その1。 API定義は &lt;a href=&#34;https://godoc.org/github.com/goadesign/goa/goagen/gen_swagger#Swagger&#34;&gt;github.com/goadesign/goagen/genswagger/Swagger&lt;/a&gt; の &lt;code&gt;Definitions&lt;/code&gt; に保持されるのですが、値の型が &lt;code&gt;map[string]*genschema.JSONSchema&lt;/code&gt; となっていて、 &lt;code&gt;JSONSchema&lt;/code&gt; の値は &lt;a href=&#34;https://github.com/goadesign/goa/blob/4d19425396efa86b61d97c3cda0b00ec21f103f7/goagen/gen_schema/json_schema.go#L100&#34;&gt;goa/json_schema.go のグローバル変数 Definitions&lt;/a&gt; に保持されています。&lt;/p&gt;

&lt;p&gt;このため &lt;a href=&#34;https://github.com/hnakamur/multiswagger/blob/ec57ee4e1b17d0b13091e0b3d17649796967ed64/generator.go#L142-L173&#34;&gt;extract 関数&lt;/a&gt; 内で JSON 文字列から最初のキーの値を取り出して書き変えた後、 &lt;a href=&#34;https://github.com/hnakamur/multiswagger/blob/ec57ee4e1b17d0b13091e0b3d17649796967ed64/generator.go#L72&#34;&gt;generator.go#L72&lt;/a&gt; で次のキー用に &lt;code&gt;Swagger&lt;/code&gt; の値を作り直しても JSONSchema は古い値が再利用されてしまいます。そこで &lt;a href=&#34;https://github.com/hnakamur/multiswagger/blob/ec57ee4e1b17d0b13091e0b3d17649796967ed64/generator.go#L71&#34;&gt;generator.go#L71&lt;/a&gt; で &lt;code&gt;genschema.Definitions&lt;/code&gt; を初期化することで対応できました。&lt;/p&gt;

&lt;p&gt;ハマった点その2。生成された &lt;a href=&#34;https://github.com/hnakamur/goa-getting-started/blob/4bef7925510700d8797831f3bb665eb87c8ca6b9/swagger/swagger.ja.yaml#L8&#34;&gt;swagger.ja.yaml の 8 行目&lt;/a&gt;  を見ると &lt;code&gt;definitions&lt;/code&gt; の &lt;code&gt;description&lt;/code&gt; に &lt;code&gt;(default view)&lt;/code&gt; という値が自動的に追加されています。  &lt;a href=&#34;https://github.com/hnakamur/goa-getting-started/blob/4bef7925510700d8797831f3bb665eb87c8ca6b9/design/design.go#L42-L45&#34;&gt;design.go#L42-L45&lt;/a&gt; に JSON を書いていても &lt;code&gt;(default view)&lt;/code&gt; という値が追加されるので JSON としてパースしようとするとエラーになってしまいます。そこで、値が &lt;code&gt;(default view)&lt;/code&gt; で終わっていたら、それを取り除いてから JSON としてパース可能か調べるようにしました。そしてパースできる場合はパースして特定のキーの値を取り出してから最後に &lt;code&gt;(default view)&lt;/code&gt; とつけるようにしました。&lt;/p&gt;

&lt;p&gt;やれやれこれで大丈夫かと思ったのですが、 &lt;a href=&#34;https://github.com/hnakamur/goa-getting-started/blob/4bef7925510700d8797831f3bb665eb87c8ca6b9/swagger/swagger.ja.yaml#L33-L69&#34;&gt;swagger.ja.yaml#L33-L69&lt;/a&gt; の &lt;code&gt;error&lt;/code&gt; の &lt;code&gt;description&lt;/code&gt; は英語になっています。 &lt;a href=&#34;https://github.com/hnakamur/goa-getting-started/blob/4bef7925510700d8797831f3bb665eb87c8ca6b9/design/design.go&#34;&gt;design.go&lt;/a&gt; に書いていないデフォルト値が出力されているようです。&lt;/p&gt;

&lt;p&gt;また、 &lt;a href=&#34;https://github.com/hnakamur/goa-getting-started/blob/4bef7925510700d8797831f3bb665eb87c8ca6b9/swagger/swagger.ja.yaml#L95&#34;&gt;swagger.ja.yaml#L95&lt;/a&gt; の &lt;code&gt;summary&lt;/code&gt; も show bottle と英語になっています。これは今はコメントにしていますが &lt;a href=&#34;https://github.com/hnakamur/goa-getting-started/blob/4bef7925510700d8797831f3bb665eb87c8ca6b9/design/design.go#L36&#34;&gt;design.go#L36&lt;/a&gt; のように &lt;code&gt;Metadata(&amp;quot;swagger:summary&amp;quot;, value)&lt;/code&gt; で設定可能なことがわかりました。&lt;/p&gt;

&lt;p&gt;しかしこの値を JSON で書くとなると &lt;a href=&#34;https://github.com/hnakamur/multiswagger/blob/ec57ee4e1b17d0b13091e0b3d17649796967ed64/generator.go#L175-L253&#34;&gt;walk 関数&lt;/a&gt; で Metadata で &lt;code&gt;&amp;quot;swagger:summary&amp;quot;&lt;/code&gt; 特定のキーの場合だけ処理するという改修が必要です。&lt;/p&gt;

&lt;p&gt;このあたりで辛くなってきました。 &lt;code&gt;design.go&lt;/code&gt; の DSL はそのままで値に JSON を書くという設計は &lt;code&gt;design.go&lt;/code&gt; で各言語のメッセージが一覧できるという利点がある一方、 generator の実装が面倒だと思います。あと、言語が増えると &lt;code&gt;design.go&lt;/code&gt; の API 定義に対するメッセージ文字列の行が増えて API 定義が見にくくなるという欠点もあると思いました。&lt;/p&gt;

&lt;h2 id=&#34;ということで-localeovrerlayswagger-という別の-swagger-仕様生成プラグインを作りました&#34;&gt;ということで LocaleOvrerlaySwagger という別の Swagger 仕様生成プラグインを作りました&lt;/h2&gt;

&lt;p&gt;ソースは &lt;a href=&#34;https://github.com/hnakamur/localeoverlayswagger&#34;&gt;hnakamur/localeoverlayswagger&lt;/a&gt; で公開しています。&lt;/p&gt;

&lt;p&gt;使い方は &lt;a href=&#34;https://github.com/hnakamur/localeoverlayswagger#usage&#34;&gt;README の Usage&lt;/a&gt; をご参照ください。メッセージの書き方ですが、 &lt;a href=&#34;https://github.com/hnakamur/goa-getting-started/blob/overlay_japanese_yaml/design/design.go&#34;&gt;design.go&lt;/a&gt; の各種 Description は標準通り英語で書きます。&lt;/p&gt;

&lt;p&gt;英語の Swagger 仕様は標準と同じ内容で &lt;a href=&#34;https://github.com/hnakamur/goa-getting-started/blob/overlay_japanese_yaml/swagger/swagger.yaml&#34;&gt;swagger/swagger.yaml&lt;/a&gt; のように生成されます。 この内置き換えた部分だけのキーを含む YAML ファイルを &lt;a href=&#34;https://github.com/hnakamur/goa-getting-started/blob/overlay_japanese_yaml/locales/ja.yaml&#34;&gt;overlay_japanese_yaml&lt;/a&gt; のように書いておくと、 &lt;a href=&#34;https://github.com/hnakamur/goa-getting-started/blob/overlay_japanese_yaml/swagger/swagger.ja.yaml&#34;&gt;swagger/swagger.ja.yaml&lt;/a&gt; のようにその部分だけ上書きされた YAML が生成されるという仕組みです。&lt;/p&gt;

&lt;p&gt;英語のメッセージに対応する日本語のメッセージを離れたところに書く必要があるのでその点は不便なのですが、生成された英語の YAML を見ながら対応するキーに日本語メッセージを書くだけで良いので、トータルではこちらのほうが管理が楽だと個人的には思います。&lt;/p&gt;

&lt;p&gt;ということで、良かったらご利用ください。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>pgpool-IIを使ってPostgreSQLのアクティブ・スタンバイ(1&#43;1構成)を試してみた</title>
      <link>https://hnakamur.github.io/blog/2016/09/15/experiment-postgresql-active-standby-using-pgpool-ii/</link>
      <pubDate>Thu, 15 Sep 2016 06:28:34 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2016/09/15/experiment-postgresql-active-standby-using-pgpool-ii/</guid>
      <description>

&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;

&lt;p&gt;pgool-IIを使ってPostgreSQLのアクティブ・スタンバイ(1+1構成)を試したのでメモです。&lt;/p&gt;

&lt;p&gt;以下のページを参考にしました。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html&#34;&gt;pgpool-II ユーザマニュアル&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.pgpool.net/pgpool-web/contrib_docs/watchdog_master_slave_3.3/ja.html&#34;&gt;pgpool-II watchdog チュートリアル（master-slave mode）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://lets.postgresql.jp/documents/technical/pgpool-II-3.3-watchdog/1&#34;&gt;pgpool-II 3.3 の watchdog 機能 — Let&amp;rsquo;s Postgres&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;テスト用のansible-playbook&#34;&gt;テスト用のAnsible playbook&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/hnakamur/postgresql-pgpool2-failover-example-playbook&#34;&gt;https://github.com/hnakamur/postgresql-pgpool2-failover-example-playbook&lt;/a&gt; に置きました。&lt;/p&gt;

&lt;p&gt;LXD をセットアップ済みの Ubuntu 16.04 上で試しました。&lt;/p&gt;

&lt;p&gt;LXD で CentOS 7 のコンテナを2つ作って環境構築しています。
PostgreSQL と pgpool-II は &lt;a href=&#34;http://yum.postgresql.org/repopackages.php&#34;&gt;PostgreSQL RPM Repository (with Yum)&lt;/a&gt; からインストールしました。
PostgreSQL のバージョンは 9.5.4、 pgpool-II のバージョンは 3.5.4 です。&lt;/p&gt;

&lt;h2 id=&#34;今回の構成&#34;&gt;今回の構成&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.pgpool.net/pgpool-web/contrib_docs/watchdog_master_slave_3.3/ja.html&#34;&gt;pgpool-II watchdog チュートリアル（master-slave mode）&lt;/a&gt; の図と同様の構成となっています。&lt;/p&gt;

&lt;p&gt;ただし、今回の構成では pgpool-II と PostgreSQL を別のコンテナにせず1つのコンテナに同居させていて、以下の2つのコンテナで構成しています。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;pgsql1 (IPアドレス &lt;code&gt;10.155.92.101&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;pgsql2 (IPアドレス &lt;code&gt;10.155.92.102&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;pgpool-II は watchdog で相互監視するマスタ・スタンバイ構成 (
&lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html#config&#34;&gt;pgpool-II の設定&lt;/a&gt; のマスタスレーブモード ) です。
pgpool-II のマスタが仮想 IP &lt;code&gt;10.155.92.100&lt;/code&gt; を持ちます。&lt;/p&gt;

&lt;p&gt;pgpool-II から PostgreSQL を監視するのは heartbeat という仕組みを今回は使っています。&lt;/p&gt;

&lt;p&gt;レプリケーションは pgpool-II ではなく PostgreSQL の非同期ストリーミング・レプリケーションを使っています。&lt;/p&gt;

&lt;p&gt;また、 pgpool-II の負荷分散 (ロードバランサ) 機能は今回は使っていません。&lt;/p&gt;

&lt;p&gt;なお、仮想 IP はあくまで pgpool-II のマスタと連動するもので、 PostgreSQL のプライマリとは別のコンテナになることもあります。&lt;/p&gt;

&lt;p&gt;pgpool-II のドキュメントやコマンドの出力を見ると、 pgpool-II のマスタはマスタ、 PostgreSQL のマスタはプライマリと用語を使い分けているようです。この記事もそれに従います。&lt;/p&gt;

&lt;p&gt;pgpool-II と PostgreSQL のポートはそれぞれデフォルトの 9999 と 5432 としています。
pgpool-II は他に管理用のポートとして 9898、 watchdog 用のポートで 9000 を使います。&lt;/p&gt;

&lt;h2 id=&#34;セットアップの事前準備&#34;&gt;セットアップの事前準備&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://hnakamur.github.io/blog/blog/2016/08/21/experiment-postgresql-active-standby-cluster-using-pacemaker/&#34;&gt;Pacemakerを使ってPostgreSQLのアクティブ・スタンバイ(1+1構成)を試してみた · hnakamur&amp;rsquo;s blog at github&lt;/a&gt; と同様です。&lt;/p&gt;

&lt;h2 id=&#34;pgpool-ii-の管理者ユーザとパスワード&#34;&gt;pgpool-II の管理者ユーザとパスワード&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html#pcp_config&#34;&gt;pcp.conf の設定&lt;/a&gt; に従って pgpool-II の管理者のユーザ名と md5 暗号化したパスワードを &lt;code&gt;/etc/pgpool-II-95/pcp.conf&lt;/code&gt; に設定しています。&lt;/p&gt;

&lt;p&gt;管理者ユーザ名は &lt;code&gt;pgpool2&lt;/code&gt; としました。&lt;/p&gt;

&lt;p&gt;パスワードは以下のコマンドを実行して &lt;code&gt;group_vars/development/secrets.yml&lt;/code&gt; を復号化し、 &lt;code&gt;development.secrets.pgpool2_admin_password&lt;/code&gt; の値を参照してください。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ansible-vault decrypt group_vars/development/secrets.yml 
Vault password: 
Decryption successful
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;コンテナの作成&#34;&gt;コンテナの作成&lt;/h2&gt;

&lt;p&gt;以下のコマンドを実行して &lt;code&gt;pgsql1&lt;/code&gt; と &lt;code&gt;pgsql2&lt;/code&gt; という2つのコンテナを作成します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ansible-playbook launch_containers.yml -D -v
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;vaultのパスワードを聞かれますので入力してください。&lt;/p&gt;

&lt;h2 id=&#34;コンテナ内に-postgresql-と-pgpool-ii-をセットアップ&#34;&gt;コンテナ内に PostgreSQL と pgpool-II をセットアップ&lt;/h2&gt;

&lt;p&gt;以下のコマンドを実行して、コンテナ内に PostgreSQL と pgpool-II をセットアップします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ansible-playbook setup_containers.yml -D -v
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;セットアップが完了したときの初期状態では &lt;code&gt;pgsql1&lt;/code&gt; の pgpool-II がマスタで仮想IPを持ち、 PostgreSQL も &lt;code&gt;pgsql1&lt;/code&gt; がプライマリとなっています。&lt;/p&gt;

&lt;h2 id=&#34;状態確認のコマンド説明&#34;&gt;状態確認のコマンド説明&lt;/h2&gt;

&lt;h3 id=&#34;postgresql-のプロセス確認&#34;&gt;PostgreSQL のプロセス確認&lt;/h3&gt;

&lt;p&gt;起動してしばらく経ってから &lt;code&gt;pgsql1&lt;/code&gt; コンテナの PostgreSQL プロセスを ps で見ると以下のようになります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql1 ~]# ps axf | grep [p]ostgres
 1464 ?        S      0:00 /usr/pgsql-9.5/bin/postgres -D /var/lib/pgsql/9.5/data
 1465 ?        Ss     0:00  \_ postgres: logger process   
 1467 ?        Ss     0:00  \_ postgres: checkpointer process   
 1468 ?        Ss     0:00  \_ postgres: writer process   
 1469 ?        Ss     0:00  \_ postgres: wal writer process   
 1470 ?        Ss     0:00  \_ postgres: autovacuum launcher process   
 1471 ?        Ss     0:00  \_ postgres: archiver process   last was 000000010000000000000002.00000028.backup
 1472 ?        Ss     0:00  \_ postgres: stats collector process   
 1720 ?        Ss     0:00  \_ postgres: wal sender process repl_user 10.155.92.102(43074) streaming 0/3000060
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;pgsql2&lt;/code&gt; ではこうなります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 ~]# ps axf | grep [p]ostgres
 1386 ?        S      0:00 /usr/pgsql-9.5/bin/postgres -D /var/lib/pgsql/9.5/data
 1387 ?        Ss     0:00  \_ postgres: logger process   
 1388 ?        Ss     0:00  \_ postgres: startup process   recovering 000000010000000000000003
 1394 ?        Ss     0:00  \_ postgres: checkpointer process   
 1395 ?        Ss     0:00  \_ postgres: writer process   
 1396 ?        Ss     0:00  \_ postgres: stats collector process   
 1399 ?        Ss     0:00  \_ postgres: wal receiver process   streaming 0/3000060
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;postgres: wal sender&lt;/code&gt; のプロセスがあれば PostgreSQL のプライマリ、 &lt;code&gt;postgres: wal receiver&lt;/code&gt; のプロセスがあれば PostgreSQL のスタンバイと判断することが出来ます。&lt;/p&gt;

&lt;p&gt;ただし、PostgreSQL のプライマリが切り替わってしばらくの間はこのプロセスは存在しないので、 次項の方法を使います。&lt;/p&gt;

&lt;h3 id=&#34;pgpool-ii-から見た-postgresql-ノードの状態確認&#34;&gt;pgpool-II から見た PostgreSQL ノードの状態確認&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://tyawan080.hatenablog.com/entry/2014/05/12/234226&#34;&gt;PostgreSQLのマスタ判断 - Marlock Homes Diary&lt;/a&gt; で知りました。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;select pg_is_in_recovery()&lt;/code&gt; を実行して &lt;code&gt;t&lt;/code&gt; であればスタンバイ、 &lt;code&gt;f&lt;/code&gt; であればプライマリかスタンドアロンです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql1 ~]# sudo -i -u postgres psql -c &amp;quot;select pg_is_in_recovery()&amp;quot;
 pg_is_in_recovery 
-------------------
 f
(1 row)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 ~]# sudo -i -u postgres psql -c &amp;quot;select pg_is_in_recovery()&amp;quot;
 pg_is_in_recovery 
-------------------
 t
(1 row)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;pgpool-ii-から見た-postgresql-ノードの状態確認-1&#34;&gt;pgpool-II から見た PostgreSQL ノードの状態確認&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html&#34;&gt;pgpool-II ユーザマニュアル&lt;/a&gt; の &lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html#show-commands&#34;&gt;SHOWコマンド&lt;/a&gt; の &lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html#pool_nodes&#34;&gt;pool_nodes&lt;/a&gt; に対応します。&lt;/p&gt;

&lt;p&gt;どちらかのコンテナで以下のコマンドを実行します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql1 ~]# sudo -i -u postgres psql -h localhost -p 9999 -c &amp;quot;show pool_nodes&amp;quot;
 node_id |   hostname    | port | status | lb_weight |  role   | select_cnt 
---------+---------------+------+--------+-----------+---------+------------
 0       | 10.155.92.101 | 5432 | 2      | 0.500000  | primary | 0
 1       | 10.155.92.102 | 5432 | 2      | 0.500000  | standby | 0
(2 rows)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;なお、 今回の設定では &lt;code&gt;postgres&lt;/code&gt; ユーザのパスワードを &lt;code&gt;/var/lib/pgsql/.pgpass&lt;/code&gt; に書いているのでパスワード入力は不要です。実運用時は書かないほうが良いでしょう。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;role&lt;/code&gt; 列の primary か standby で PostgreSQL のプライマリかスタンバイもわかるようですが、切替時はすぐに更新されなかったことがあったような気がします。&lt;/p&gt;

&lt;p&gt;切り替え直後は &lt;code&gt;select pg_is_in_recovery()&lt;/code&gt; を実行する方式のほうが良さそうです。&lt;/p&gt;

&lt;p&gt;status 列の値については &lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html&#34;&gt;pgpool-II ユーザマニュアル&lt;/a&gt; の &lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html#pcp_node_info&#34;&gt;pcp_node_info&lt;/a&gt; に説明があります。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;0 - 初期化時のみに表われる。PCP コマンドで表示されることはない。&lt;/li&gt;
&lt;li&gt;1 - ノード稼働中。接続無し&lt;/li&gt;
&lt;li&gt;2 - ノード稼働中。接続有り&lt;/li&gt;
&lt;li&gt;3 - ノードダウン&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;watchdog-から見た-pgpool-ii-の状態確認&#34;&gt;watchdog から見た pgpool-II の状態確認&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html&#34;&gt;pgpool-II ユーザマニュアル&lt;/a&gt; の &lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html#pcp_watchdog_info&#34;&gt;pcp_watchdog_info&lt;/a&gt; に対応します。&lt;/p&gt;

&lt;p&gt;以下のコマンドを実行します。 pgpool-II の管理者 &lt;code&gt;pgpool2&lt;/code&gt; のパスワードを聞かれますので入力してください。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql1 ~]# sudo -i -u postgres /usr/pgpool-9.5/bin/pcp_watchdog_info -h localhost -U pgpool2 -v
Password: 
Watchdog Cluster Information 
Total Nodes          : 2
Remote Nodes         : 1
Quorum state         : QUORUM EXIST
Alive Remote Nodes   : 1
VIP up on local node : YES
Master Node Name     : Linux_pgsql1_9999
Master Host Name     : 10.155.92.101

Watchdog Node Information 
Node Name      : Linux_pgsql1_9999
Host Name      : 10.155.92.101
Delegate IP    : 10.155.92.100
Pgpool port    : 9999
Watchdog port  : 9000
Node priority  : 1
Status         : 4
Status Name    : MASTER

Node Name      : Linux_pgsql2_9999
Host Name      : 10.155.92.102
Delegate IP    : 10.155.92.100
Pgpool port    : 9999
Watchdog port  : 9000
Node priority  : 1
Status         : 7
Status Name    : STANDBY

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;Status Name&lt;/code&gt; の種類は src/watchdog/watchdog.c 内にで定義されていました。 &lt;code&gt;Status&lt;/code&gt; はこの配列内のゼロオリジンのインデクスです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;char *wd_state_names[] = {
        &amp;quot;DEAD&amp;quot;,
        &amp;quot;LOADING&amp;quot;,
        &amp;quot;JOINING&amp;quot;,
        &amp;quot;INITIALIZING&amp;quot;,
        &amp;quot;MASTER&amp;quot;,
        &amp;quot;PARTICIPATING IN ELECTION&amp;quot;,
        &amp;quot;STANDING FOR MASTER&amp;quot;,
        &amp;quot;STANDBY&amp;quot;,
        &amp;quot;LOST&amp;quot;,
        &amp;quot;IN NETWORK TROUBLE&amp;quot;,
        &amp;quot;SHUTDOWN&amp;quot;,
        &amp;quot;ADD MESSAGE SENT&amp;quot;};
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;pgsql1-プライマリ-の-postgresql-を強制停止してフェイルオーバーのテスト&#34;&gt;pgsql1 (プライマリ)の PostgreSQL を強制停止してフェイルオーバーのテスト&lt;/h2&gt;

&lt;h3 id=&#34;フェイルオーバー時に呼び出されるスクリプト&#34;&gt;フェイルオーバー時に呼び出されるスクリプト&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html&#34;&gt;pgpool-II ユーザマニュアル&lt;/a&gt; の &lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html#failover_in_stream_mode&#34;&gt;Streaming Replicationでのフェイルオーバ&lt;/a&gt; に対応します。&lt;/p&gt;

&lt;p&gt;フェイルオーバー時には &lt;code&gt;/etc/pgpool-II-95/pgpool.conf&lt;/code&gt; の &lt;code&gt;failover_command&lt;/code&gt; に設定したスクリプトが実行されます。今回の構成では以下のような設定にしました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;failover_command = &#39;/var/lib/pgsql/9.5/data/pgpool_failover %d %P %H %R&#39;
                   # NOTE: %dなどの値は src/main/pgpool_main.c の trigger_failover_command で設定しています。
                   # Executes this command at failover
                   # Special values:
                   #   %d = node id
                   #   %h = host name
                   #   %p = port number
                   #   %D = database cluster path
                   #   %m = new master node id
                   #   %H = hostname of the new master node
                   #   %M = old master node id
                   #   %P = old primary node id
                   #   %r = new master port number
                   #   %R = new master database cluster path
                   #   %% = &#39;%&#39; character
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;/var/lib/pgsql/9.5/data/pgpool_failover&lt;/code&gt; は &lt;a href=&#34;https://github.com/hnakamur/postgresql-pgpool2-failover-example-playbook/blob/7f3ef8af54a4f9ec948d65bfc47e33db4792737d/roles/postgresql_db/templates/pgpool_failover.j2&#34;&gt;roles/postgresql_db/templates/pgpool_failover.j2&lt;/a&gt; から Ansible の template モジュールで生成しています。&lt;/p&gt;

&lt;h3 id=&#34;フェイルオーバーの実行&#34;&gt;フェイルオーバーの実行&lt;/h3&gt;

&lt;p&gt;実行後にどう動いたか確認できるように &lt;code&gt;logger&lt;/code&gt; コマンドでログを出力するようにしています。 &lt;code&gt;journalctl -f&lt;/code&gt; でログを &lt;code&gt;tail -f&lt;/code&gt; 的な感じで見られるので、これで見ながら実行します。&lt;/p&gt;

&lt;p&gt;初期状態では pgsql1 が PostgreSQL のプライマリになっています。&lt;/p&gt;

&lt;p&gt;pgsql2 で以下のコマンドを実行します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 ~]# journalctl -f | grep pgpool_failover
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pgsql1 で以下のコマンドを実行しバックグラウンド ( &lt;code&gt;&amp;amp;&lt;/code&gt; は 1つ) で PostgreSQL を強制停止しつつ、 journald のログを表示します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql1 ~]# sudo -i -u postgres /usr/pgsql-9.5/bin/pg_ctl stop -m immediate -D /var/lib/pgsql/9.5/data &amp;amp; journalctl -f | grep pgpool_failover
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;何回か試してみたのですが、 &lt;code&gt;/var/lib/pgsql/9.5/data/pgpool_failover&lt;/code&gt; は pgsql1 で実行される場合と pgsql2 で実行される場合があり、どうやらランダムにどちらか一方で実行されるということのようです。また今回の構成では root ユーザで実行されました。&lt;/p&gt;

&lt;p&gt;以下は pgsql1 での出力結果です。
Ctrl-C で &lt;code&gt;journalctl -f&lt;/code&gt; を停止します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[1] 2319
waiting for server to shut down.... done
server stopped

^C
[1]+  Done                  sudo -i -u postgres /usr/pgsql-9.5/bin/pg_ctl stop -m immediate -D /var/lib/pgsql/9.5/data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下は pgsql2 での出力結果です。
Ctrl-C で &lt;code&gt;journalctl -f&lt;/code&gt; を停止します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Sep 15 13:06:47 pgsql2 pgpool[1432]: 2016-09-15 13:06:47: pid 1432: LOG:  execute command: /var/lib/pgsql/9.5/data/pgpool_failover 0 0 10.155.92.102 /var/lib/pgsql/9.5/data
Sep 15 13:06:47 pgsql2 pgpool_failover[32612]: start args=0 0 10.155.92.102 /var/lib/pgsql/9.5/data UID=0
Sep 15 13:06:47 pgsql2 pgpool_failover[32620]: created promote_trigger file
^C
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;少ししてから PostgreSQL のノードの状態を確認すると以下のようになりました。
pgsql2 (10.155.92.102) の role が primary になり、 pgsql1 (10.155.92.101) は
role が standby で status が &lt;code&gt;3&lt;/code&gt; (ノードダウン) になっています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 ~]# sudo -i -u postgres psql -h localhost -p 9999 -c &amp;quot;show pool_nodes&amp;quot;
 node_id |   hostname    | port | status | lb_weight |  role   | select_cnt 
---------+---------------+------+--------+-----------+---------+------------
 0       | 10.155.92.101 | 5432 | 3      | 0.500000  | standby | 0
 1       | 10.155.92.102 | 5432 | 2      | 0.500000  | primary | 0
(2 rows)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pgsql2 で &lt;code&gt;select pg_is_in_recovery()&lt;/code&gt; を実行すると &lt;code&gt;f&lt;/code&gt; になってプライマリになっていることがわかります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 pgsql]# sudo -i -u postgres psql -c &amp;quot;select pg_is_in_recovery()&amp;quot;
 pg_is_in_recovery 
-------------------
 f
(1 row)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pgsql2 側での変更実験としてデータベースを作成してみます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 ~]# sudo -i -u postgres createdb foo
[root@pgsql2 ~]# sudo -i -u postgres psql -l
                             List of databases
   Name    |  Owner   | Encoding | Collate | Ctype |   Access privileges   
-----------+----------+----------+---------+-------+-----------------------
 foo       | postgres | UTF8     | C       | C     | 
 postgres  | postgres | UTF8     | C       | C     | 
 template0 | postgres | UTF8     | C       | C     | =c/postgres          +
           |          |          |         |       | postgres=CTc/postgres
 template1 | postgres | UTF8     | C       | C     | =c/postgres          +
           |          |          |         |       | postgres=CTc/postgres
(4 rows)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;なお、 pgpool-II 自体のマスタ・スタンバイの役割は変わらず同じで、 pgsql1 がマスタ、 pgsql2 がスタンバイで、 仮想IP は pgsql1 についた状態です。&lt;/p&gt;

&lt;h2 id=&#34;pgsql1-の-postgresql-をオンラインリカバリしスタンバイとして復帰させる&#34;&gt;pgsql1 の PostgreSQL をオンラインリカバリしスタンバイとして復帰させる&lt;/h2&gt;

&lt;h3 id=&#34;オンラインリカバリで呼び出される2つのスクリプト&#34;&gt;オンラインリカバリで呼び出される2つのスクリプト&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html&#34;&gt;pgpool-II ユーザマニュアル&lt;/a&gt; の &lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html#online_recovery_in_stream_mode&#34;&gt;Streaming Replicationでのオンラインリカバリ&lt;/a&gt; に対応します。&lt;/p&gt;

&lt;p&gt;オンラインリカバリで呼び出されるスクリプトは2つあります。この2つのスクリプトは &lt;code&gt;failover_command&lt;/code&gt; とは違って、必ずプライマリ側で &lt;code&gt;postgres&lt;/code&gt; ユーザで実行されます。&lt;/p&gt;

&lt;p&gt;1つ目は &lt;code&gt;/etc/pgpool-II-95/pgpool.conf&lt;/code&gt; の &lt;code&gt;recovery_1st_stage_command&lt;/code&gt; に指定したスクリプトです。
ここではファイル名のみが指定可能で、ディレクトリは PostgreSQL のデータディレクトリ (今回の構成では /var/lib/pgsql/9.5/data ) と決められています。
&lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html&#34;&gt;pgpool-II ユーザマニュアル&lt;/a&gt; の &lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html#RECOVERY_1ST_STAGE_COMMAND&#34;&gt;recovery_1st_stage_command&lt;/a&gt; によるとセキュリティ上の観点からそうしているそうです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;recovery_1st_stage_command = &#39;recovery_1st_stage&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;/var/lib/pgsql/9.5/data/recovery_1st_stage&lt;/code&gt; は &lt;a href=&#34;https://github.com/hnakamur/postgresql-pgpool2-failover-example-playbook/blob/7f3ef8af54a4f9ec948d65bfc47e33db4792737d/roles/postgresql_db/templates/recovery_1st_stage.j2&#34;&gt;roles/postgresql_db/templates/recovery_1st_stage.j2&lt;/a&gt; から Ansible の template モジュールで生成しています。&lt;/p&gt;

&lt;p&gt;今回の構成では古いデータディレクトリを &lt;code&gt;mv&lt;/code&gt; コマンドでリネームして、 &lt;code&gt;pg_basebackup&lt;/code&gt; コマンドでプライマリ・データベースの複製を作り、 &lt;code&gt;recovery.conf&lt;/code&gt; を作ってスタンバイとして稼働させる準備をしています。&lt;/p&gt;

&lt;p&gt;2つ目は PostgreSQL のデータディレクトリ下の &lt;code&gt;pgpool_remote_start&lt;/code&gt; というファイル名のスクリプトです。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html&#34;&gt;pgpool-II ユーザマニュアル&lt;/a&gt; の &lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html#pool_remote_start&#34;&gt;pgpool_remote_start&lt;/a&gt; に説明があります。&lt;/p&gt;

&lt;p&gt;こちらはファイル名が pgpool-II のソースコード &lt;code&gt;src/sql/pgpool-recovery/pgpool-recovery.c&lt;/code&gt; 内に&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#define REMOTE_START_FILE &amp;quot;pgpool_remote_start&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;のように固定の定義になっています。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;/var/lib/pgsql/9.5/data/pgpool_remote_start&lt;/code&gt; は &lt;a href=&#34;https://github.com/hnakamur/postgresql-pgpool2-failover-example-playbook/blob/7f3ef8af54a4f9ec948d65bfc47e33db4792737d/roles/postgresql_db/templates/pgpool_remote_start.j2&#34;&gt;roles/postgresql_db/templates/pgpool_remote_start.j2&lt;/a&gt; から Ansible の template モジュールで生成しています。&lt;/p&gt;

&lt;p&gt;処理内容はリモートのノードの PostgreSQL を起動するというものになっています。
ファイル名は &lt;code&gt;pgpool_remote_start&lt;/code&gt; なので最初見たときは &lt;code&gt;pgpool&lt;/code&gt; を起動するのかと勘違いしましたが、 &lt;code&gt;pgpool_&lt;/code&gt; は &lt;code&gt;pgpool&lt;/code&gt; のファイルであることを示す接頭辞的な意味合いのようです。
&lt;code&gt;pgpool&lt;/code&gt; を起動するなら &lt;code&gt;start_remote_pgoool&lt;/code&gt; のほうがわかりやすいでしょうしね。&lt;/p&gt;

&lt;p&gt;なお、 &lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html&#34;&gt;pgpool-II ユーザマニュアル&lt;/a&gt; の &lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html#pool_remote_start&#34;&gt;pgpool_remote_start&lt;/a&gt; で書かれているサンプルスクリプトでは &lt;code&gt;pg_ctl&lt;/code&gt; コマンドで PostgreSQL を起動していますが、 &lt;code&gt;systemctl status postgresql-9.5&lt;/code&gt; でサービス状態が確認できなくなってしまうため、 &lt;a href=&#34;https://github.com/hnakamur/postgresql-pgpool2-failover-example-playbook/blob/7f3ef8af54a4f9ec948d65bfc47e33db4792737d/roles/postgresql_db/templates/pgpool_remote_start.j2&#34;&gt;roles/postgresql_db/templates/pgpool_remote_start.j2&lt;/a&gt; では &lt;code&gt;sudo systemctl start postgresql-9.5&lt;/code&gt; で起動しています。
またそのために postgres ユーザ用の sudoers 設定も &lt;a href=&#34;https://github.com/hnakamur/postgresql-pgpool2-failover-example-playbook/blob/7f3ef8af54a4f9ec948d65bfc47e33db4792737d/roles/postgresql/templates/sudoers_postgres.j2&#34;&gt;roles/postgresql/templates/sudoers_postgres.j2&lt;/a&gt; を元に &lt;code&gt;/etc/sudoers.d/01_postgres&lt;/code&gt; を生成し行っています。&lt;/p&gt;

&lt;h3 id=&#34;リカバリの実行&#34;&gt;リカバリの実行&lt;/h3&gt;

&lt;p&gt;今回は &lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html#perform_online_recovery&#34;&gt;リカバリの実行&lt;/a&gt; で説明されている &lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html#pcp_recovery_node&#34;&gt;pcp_recovery_node&lt;/a&gt; コマンドを使います。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;pcp_recovery_node&lt;/code&gt; コマンドは pgpool-II のユーザ &lt;code&gt;pgpool2&lt;/code&gt; のパスワードを入力する必要があるためフォアグラウンドで実行し ( &lt;code&gt;&amp;amp;&lt;/code&gt; は &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt; と2つ)、ログを表示します。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;pcp_recovery_node&lt;/code&gt; コマンドは pgsql1 と pgsql2 のどちらで実行しても良いようです。ここでは対象のサーバが pgsql1 なので対応する &lt;code&gt;node_id&lt;/code&gt; の &lt;code&gt;0&lt;/code&gt; を引数の最後に指定しています。 &lt;code&gt;node_id&lt;/code&gt; は上記の &lt;code&gt;show pool_nodes&lt;/code&gt; の出力で確認できます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 pgsql]# sudo -i -u postgres /usr/pgpool-9.5/bin/pcp_recovery_node -h localhost -p 9898 -U pgpool2 0
Password: 
pcp_recovery_node -- Command Successful
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ログを確認すると以下のようになっていました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 pgsql]# journalctl | grep -E &#39;(recovery_1st_stage|pgpool_remote_start)&#39;
Sep 15 14:03:38 pgsql2 pgpool[1416]: 2016-09-15 14:03:38: pid 1700: DETAIL:  starting recovery command: &amp;quot;SELECT pgpool_recovery(&#39;recovery_1st_stage&#39;, &#39;10.155.92.101&#39;, &#39;/var/lib/pgsql/9.5/data&#39;, &#39;5432&#39;)&amp;quot;
Sep 15 14:03:38 pgsql2 recovery_1st_stage[1704]: start args=/var/lib/pgsql/9.5/data 10.155.92.101 /var/lib/pgsql/9.5/data 5432 UID=26
Sep 15 14:03:39 pgsql2 recovery_1st_stage[1713]: pg_basebackup done.
Sep 15 14:03:39 pgsql2 recovery_1st_stage[1716]: created archive_status.
Sep 15 14:03:39 pgsql2 recovery_1st_stage[1719]: created recovery.conf.
Sep 15 14:03:39 pgsql2 pgpool_remote_start[1722]: start args=10.155.92.101 /var/lib/pgsql/9.5/data UID=26
Sep 15 14:03:42 pgsql2 pgpool_remote_start[1738]: finished to start postgresql service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;しばらくして PostgreSQL の状態を確認すると以下のように pgsql1 の role が standby で status が 2 (ノード稼働中。接続有り) になりました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 pgsql]# sudo -i -u postgres psql -h localhost -p 9999 -c &amp;quot;show pool_nodes&amp;quot;
 node_id |   hostname    | port | status | lb_weight |  role   | select_cnt 
---------+---------------+------+--------+-----------+---------+------------
 0       | 10.155.92.101 | 5432 | 2      | 0.500000  | standby | 0
 1       | 10.155.92.102 | 5432 | 2      | 0.500000  | primary | 0
(2 rows)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ps を実行してみると pgsql2 で &lt;code&gt;postgres: wal sender process&lt;/code&gt; が稼働しており、 pgsql1 で &lt;code&gt;postgres: wal receiver process&lt;/code&gt; が稼働しており、ストリーミング・レプリケーションが動いていることが確認できました。&lt;/p&gt;

&lt;h2 id=&#34;pgsql1-スタンバイ-の-postgresql-を強制停止してみる&#34;&gt;pgsql1 (スタンバイ) の PostgreSQL を強制停止してみる&lt;/h2&gt;

&lt;h3 id=&#34;スタンバイの-postgresql-を強制停止&#34;&gt;スタンバイの PostgreSQL を強制停止&lt;/h3&gt;

&lt;p&gt;pgsql2 で以下のコマンドを実行します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 ~]# journalctl -f | grep pgpool_failover
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pgsql1 で以下のコマンドを実行します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql1 ~]# sudo -i -u postgres /usr/pgsql-9.5/bin/pg_ctl stop -m immediate -D /var/lib/pgsql/9.5/data &amp;amp; journalctl -f | grep pgpool_failover
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;今回は pgsql2 に以下のログが出ました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Sep 15 14:21:05 pgsql2 pgpool[1416]: 2016-09-15 14:21:05: pid 1416: LOG:  execute command: /var/lib/pgsql/9.5/data/pgpool_failover 0 1 10.155.92.102 /var/lib/pgsql/9.5/data
Sep 15 14:21:05 pgsql2 pgpool_failover[2466]: start args=0 1 10.155.92.102 /var/lib/pgsql/9.5/data UID=0
Sep 15 14:21:05 pgsql2 pgpool_failover[2468]: do nothing since failed node was not primary
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;停止された PostgreSQL がプライマリではなかったのでフェイルオーバは行わず、プライマリ (pgsql2) をそのままスタンドアロンで稼働させています。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html&#34;&gt;pgpool-II ユーザマニュアル&lt;/a&gt; の &lt;a href=&#34;http://www.pgpool.net/docs/latest/pgpool-ja.html#failover_in_stream_mode&#34;&gt;Streaming Replicationでのフェイルオーバ&lt;/a&gt; に上げられているフェイルオーバ用スクリプトではノード 0 がプライマリで 1 がスタンバイという想定で書かれているため、今回の状況ではうまく行きませんでした。
が、 &lt;a href=&#34;https://github.com/hnakamur/postgresql-pgpool2-failover-example-playbook/blob/7f3ef8af54a4f9ec948d65bfc47e33db4792737d/roles/postgresql_db/templates/pgpool_failover.j2&#34;&gt;roles/postgresql_db/templates/pgpool_failover.j2&lt;/a&gt; では &lt;code&gt;pgpool.conf&lt;/code&gt; の &lt;code&gt;failover_command&lt;/code&gt; の4つの引数のうち、最初の2つに停止したノード &lt;code&gt;%d&lt;/code&gt; と旧プライマリノード %P を渡していて、値が違う場合はスタンバイと判定していますので、一度フェイルオーバ→リカバリをした後でも大丈夫です。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;show pool_nodes&lt;/code&gt; の実行結果は以下の通りです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 pgsql]# sudo -u postgres psql -h localhost -p 9999 -c &amp;quot;show pool_nodes&amp;quot;
 node_id |   hostname    | port | status | lb_weight |  role   | select_cnt 
---------+---------------+------+--------+-----------+---------+------------
 0       | 10.155.92.101 | 5432 | 3      | 0.500000  | standby | 0
 1       | 10.155.92.102 | 5432 | 2      | 0.500000  | primary | 0
(2 rows)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pgsql2 で postgres のプロセスを見ると &lt;code&gt;wal sender&lt;/code&gt; がいないので、スタンドアロン状態であることがわかります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 pgsql]# ps axf | grep [p]ostgres
 1370 ?        S      0:00 /usr/pgsql-9.5/bin/postgres -D /var/lib/pgsql/9.5/data
 1371 ?        Ss     0:00  \_ postgres: logger process   
 1378 ?        Ss     0:00  \_ postgres: checkpointer process   
 1379 ?        Ss     0:00  \_ postgres: writer process   
 1381 ?        Ss     0:00  \_ postgres: stats collector process   
 1554 ?        Ss     0:00  \_ postgres: wal writer process   
 1555 ?        Ss     0:00  \_ postgres: autovacuum launcher process   
 1556 ?        Ss     0:00  \_ postgres: archiver process   last was 000000020000000000000004.00000060.backup
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;スタンドアロン状態であることは &lt;code&gt;select * from pg_stat_replication&lt;/code&gt; の結果が 0であることからもわかります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 pgsql]# sudo -i -u postgres psql -x -c &amp;quot;select * from pg_stat_replication&amp;quot;
(0 rows)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;スタンバイの-postgresql-を復活させる&#34;&gt;スタンバイの PostgreSQL を復活させる&lt;/h3&gt;

&lt;h4 id=&#34;単に-postgresql-を起動すれば動く場合&#34;&gt;単に PostgreSQL を起動すれば動く場合&lt;/h4&gt;

&lt;p&gt;実運用の際はデータディレクトリの状況を調査したりするところですが、ここでは問題ないという前提で、 pgsql1 で PostgreSQL を起動して pgpool-II の管理下に追加します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql1 pgsql]# systemctl start postgresql-9.5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;で起動し&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql1 pgsql]# systemctl status postgresql-9.5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;で状態を確認します。
Active の行が以下のように &lt;code&gt;active (running)&lt;/code&gt; になっていれば OK です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   Active: active (running) since Thu 2016-09-15 14:36:12 UTC; 3s ago
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;しばらくすると pgpool-II の heartbeat で pgsql1 の PostgreSQL が稼働していることを検知し、 pgsql2 をプライマリとするリプリケーションが再開されます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql1 pgsql]# ps axf | grep [p]ostgres
 2778 ?        S      0:00 /usr/pgsql-9.5/bin/postgres -D /var/lib/pgsql/9.5/data
 2779 ?        Ss     0:00  \_ postgres: logger process   
 2780 ?        Ss     0:00  \_ postgres: startup process   recovering 000000020000000000000005
 2785 ?        Ss     0:00  \_ postgres: checkpointer process   
 2786 ?        Ss     0:00  \_ postgres: writer process   
 2787 ?        Ss     0:00  \_ postgres: stats collector process   
 2788 ?        Ss     0:00  \_ postgres: wal receiver process   streaming 0/5000680
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 pgsql]# ps axf | grep [p]ostgres
 1370 ?        S      0:00 /usr/pgsql-9.5/bin/postgres -D /var/lib/pgsql/9.5/data
 1371 ?        Ss     0:00  \_ postgres: logger process   
 1378 ?        Ss     0:00  \_ postgres: checkpointer process   
 1379 ?        Ss     0:00  \_ postgres: writer process   
 1381 ?        Ss     0:00  \_ postgres: stats collector process   
 1554 ?        Ss     0:00  \_ postgres: wal writer process   
 1555 ?        Ss     0:00  \_ postgres: autovacuum launcher process   
 1556 ?        Ss     0:00  \_ postgres: archiver process   last was 000000020000000000000004.00000060.backup
 3111 ?        Ss     0:00  \_ postgres: wal sender process repl_user 10.155.92.101(57142) streaming 0/5000680
[root@pgsql2 pgsql]# sudo -i -u postgres psql -x -c &amp;quot;select * from pg_stat_replication&amp;quot;
-[ RECORD 1 ]----+------------------------------
pid              | 3111
usesysid         | 16394
usename          | repl_user
application_name | walreceiver
client_addr      | 10.155.92.101
client_hostname  | 
client_port      | 57142
backend_start    | 2016-09-15 14:36:12.312321+00
backend_xmin     | 1842
state            | streaming
sent_location    | 0/5000680
write_location   | 0/5000680
flush_location   | 0/5000680
replay_location  | 0/5000680
sync_priority    | 0
sync_state       | async
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;スタンバイデータベースを作り直す場合&#34;&gt;スタンバイデータベースを作り直す場合&lt;/h4&gt;

&lt;p&gt;スタンバイデータベースの損傷がひどくて起動できない場合は、プライマリデータベースを &lt;code&gt;pg_basebackup&lt;/code&gt; コマンドで複製して作り直し、スタンバイ用の設定を加えて起動することになります。&lt;/p&gt;

&lt;p&gt;これは上記の「リカバリの実行」でやっていることと同じなので、今のプライマリである pgsql2 で以下のコマンドを実行すれば OK です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pgsql2 pgsql]# sudo -i -u postgres /usr/pgpool-9.5/bin/pcp_recovery_node -h localhost -p 9898 -U pgpool2 0
Password: 
pcp_recovery_node -- Command Successful
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;

&lt;p&gt;さらに pgpool-II が落ちたときやサーバ全体が落ちたときなども検証が必要ですがこのブログ記事を書くのに、今日の早朝と今でなんだかんだで4時間ぐらいはかかっているので (環境構築と動作検証には3日かかってます)、一旦ここまでとします。&lt;/p&gt;

&lt;p&gt;pgpool-II でのフェイルオーバ、リカバリは呼び出されるスクリプトをどうすれば良いのかが最初は全くわからなくて苦労しましたが、ドキュメントとソースを読んで理解できたので良かったです。&lt;/p&gt;

&lt;p&gt;Pacemaker は高機能なんですが複雑すぎるように私には思えたので、 pgpool-II のほうが好感触です。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pacemakerを使ってPostgreSQLのアクティブ・スタンバイ(1&#43;1構成)を試してみた</title>
      <link>https://hnakamur.github.io/blog/2016/08/21/experiment-postgresql-active-standby-cluster-using-pacemaker/</link>
      <pubDate>Sun, 21 Aug 2016 11:23:01 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2016/08/21/experiment-postgresql-active-standby-cluster-using-pacemaker/</guid>
      <description>

&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;

&lt;p&gt;STONITH無し、quorum無しのアクティブ・スタンバイ(1+1構成)がとりあえず動くところまでは来たので、一旦メモです。&lt;/p&gt;

&lt;h2 id=&#34;参考資料&#34;&gt;参考資料&lt;/h2&gt;

&lt;p&gt;以下の資料と連載記事がわかりやすくて非常に参考になりました。ありがとうございます！&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://linux-ha.osdn.jp/wp/archives/3244&#34;&gt;JPUG 第23回しくみ+アプリケーション勉強会 セミナー資料公開 « Linux-HA Japan&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://linux-ha.osdn.jp/wp/wp-content/uploads/pacemaker_20120526JPUG.pdf&#34;&gt;HAクラスタでPostgreSQLを高可用化(前編) ～Pacemaker入門編～(PDF)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://linux-ha.osdn.jp/wp/wp-content/uploads/b754c737d835c2546415009387407b7b.pdf&#34;&gt;PostgreSQLを高可用化(後編) 〜レプリケーション編〜(PDF)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://linux-ha.osdn.jp/wp/archives/3589&#34;&gt;OSC 2013 Tokyo/Spring 講演資料公開 « Linux-HA Japan&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.slideshare.net/takmatsuo/osc-tokyospring2013-16694861&#34;&gt;Pacemaker+PostgreSQLレプリケーションで共有ディスクレス高信頼クラスタの構築＠OSC 2013 Tokyo/Spring&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://gihyo.jp/admin/serial/01/pacemaker&#34;&gt;Pacemakerでかんたんクラスタリング体験してみよう！：連載｜gihyo.jp … 技術評論社&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;さらに以下の記事と電子書籍も参考にしました。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://clusterlabs.org/wiki/PgSQL_Replicated_Cluster&#34;&gt;PgSQL Replicated Cluster - ClusterLabs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://shop.oreilly.com/product/9781783550609.do&#34;&gt;PostgreSQL Replication, 2nd Edition - O&amp;rsquo;Reilly Media&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;テスト用のansible-playbook&#34;&gt;テスト用のAnsible playbook&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/hnakamur/postgresql-pacemaker-example-playbook&#34;&gt;https://github.com/hnakamur/postgresql-pacemaker-example-playbook&lt;/a&gt;
に置きました。&lt;/p&gt;

&lt;p&gt;LXD をセットアップ済みの Ubuntu 16.04 上で試しました。&lt;/p&gt;

&lt;h2 id=&#34;セットアップの事前準備&#34;&gt;セットアップの事前準備&lt;/h2&gt;

&lt;p&gt;上記のplaybookを取得します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/hnakamur/postgresql-pacemaker-example-playbook
cd postgresql-pacemaker-example-playbook
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ansibleの &lt;code&gt;lxd_container&lt;/code&gt; モジュールを使うので、virtualenvで仮想環境を作ってAnsibleのmaster版をインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;virtualenv venv
source venv/bin/activate
pip install git+https://github.com/ansible/ansible
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;今回はコンテナのIPアドレスをDHCPではなく静的アドレスを使うようにしてみました。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;/etc/default/lxd-bridge&lt;/code&gt; の &lt;code&gt;LXD_IPV4_DHCP_RANGE&lt;/code&gt; に DHCP のアドレス範囲が設定されているので、ファイルを編集して範囲を狭めます。私の環境では以下のようにしました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## IPv4 network (e.g. 10.0.8.0/24)
LXD_IPV4_NETWORK=&amp;quot;10.155.92.1/24&amp;quot;

## IPv4 DHCP range (e.g. 10.0.8.2,10.0.8.254)
LXD_IPV4_DHCP_RANGE=&amp;quot;10.155.92.200,10.155.92.254&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;LXDをインストールしたときに &lt;code&gt;LXD_IPV4_NETWORK&lt;/code&gt; はランダムなアドレスになるかあるいは自分で指定しますので、それに応じた値に適宜変更してください。&lt;/p&gt;

&lt;p&gt;変更したら &lt;code&gt;lxd-bridge&lt;/code&gt; を再起動して変更を反映します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl restart lxd-bridge
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;group_vars/development/vars.yml&lt;/code&gt; ファイル内のIPアドレスも適宜変更します。&lt;/p&gt;

&lt;p&gt;また、 &lt;code&gt;group_vars/development/secrets.yml&lt;/code&gt; 内にパスワードやsshの鍵ペアなどが含まれています。これを違う値に変更したい場合は以下のようにします。&lt;/p&gt;

&lt;p&gt;まず、以下のコマンドを実行して一旦復号化します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ansible-vault decrypt group_vars/development/secrets.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;vaultのパスワードを聞かれますので入力します。この例では &lt;code&gt;password&lt;/code&gt; としています。これはあくまで例なのでこういう弱いパスワードにしていますが、実際の案件で使うときは、もっと強いパスワードを指定してください。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;group_vars/development/secrets.yml&lt;/code&gt; 内の変数を適宜変更したら、以下のコマンドを実行して暗号化します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ansible-vault encrypt group_vars/development/secrets.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;vaultの新しいパスワードを聞かれますので入力してください。&lt;/p&gt;

&lt;h2 id=&#34;コンテナの作成&#34;&gt;コンテナの作成&lt;/h2&gt;

&lt;p&gt;以下のコマンドを実行して &lt;code&gt;node1&lt;/code&gt; と &lt;code&gt;node2&lt;/code&gt; という2つのコンテナを作成します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ansible-playbook launch_containers.yml -D -v
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;vaultのパスワードを聞かれますので入力してください。&lt;/p&gt;

&lt;h2 id=&#34;コンテナ内にpostgresqlとpacemakerをセットアップ&#34;&gt;コンテナ内にPostgreSQLとPacemakerをセットアップ&lt;/h2&gt;

&lt;p&gt;以下のコマンドを実行して、コンテナ内にPostgreSQLとPacemakerをセットアップします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ansible-playbook setup_containers.yml -D -v
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ここでは、セットアップ完了後、アクティブスタンバイ構成が開始するまでの時間を図りたいので、以下のように &lt;code&gt;date -u&lt;/code&gt; コマンドも実行するようにします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ansible-playbook setup_containers.yml -D -v; date -u
…(略)…
Sun Aug 21 13:51:21 UTC 2016
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下のコマンドを実行して &lt;code&gt;node2&lt;/code&gt; コンテナに入ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ lxc exec node2 bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下のコマンドを実行して、クラスタの状態をモニターします。
&lt;code&gt;node1&lt;/code&gt;, &lt;code&gt;node2&lt;/code&gt; が両方 Slaves の状態を経て、 &lt;code&gt;node1&lt;/code&gt; が Master になり master-ip が &lt;code&gt;node1&lt;/code&gt; につくまで待ちます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Last updated: Sun Aug 21 13:52:07 2016          Last change: Sun Aug 21 13:52:03 2016 by root via crm_attribute on node1
Stack: corosync
Current DC: node1 (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum
2 nodes and 3 resources configured

Online: [ node1 node2 ]

 Master/Slave Set: pgsql-master [pgsql]
     Masters: [ node1 ]
     Slaves: [ node2 ]
master-ip       (ocf::heartbeat:IPaddr2):       Started node1

Node Attributes:
* Node node1:
    + master-pgsql                      : 1000
    + pgsql-data-status                 : LATEST
    + pgsql-master-baseline             : 0000000003000098
    + pgsql-status                      : PRI
    + pgsql-xlog-loc                    : 0000000003000098
* Node node2:
    + master-pgsql                      : -INFINITY
    + pgsql-data-status                 : STREAMING|ASYNC
    + pgsql-status                      : HS:async
    + pgsql-xlog-loc                    : 0000000003000000

Migration Summary:
* Node node2:
* Node node1:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;この端末は開いたままにしておきます。&lt;/p&gt;

&lt;h2 id=&#34;node1-コンテナを強制停止してフェールオーバのテスト&#34;&gt;node1 コンテナを強制停止してフェールオーバのテスト&lt;/h2&gt;

&lt;p&gt;別の端末を開いて以下のコマンドを実行し、 &lt;code&gt;node1&lt;/code&gt; コンテナを強制停止し時刻を記録します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ lxc stop -f node1; date -u
Sun Aug 21 13:52:57 UTC 2016
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;しばらくすると　&lt;code&gt;crm_mon -fA&lt;/code&gt; の出力が以下のようになります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Last updated: Sun Aug 21 13:53:11 2016          Last change: Sun Aug 21 13:53:05 2016 by root via crm_attribute on node2
Stack: corosync
Current DC: node2 (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum
2 nodes and 3 resources configured

Online: [ node2 ]
OFFLINE: [ node1 ]

 Master/Slave Set: pgsql-master [pgsql]
     Masters: [ node2 ]
master-ip       (ocf::heartbeat:IPaddr2):       Started node2

Node Attributes:
* Node node2:
    + master-pgsql                      : 1000
    + pgsql-data-status                 : LATEST
    + pgsql-master-baseline             : 00000000030001A8
    + pgsql-status                      : PRI
    + pgsql-xlog-loc                    : 0000000003000000

Migration Summary:
* Node node2:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;LXDホストで以下のコマンドを実行して &lt;code&gt;node1&lt;/code&gt; を起動します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ lxc start node1; date -u
Sun Aug 21 13:53:58 UTC 2016
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;起動後しばらくしても &lt;code&gt;node1&lt;/code&gt; はオフラインのままですが、これは意図した挙動です。実際のケースではディスク障害などが起きているかもしれないので、マシンの状況を確認してから手動でクラスタに復帰させることになるためです。&lt;/p&gt;

&lt;p&gt;以下のコマンドで &lt;code&gt;node1&lt;/code&gt; コンテナに入ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ lxc exec node1 bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;PacemakerがPostgreSQLのロックファイルを作っているのでそれを削除します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@node1 ~]# ll /var/run/postgresql/
total 4
-rw-r----- 1 root     root      0 Aug 21 13:52 PGSQL.lock
-rw-r----- 1 postgres postgres 36 Aug 21 13:52 rep_mode.conf
[root@node1 ~]# rm /var/run/postgresql/PGSQL.lock
rm: remove regular empty file &#39;/var/run/postgresql/PGSQL.lock&#39;? y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下のコマンドで &lt;code&gt;node1&lt;/code&gt; をクラスタに復帰させ、時刻を記録します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@node1 ~]# pcs cluster start node1; date -u
node1: Starting Cluster...
Sun Aug 21 13:55:30 UTC 2016
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;15秒後、 &lt;code&gt;crm_mon -fA&lt;/code&gt; の画面で &lt;code&gt;node1&lt;/code&gt; の PostgreSQL が Slaves に追加されました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Last updated: Sun Aug 21 13:55:45 2016          Last change: Sun Aug 21 13:55:42 2016 by root via crm_attribute on node2
Stack: corosync
Current DC: node2 (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum
2 nodes and 3 resources configured

Online: [ node1 node2 ]

 Master/Slave Set: pgsql-master [pgsql]
     Masters: [ node2 ]
     Slaves: [ node1 ]
master-ip       (ocf::heartbeat:IPaddr2):       Started node2

Node Attributes:
* Node node1:
    + master-pgsql                      : 100
    + pgsql-data-status                 : STREAMING|SYNC
    + pgsql-status                      : HS:sync
* Node node2:
    + master-pgsql                      : 1000
    + pgsql-data-status                 : LATEST
    + pgsql-master-baseline             : 00000000030001A8
    + pgsql-status                      : PRI
    + pgsql-xlog-loc                    : 0000000003000000

Migration Summary:
* Node node2:
* Node node1:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ここで、 &lt;code&gt;node2&lt;/code&gt; で &lt;code&gt;crm_mon -fA&lt;/code&gt; を実行していた端末で Control-C を入力してモニターを終了します。&lt;/p&gt;

&lt;h2 id=&#34;postgresqlのプロセスを強制終了してフェールオーバのテスト&#34;&gt;PostgreSQLのプロセスを強制終了してフェールオーバのテスト&lt;/h2&gt;

&lt;p&gt;今度は &lt;code&gt;node2&lt;/code&gt; の PostgreSQL のプロセスを強制終了してフェールオーバしてみます。&lt;/p&gt;

&lt;p&gt;経過を見るために &lt;code&gt;node1&lt;/code&gt; で以下のコマンドを実行して、その端末を開いたままにしておきます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@node1 ~]# crm_mon -fA
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;開始時点では以下のような出力になっていました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Last updated: Sun Aug 21 13:57:17 2016          Last change: Sun Aug 21 13:55:42 2016 by root via crm_attribute on node2
Stack: corosync
Current DC: node2 (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum
2 nodes and 3 resources configured

Online: [ node1 node2 ]

 Master/Slave Set: pgsql-master [pgsql]
     Masters: [ node2 ]
     Slaves: [ node1 ]
master-ip       (ocf::heartbeat:IPaddr2):       Started node2

Node Attributes:
* Node node1:
    + master-pgsql                      : 100
    + pgsql-data-status                 : STREAMING|SYNC
    + pgsql-status                      : HS:sync
* Node node2:
    + master-pgsql                      : 1000
    + pgsql-data-status                 : LATEST
    + pgsql-master-baseline             : 00000000030001A8
    + pgsql-status                      : PRI
    + pgsql-xlog-loc                    : 0000000003000000

Migration Summary:
* Node node2:
* Node node1:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;node2&lt;/code&gt; で以下のコマンドを実行して PostgreSQL のプロセスを強制終了し、時刻を記録します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@node2 ~]# kill -KILL `head -1 /var/lib/pgsql/9.5/data/postmaster.pid`; date -u
Sun Aug 21 13:58:20 UTC 2016
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;11秒後 &lt;code&gt;node1&lt;/code&gt; の PostgreSQL が Masterに昇格されました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Last updated: Sun Aug 21 13:58:31 2016          Last change: Sun Aug 21 13:58:27 2016 by root via crm_attribute on node1
Stack: corosync
Current DC: node2 (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum
2 nodes and 3 resources configured

Online: [ node1 node2 ]

 Master/Slave Set: pgsql-master [pgsql]
     Masters: [ node1 ]
master-ip       (ocf::heartbeat:IPaddr2):       Started node1

Node Attributes:
* Node node1:
    + master-pgsql                      : 1000
    + pgsql-data-status                 : LATEST
    + pgsql-master-baseline             : 0000000003000398
    + pgsql-status                      : PRI
* Node node2:
    + master-pgsql                      : -INFINITY
    + pgsql-data-status                 : DISCONNECT
    + pgsql-status                      : STOP

Migration Summary:
* Node node2:
   pgsql: migration-threshold=2 fail-count=1000000 last-failure=&#39;Sun Aug 21 13:58:23 2016&#39;
* Node node1:

Failed Actions:
* pgsql_start_0 on node2 &#39;unknown error&#39; (1): call=23, status=complete, exitreason=&#39;My data may be inconsistent. You have to remove /va
r/run/postgresql/PGSQL.lock file to force start.&#39;,
    last-rc-change=&#39;Sun Aug 21 13:58:23 2016&#39;, queued=0ms, exec=383ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;次に、 &lt;code&gt;node2&lt;/code&gt; の PostgreSQL を再び稼働してスタンバイにさせてみます。&lt;/p&gt;

&lt;p&gt;まず Pacemaker が作成した PostgreSQL のロックファイル &lt;code&gt;/var/run/postgresql/PGSQL.lock&lt;/code&gt; を削除します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@node2 ~]# ll /var/run/postgresql/
total 4
-rw-r----- 1 root     root      0 Aug 21 13:53 PGSQL.lock
-rw-r----- 1 postgres postgres 31 Aug 21 13:58 rep_mode.conf
[root@node2 ~]# \rm /var/run/postgresql/PGSQL.lock
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;次に以下のコマンドを実行して &lt;code&gt;node2&lt;/code&gt; のPostgreSQL の failcount をリセットし、時刻を記録します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@node2 ~]# pcs resource failcount reset pgsql node2; date -u
Sun Aug 21 14:00:04 UTC 2016
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;9秒後、 &lt;code&gt;node1&lt;/code&gt; での &lt;code&gt;crm_mon -fA&lt;/code&gt; の出力を見ると &lt;code&gt;node2&lt;/code&gt; がスタンバイになりました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Last updated: Sun Aug 21 14:00:13 2016          Last change: Sun Aug 21 14:00:10 2016 by root via crm_attribute on node1
Stack: corosync
Current DC: node2 (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum
2 nodes and 3 resources configured

Online: [ node1 node2 ]

 Master/Slave Set: pgsql-master [pgsql]
     Masters: [ node1 ]
     Slaves: [ node2 ]
master-ip       (ocf::heartbeat:IPaddr2):       Started node1

Node Attributes:
* Node node1:
    + master-pgsql                      : 1000
    + pgsql-data-status                 : LATEST
    + pgsql-master-baseline             : 0000000003000398
    + pgsql-status                      : PRI
* Node node2:
    + master-pgsql                      : 100
    + pgsql-data-status                 : STREAMING|SYNC
    + pgsql-status                      : HS:sync

Migration Summary:
* Node node2:
* Node node1:

Failed Actions:
* pgsql_start_0 on node2 &#39;unknown error&#39; (1): call=23, status=complete, exitreason=&#39;My data may be inconsistent. You have to remove /va
r/run/postgresql/PGSQL.lock file to force start.&#39;,
    last-rc-change=&#39;Sun Aug 21 13:58:23 2016&#39;, queued=0ms, exec=383ms
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;おわりに&#34;&gt;おわりに&lt;/h2&gt;

&lt;p&gt;STONITH無し、quorum無しという簡易構成ですが、アクティブ・スタンバイ(1+1構成)でフフェールオーバする検証ができました。本番運用するにはSTONITHやquorumも重要そうなので、そちらも調べて行きたいです。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LXDコンテナ上でPacemakerを使って仮想IPとApacheのアクティブ・パッシブ・クラスタを試してみた</title>
      <link>https://hnakamur.github.io/blog/2016/08/12/experiment-vip-and-apache-with-pacemaker-on-lxd-containers/</link>
      <pubDate>Fri, 12 Aug 2016 18:54:27 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2016/08/12/experiment-vip-and-apache-with-pacemaker-on-lxd-containers/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://clusterlabs.org/doc/&#34;&gt;Cluster Labs - Pacemaker Documentation&lt;/a&gt; の &amp;ldquo;Pacemaker 1.1 for Corosync 2.x and pcs&amp;rdquo; の &amp;ldquo;Clusters from Scratch (en-US)&amp;rdquo; を参考にしつつ、多少手順を変更して試してみました。&lt;/p&gt;

&lt;h2 id=&#34;実験用コンテナの環境構築&#34;&gt;実験用コンテナの環境構築&lt;/h2&gt;

&lt;h3 id=&#34;コンテナの作成&#34;&gt;コンテナの作成&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://hnakamur.github.io/blog/blog/2016/08/12/update-lxd-dnsmasq-dhcp-hosts-config-with-sighup/&#34;&gt;LXDのdnsmasqの固定IP設定をSIGHUPで更新する · hnakamur&amp;rsquo;s blog at github&lt;/a&gt; の手法を使って、2つのコンテナ用のIPアドレスを設定しておきます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lxdhost:~$ cat /var/lib/lxd-bridge/dhcp-hosts 
pcmk-1,10.155.92.101
pcmk-2,10.155.92.102
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;また、仮想IPとして &lt;code&gt;10.155.92.100&lt;/code&gt; を使用しますので、 &lt;code&gt;/var/lib/lxd-bridge/dnsmasq.lxdbr0.leases&lt;/code&gt; で使われていないことを確認しておきます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo kill -HUP `cat /var/run/lxd-bridge/dnsmasq.pid`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;で設定を dnsmasq に反映します。&lt;/p&gt;

&lt;p&gt;なお、  &lt;a href=&#34;http://clusterlabs.org/doc/en-US/Pacemaker/1.1-pcs/html/Clusters_from_Scratch/ch02.html#_configure_network&#34;&gt;2.1.3. Configure Network&lt;/a&gt; の &amp;ldquo;Important&amp;rdquo; の囲み部分によるとDHCPはcorosyncと干渉するので、 &lt;strong&gt;クラスタのマシンはDHCPを決して使うべきではない&lt;/strong&gt; そうです。この記事はあくまでPacemakerの使い方を把握するために試してみるだけなので気にしないことにしますが、実運用の際には DHCP を使わない構成にする必要があります。&lt;/p&gt;

&lt;p&gt;以下のコマンドでコンテナ &lt;code&gt;pcmk-1&lt;/code&gt; と &lt;code&gt;pcmk-2&lt;/code&gt; を作成します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lxdhost:~$ lxc launch images:centos/7/amd64 pcmk-1
lxdhost:~$ lxc launch images:centos/7/amd64 pcmk-2
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;コンテナ内の-etc-hosts-設定&#34;&gt;コンテナ内の /etc/hosts 設定&lt;/h3&gt;

&lt;p&gt;端末を2つ開いて &lt;code&gt;lxc exec pcmk-1 bash&lt;/code&gt; と &lt;code&gt;lxc exec pcmk-2 bash&lt;/code&gt; を実行し、それぞれ環境構築していきます。&lt;/p&gt;

&lt;p&gt;まず、コンテナ作成直後の &lt;code&gt;pcmk-1&lt;/code&gt; の &lt;code&gt;/etc/hosts&lt;/code&gt; を確認すると以下のようになっていました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;127.0.0.1   localhost
127.0.1.1   pcmk-1

# The following lines are desirable for IPv6 capable hosts
::1     ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当初 &lt;code&gt;pcmk-1&lt;/code&gt; ではIPv4の部分を&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;127.0.0.1   localhost
127.0.1.1   pcmk-1
10.155.92.102 pcmk-2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;と変更し、 &lt;code&gt;pcmk-2&lt;/code&gt; では&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;127.0.0.1   localhost
127.0.1.1   pcmk-2
10.155.92.101 pcmk-1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;と変更してみたのですが、Pacemakerがうまく動かなかったようです（要追試）。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;pcmk-1&lt;/code&gt; と &lt;code&gt;pcmk-2&lt;/code&gt; で &lt;code&gt;/etc/hosts&lt;/code&gt; の IPv4 部分を以下のコマンドで変更したら、うまくいったので、とりあえずこれで試しました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# cat &amp;gt; /etc/hosts &amp;lt;&amp;lt;&#39;EOF&#39;
127.0.0.1   localhost

10.155.92.101   pcmk-1
10.155.92.102   pcmk-2

# The following lines are desirable for IPv6 capable hosts
::1     ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;pacemakerのインストール&#34;&gt;Pacemakerのインストール&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;pcmk-1&lt;/code&gt; と &lt;code&gt;pcmk-2&lt;/code&gt; で以下のコマンドを実行します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# yum -y update
# yum -y install pacemaker pcs psmisc policycoreutils-python which
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;whichは仮想IPを使うための ocf:heartbeat:IPaddr2 のリソース用の resource agent スクリプト &lt;code&gt;/usr/lib/ocf/resource.d/heartbeat/IPaddr2&lt;/code&gt; で必要となります。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;pcsd&lt;/code&gt; サービスを起動し、OS起動時に自動起動するようにします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# systemctl start pcsd
# systemctl enable pcsd
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;クラスタを作成して仮想ipの作成-移動実験&#34;&gt;クラスタを作成して仮想IPの作成・移動実験&lt;/h2&gt;

&lt;h3 id=&#34;クラスタの作成と開始&#34;&gt;クラスタの作成と開始&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;hacluster&lt;/code&gt; のパスワードを設定します。ここでは &lt;code&gt;password&lt;/code&gt; という値にしていますが適宜変更してください。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# echo password | passwd --stdin hacluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ここから先は &lt;code&gt;pcmk-1&lt;/code&gt; だけでコマンドを実行します。 &lt;code&gt;-p&lt;/code&gt; の値は上で設定したパスワードに合わせてください。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# pcs cluster auth pcmk-1 pcmk-2 -u hacluster -p password
# pcs cluster setup --name mycluster pcmk-1 pcmk-2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;この時点で &lt;code&gt;/etc/corosync/corosync.conf&lt;/code&gt; が作られます。&lt;/p&gt;

&lt;p&gt;以下のコマンドでクラスタを開始します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# pcs cluster start --all
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;起動してすぐにステータスを確認すると Node の行が UNCLEAN (offline) になりました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-1 ~]# pcs status
Cluster name: mycluster
WARNING: no stonith devices and stonith-enabled is not false
Last updated: Thu Aug 11 15:55:17 2016          Last change: Thu Aug 11 15:55:16 2016 by hacluster via crmd on pcmk-1
Stack: unknown
Current DC: NONE
2 nodes and 0 resources configured

Node pcmk-1: UNCLEAN (offline)
Node pcmk-2: UNCLEAN (offline)

Full list of resources:


PCSD Status:
  pcmk-1: Online
  pcmk-2: Online

Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;しばらくしてから再度ステータスを確認すると pcmk-1 も pcmk-2 も Online になっていました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# pcs status
Cluster name: mycluster
WARNING: no stonith devices and stonith-enabled is not false
Last updated: Thu Aug 11 15:56:41 2016          Last change: Thu Aug 11 15:55:37 2016 by hacluster via crmd on pcmk-2
Stack: corosync
Current DC: pcmk-2 (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum
2 nodes and 0 resources configured

Online: [ pcmk-1 pcmk-2 ]

Full list of resources:


PCSD Status:
  pcmk-1: Online
  pcmk-2: Online

Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;stonithを無効化&#34;&gt;STONITHを無効化&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://clusterlabs.org/doc/en-US/Pacemaker/1.1-pcs/html/Clusters_from_Scratch/ch05.html&#34;&gt;Chapter 5. Create an Active/Passive Cluster&lt;/a&gt;の手順でクラスタの設定エラーを確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# crm_verify -L -V
   error: unpack_resources:     Resource start-up disabled since no STONITH resources have been defined
   error: unpack_resources:     Either configure some or disable STONITH with the stonith-enabled option
   error: unpack_resources:     NOTE: Clusters with shared data need STONITH to ensure data integrity
Errors found during check: config not valid
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ここでは簡単に Pacemaker を試すために STONITH を無効にします。無効にしたあと設定エラーを再度確認すると、今度はエラーが無くなりました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# pcs property set stonith-enabled=false
# crm_verify -L -V
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;なお、 &lt;a href=&#34;http://clusterlabs.org/doc/en-US/Pacemaker/1.1-pcs/html/Clusters_from_Scratch/ch05.html&#34;&gt;Chapter 5. Create an Active/Passive Cluster&lt;/a&gt; の最後の Warning にもある通り、 &lt;strong&gt;実運用では STONITH を無効にするのは全く不適切&lt;/strong&gt; とのことなので、きちんと設定する必要があります。 STONITH についての説明は上記の Warning の囲み内からもリンクされている &lt;a href=&#34;http://clusterlabs.org/doc/en-US/Pacemaker/1.1-pcs/html/Clusters_from_Scratch/ch08.html#_what_is_stonith&#34;&gt;Chapter 8. Configure STONITH&lt;/a&gt; を参照してください。&lt;/p&gt;

&lt;h3 id=&#34;仮想ipアドレス用のリソース作成&#34;&gt;仮想IPアドレス用のリソース作成&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;pcmk-1&lt;/code&gt; で以下のコマンドを実行して、仮想IPアドレス &lt;code&gt;10.155.92.100&lt;/code&gt; 用のリソースを作成します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# pcs resource create ClusterIP ocf:heartbeat:IPaddr2 \
    ip=10.155.92.100 cidr_netmask=32 op monitor interval=30s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;数秒してから状態を確認してみます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# pcs status
Cluster name: mycluster
Last updated: Thu Aug 11 16:04:50 2016          Last change: Thu Aug 11 16:04:47 2016 by root via cibadmin on pcmk-1
Stack: corosync
Current DC: pcmk-2 (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum
2 nodes and 1 resource configured

Online: [ pcmk-1 pcmk-2 ]

Full list of resources:

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-1

PCSD Status:
  pcmk-1: Online
  pcmk-2: Online

Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;# pcs resource --full
 Resource: ClusterIP (class=ocf provider=heartbeat type=IPaddr2)
  Attributes: ip=10.155.92.100 cidr_netmask=32 
  Operations: start interval=0s timeout=20s (ClusterIP-start-interval-0s)
              stop interval=0s timeout=20s (ClusterIP-stop-interval-0s)
              monitor interval=30s (ClusterIP-monitor-interval-30s)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;ip&lt;/code&gt; コマンドを実行して、仮想IPアドレスが &lt;code&gt;pcmk-1&lt;/code&gt; 側についており &lt;code&gt;pcmk-2&lt;/code&gt; 側にはついていないことを確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-1 ~]# ip a s eth0
120: eth0@if121: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP qlen 1000
    link/ether 00:16:3e:e6:fb:ab brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.155.92.101/24 brd 10.155.92.255 scope global dynamic eth0
       valid_lft 3203sec preferred_lft 3203sec
    inet 10.155.92.100/32 brd 10.155.92.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::216:3eff:fee6:fbab/64 scope link 
       valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-2 ~]# ip a s eth0
122: eth0@if123: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP qlen 1000
    link/ether 00:16:3e:4b:6d:b1 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.155.92.102/24 brd 10.155.92.255 scope global dynamic eth0
       valid_lft 3560sec preferred_lft 3560sec
    inet6 fe80::216:3eff:fe4b:6db1/64 scope link 
       valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;pcmk-1-をクラスタから離脱させて仮想ipアドレスが-pcmk-2-に移動するか確認&#34;&gt;&lt;code&gt;pcmk-1&lt;/code&gt; をクラスタから離脱させて仮想IPアドレスが &lt;code&gt;pcmk-2&lt;/code&gt; に移動するか確認&lt;/h3&gt;

&lt;p&gt;以下のコマンドで &lt;code&gt;pcmk-1&lt;/code&gt; をクラスタから離脱させます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-1 ~]# pcs cluster stop pcmk-1
pcmk-1: Stopping Cluster (pacemaker)...
pcmk-1: Stopping Cluster (corosync)...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;pcmk-1&lt;/code&gt; で状態を確認すると以下のようになります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-1 ~]# pcs status
Error: cluster is not currently running on this node
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;pcmk-2&lt;/code&gt; で状態を確認すると以下のようになります。 &lt;code&gt;pcmk-1&lt;/code&gt; は &lt;code&gt;OFFLINE&lt;/code&gt; となっていますが、 &lt;code&gt;pcsd&lt;/code&gt; は動いているので &lt;code&gt;PCSD Status&lt;/code&gt; のほうは &lt;code&gt;Online&lt;/code&gt; のままです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-2 ~]# pcs status
Cluster name: mycluster
Last updated: Thu Aug 11 16:10:04 2016          Last change: Thu Aug 11 16:04:47 2016 by root via cibadmin on pcmk-1
Stack: corosync
Current DC: pcmk-2 (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum
2 nodes and 1 resource configured

Online: [ pcmk-2 ]
OFFLINE: [ pcmk-1 ]

Full list of resources:

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-2

PCSD Status:
  pcmk-1: Online
  pcmk-2: Online

Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;ip&lt;/code&gt; コマンドを実行して、仮想IPアドレスが &lt;code&gt;pcmk-2&lt;/code&gt; 側についており &lt;code&gt;pcmk-1&lt;/code&gt; 側にはついていないことを確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-1 ~]# ip a s eth0
120: eth0@if121: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP qlen 1000
    link/ether 00:16:3e:e6:fb:ab brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.155.92.101/24 brd 10.155.92.255 scope global dynamic eth0
       valid_lft 3024sec preferred_lft 3024sec
    inet6 fe80::216:3eff:fee6:fbab/64 scope link 
       valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-2 ~]# ip a s eth0
122: eth0@if123: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP qlen 1000
    link/ether 00:16:3e:4b:6d:b1 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.155.92.102/24 brd 10.155.92.255 scope global dynamic eth0
       valid_lft 3385sec preferred_lft 3385sec
    inet 10.155.92.100/32 brd 10.155.92.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::216:3eff:fe4b:6db1/64 scope link 
       valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;pcmk-1-をクラスタに復帰させる&#34;&gt;&lt;code&gt;pcmk-1&lt;/code&gt; をクラスタに復帰させる&lt;/h3&gt;

&lt;p&gt;以下のコマンドで &lt;code&gt;pcmk-1&lt;/code&gt; をクラスタに復帰させます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-1 ~]# pcs cluster start pcmk-1
pcmk-1: Starting Cluster...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;状態を確認してみると、仮想IP は &lt;code&gt;pcmk-2&lt;/code&gt; のほうについたままです。
&lt;a href=&#34;http://clusterlabs.org/doc/en-US/Pacemaker/1.1-pcs/html/Clusters_from_Scratch/_perform_a_failover.html&#34;&gt;5.3. Perform a Failover&lt;/a&gt; の最後の Note によると Pacemakerのより古いバージョンでは &lt;code&gt;pcmk-1&lt;/code&gt; のほうに切り替わっていたそうですが、挙動が変更されたとのことです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-1 ~]# pcs status
Cluster name: mycluster
Last updated: Thu Aug 11 16:12:35 2016          Last change: Thu Aug 11 16:04:47 2016 by root via cibadmin on pcmk-1
Stack: corosync
Current DC: pcmk-2 (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum
2 nodes and 1 resource configured

Online: [ pcmk-1 pcmk-2 ]

Full list of resources:

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-2

PCSD Status:
  pcmk-1: Online
  pcmk-2: Online

Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下のコマンドを実行して、仮想IPを &lt;code&gt;pcmk-1&lt;/code&gt; のほうに移動します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pcs cluster stop pcmk-2 &amp;amp;&amp;amp; pcs cluster start pcmk-2
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;apacheのactive-passiveクラスタを作って仮想ipと連動させる&#34;&gt;ApacheのActive/Passiveクラスタを作って仮想IPと連動させる&lt;/h2&gt;

&lt;h3 id=&#34;リソースのスティッキネスのデフォルト値を設定&#34;&gt;リソースのスティッキネスのデフォルト値を設定&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://clusterlabs.org/doc/en-US/Pacemaker/1.1-pcs/html/Clusters_from_Scratch/_prevent_resources_from_moving_after_recovery.html&#34;&gt;5.4. Prevent Resources from Moving after Recovery&lt;/a&gt; を見て設定します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-1 ~]# pcs resource defaults resource-stickiness=100
[root@pcmk-1 ~]# pcs resource defaults 
resource-stickiness: 100
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;apache-のインストールと設定&#34;&gt;Apache のインストールと設定&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;pcmk-1&lt;/code&gt; と &lt;code&gt;pcmk-2&lt;/code&gt; で以下のコマンドを実行します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# yum -y install httpd wget
# mkdir -p /var/www/html /var/log/httpd
# cat &amp;gt; /var/www/html/index.html &amp;lt;&amp;lt;EOF
&amp;lt;html&amp;gt;
&amp;lt;body&amp;gt;My Test Site - $(hostname)&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
EOF
# cat &amp;gt; /etc/httpd/conf.d/status.conf &amp;lt;&amp;lt;&#39;EOF&#39;
&amp;lt;Location /server-status&amp;gt;
  SetHandler server-status
  Require ip 127.0.0.1
&amp;lt;/Location&amp;gt;
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;apache-の-active-passive-クラスタ作成&#34;&gt;Apache の Active/Passive クラスタ作成&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;pcmk-1&lt;/code&gt; か &lt;code&gt;pcmk-2&lt;/code&gt; のどちらか一方で以下のコマンドを実行します。
公式のドキュメントでは、 &lt;code&gt;pcmk-2&lt;/code&gt; で開始した後、制約を追加しただけでは &lt;code&gt;pcmk-1&lt;/code&gt; に移動しないというデモをしていますが、ここでは &lt;code&gt;--disabled&lt;/code&gt; つきでリソースを作成後、制約を追加してから有効化することで最初から &lt;code&gt;pcmk-1&lt;/code&gt; で開始させています。&lt;/p&gt;

&lt;p&gt;また、制約を追加するごとに &lt;code&gt;crm_simulate -sL&lt;/code&gt; を実行してリソースをどのノードに割り当てるかのスコアを確認しています。&lt;/p&gt;

&lt;p&gt;まず WebSite という名前のリソースを &lt;code&gt;disabled&lt;/code&gt; 状態で作成します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# pcs resource create WebSite ocf:heartbeat:apache \
    configfile=/etc/httpd/conf/httpd.conf \
    statusurl=&amp;quot;http://localhost/server-status&amp;quot; \
    op monitor interval=3s on-fail=restart \
    --disabled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;WebSite リソースは ClusterIP リソースと同じノードで動かすという制約を追加します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# pcs constraint colocation add WebSite with ClusterIP INFINITY
# crm_simulate -sL

Current cluster status:
Online: [ pcmk-1 pcmk-2 ]

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-1
 WebSite        (ocf::heartbeat:apache):        (target-role:Stopped) Stopped

Allocation scores:
native_color: ClusterIP allocation score on pcmk-1: 100
native_color: ClusterIP allocation score on pcmk-2: 0
native_color: WebSite allocation score on pcmk-1: -INFINITY
native_color: WebSite allocation score on pcmk-2: -INFINITY

Transition Summary:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;リソースの開始順序を ClusterIP、 WebSite にする制約を追加します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# pcs constraint order ClusterIP then WebSite
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;WebSite のリソースをなるべく &lt;code&gt;pcmk-1&lt;/code&gt; 側で動かすようにする制約を追加します。 &lt;code&gt;250&lt;/code&gt; という値はこの後の操作を一度試行錯誤してみて適当に選びましたが、希望通りの動作が実現できさえすれば違う値でも構いません。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# pcs constraint location WebSite prefers pcmk-1=250
# crm_simulate -sL

Current cluster status:
Online: [ pcmk-1 pcmk-2 ]

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-1
 WebSite        (ocf::heartbeat:apache):        (target-role:Stopped) Stopped

Allocation scores:
native_color: ClusterIP allocation score on pcmk-1: 350
native_color: ClusterIP allocation score on pcmk-2: 0
native_color: WebSite allocation score on pcmk-1: -INFINITY
native_color: WebSite allocation score on pcmk-2: -INFINITY

Transition Summary:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;希望する制約を一通り追加したので、 WebSite リソースを稼働開始します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# pcs resource enable WebSite
# crm_simulate -sL

Current cluster status:
Online: [ pcmk-1 pcmk-2 ]

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-1
 WebSite        (ocf::heartbeat:apache):        Started pcmk-1

Allocation scores:
native_color: ClusterIP allocation score on pcmk-1: 450
native_color: ClusterIP allocation score on pcmk-2: 0
native_color: WebSite allocation score on pcmk-1: 350
native_color: WebSite allocation score on pcmk-2: -INFINITY

Transition Summary:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;pcmk-1&lt;/code&gt; と &lt;code&gt;pcmk-2&lt;/code&gt; で &lt;code&gt;ps auxww | grep httpd&lt;/code&gt; すると &lt;code&gt;pcmk-1&lt;/code&gt; 側で Apache が稼働して &lt;code&gt;pcmk-2&lt;/code&gt; 側では稼働していないことを確認できます。&lt;/p&gt;

&lt;h3 id=&#34;手動で制約を調整して仮想ipとapacheを-pcmk-2-に移動する&#34;&gt;手動で制約を調整して仮想IPとApacheを &lt;code&gt;pcmk-2&lt;/code&gt; に移動する&lt;/h3&gt;

&lt;p&gt;以下のコマンドで制約を調整し、移動が完了するまでのスコアの変遷を確認します。 &lt;code&gt;500&lt;/code&gt; という値は前項の最後の &lt;code&gt;pcmk-1&lt;/code&gt; の ClusterIP のスコアを上回る値として選びました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# pcs constraint location WebSite prefers pcmk-2=500 \
  &amp;amp;&amp;amp; for i in `seq 1 20`; do crm_simulate -sL; sleep 0.1; done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;出力結果のうち変化があったものだけを抜粋します。
まず開始直後の状態。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Current cluster status:
Online: [ pcmk-1 pcmk-2 ]

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-1
 WebSite        (ocf::heartbeat:apache):        Started pcmk-1

Allocation scores:
native_color: ClusterIP allocation score on pcmk-1: 450
native_color: ClusterIP allocation score on pcmk-2: 500
native_color: WebSite allocation score on pcmk-1: -INFINITY
native_color: WebSite allocation score on pcmk-2: 500

Transition Summary:
 * Move    ClusterIP    (Started pcmk-1 -&amp;gt; pcmk-2)
 * Move    WebSite      (Started pcmk-1 -&amp;gt; pcmk-2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;WebSiteが停止した状態。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Current cluster status:
Online: [ pcmk-1 pcmk-2 ]

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-1
 WebSite        (ocf::heartbeat:apache):        Stopped

Allocation scores:
native_color: ClusterIP allocation score on pcmk-1: 350
native_color: ClusterIP allocation score on pcmk-2: 500
native_color: WebSite allocation score on pcmk-1: -INFINITY
native_color: WebSite allocation score on pcmk-2: 500

Transition Summary:
 * Move    ClusterIP    (Started pcmk-1 -&amp;gt; pcmk-2)
 * Start   WebSite      (pcmk-2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ClusterIPがpcmk-2に移った状態。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Current cluster status:
Online: [ pcmk-1 pcmk-2 ]

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-2
 WebSite        (ocf::heartbeat:apache):        Stopped

Allocation scores:
native_color: ClusterIP allocation score on pcmk-1: 250
native_color: ClusterIP allocation score on pcmk-2: 600
native_color: WebSite allocation score on pcmk-1: -INFINITY
native_color: WebSite allocation score on pcmk-2: 500

Transition Summary:
 * Start   WebSite      (pcmk-2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;WebSiteが &lt;code&gt;pcmk-2&lt;/code&gt; で稼働開始した状態。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Current cluster status:
Online: [ pcmk-1 pcmk-2 ]

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-2
 WebSite        (ocf::heartbeat:apache):        Started pcmk-2

Allocation scores:
native_color: ClusterIP allocation score on pcmk-1: 250
native_color: ClusterIP allocation score on pcmk-2: 700
native_color: WebSite allocation score on pcmk-1: -INFINITY
native_color: WebSite allocation score on pcmk-2: 600

Transition Summary:
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;手動で制約を調整して仮想ipとapacheを-pcmk-1-に戻す&#34;&gt;手動で制約を調整して仮想IPとApacheを &lt;code&gt;pcmk-1&lt;/code&gt; に戻す&lt;/h3&gt;

&lt;p&gt;次に &lt;code&gt;pcmk-2&lt;/code&gt; から &lt;code&gt;pcmk-1&lt;/code&gt; に戻してみます。
以下のコマンドで制約のIDを確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-1 ~]# pcs constraint --full
Location Constraints:
  Resource: WebSite
    Enabled on: pcmk-1 (score:250) (id:location-WebSite-pcmk-1-250)
    Enabled on: pcmk-2 (score:500) (id:location-WebSite-pcmk-2-500)
Ordering Constraints:
  start ClusterIP then start WebSite (kind:Mandatory) (id:order-ClusterIP-WebSite-mandatory)
Colocation Constraints:
  WebSite with ClusterIP (score:INFINITY) (id:colocation-WebSite-ClusterIP-INFINITY)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下のコマンドで &lt;code&gt;pcmk-2&lt;/code&gt; 側の制約を削除し、 &lt;code&gt;pcmk-1&lt;/code&gt; 側に戻るまでのスコアの動きを確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# pcs constraint remove location-WebSite-pcmk-2-500 \
  &amp;amp;&amp;amp; for i in `seq 1 20`; do crm_simulate -sL; sleep 0.1; done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;開始直後の状態。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Current cluster status:
Online: [ pcmk-1 pcmk-2 ]

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-2
 WebSite        (ocf::heartbeat:apache):        Started pcmk-2

Allocation scores:
native_color: ClusterIP allocation score on pcmk-1: 250
native_color: ClusterIP allocation score on pcmk-2: 200
native_color: WebSite allocation score on pcmk-1: 250
native_color: WebSite allocation score on pcmk-2: -INFINITY

Transition Summary:
 * Move    ClusterIP    (Started pcmk-2 -&amp;gt; pcmk-1)
 * Move    WebSite      (Started pcmk-2 -&amp;gt; pcmk-1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;WebSiteが停止した状態。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Current cluster status:
Online: [ pcmk-1 pcmk-2 ]

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-2
 WebSite        (ocf::heartbeat:apache):        Stopped

Allocation scores:
native_color: ClusterIP allocation score on pcmk-1: 250
native_color: ClusterIP allocation score on pcmk-2: 100
native_color: WebSite allocation score on pcmk-1: 250
native_color: WebSite allocation score on pcmk-2: -INFINITY

Transition Summary:
 * Move    ClusterIP    (Started pcmk-2 -&amp;gt; pcmk-1)
 * Start   WebSite      (pcmk-1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ClusterIPが &lt;code&gt;pcmk-1&lt;/code&gt; に移動した状態。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Current cluster status:
Online: [ pcmk-1 pcmk-2 ]

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-1
 WebSite        (ocf::heartbeat:apache):        Stopped

Allocation scores:
native_color: ClusterIP allocation score on pcmk-1: 350
native_color: ClusterIP allocation score on pcmk-2: 0
native_color: WebSite allocation score on pcmk-1: 250
native_color: WebSite allocation score on pcmk-2: -INFINITY

Transition Summary:
 * Start   WebSite      (pcmk-1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;WebSiteが &lt;code&gt;pcmk-1&lt;/code&gt; で稼働開始した状態。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Current cluster status:
Online: [ pcmk-1 pcmk-2 ]

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-1
 WebSite        (ocf::heartbeat:apache):        Started pcmk-1

Allocation scores:
native_color: ClusterIP allocation score on pcmk-1: 450
native_color: ClusterIP allocation score on pcmk-2: 0
native_color: WebSite allocation score on pcmk-1: 350
native_color: WebSite allocation score on pcmk-2: -INFINITY

Transition Summary:
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;active側のコンテナに障害が発生してコンテナごと落ちるケースの模擬実験&#34;&gt;Active側のコンテナに障害が発生してコンテナごと落ちるケースの模擬実験&lt;/h2&gt;

&lt;h3 id=&#34;active側-pcmk-1-のコンテナを停止させたときの挙動を確認&#34;&gt;Active側 &lt;code&gt;pcmk-1&lt;/code&gt; のコンテナを停止させたときの挙動を確認&lt;/h3&gt;

&lt;p&gt;LXDホストで以下のコマンドを実行して &lt;code&gt;pcmk-1&lt;/code&gt; を停止させます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ lxc stop -f pcmk-1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;pcmk-2&lt;/code&gt; で &lt;code&gt;pcs status&lt;/code&gt; を実行すると &lt;code&gt;pcmk-1&lt;/code&gt; が OFFLINE になったことがわかりますが、 &lt;code&gt;PCSD Status:&lt;/code&gt; の後を表示するところでブロックしたので Ctrl-C で止めました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-2 ~]# pcs status
Cluster name: mycluster
Last updated: Fri Aug 12 13:13:37 2016          Last change: Fri Aug 12 09:34:39 2016 by root via cibadmin on pcmk-1
Stack: corosync
Current DC: pcmk-2 (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum
2 nodes and 2 resources configured

Online: [ pcmk-2 ]
OFFLINE: [ pcmk-1 ]

Full list of resources:

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-2
 WebSite        (ocf::heartbeat:apache):        Started pcmk-2

PCSD Status:
^C
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;ip a s eth0&lt;/code&gt; と &lt;code&gt;ps auxww | grep httpd&lt;/code&gt; で仮想IPとApacheが &lt;code&gt;pcmk-2&lt;/code&gt; で動いていることが確認できました。&lt;/p&gt;

&lt;h3 id=&#34;pcmk-1-のコンテナを起動させた時の挙動を確認&#34;&gt;&lt;code&gt;pcmk-1&lt;/code&gt; のコンテナを起動させた時の挙動を確認&lt;/h3&gt;

&lt;p&gt;LXDホストで以下のコマンドを実行して &lt;code&gt;pcmk-1&lt;/code&gt; を起動し、コンテナ内に入ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ lxc satrt pcmk-1
$ lxc exec pcmk-1 bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;pcmk-1&lt;/code&gt; 側は &lt;code&gt;pcsd&lt;/code&gt; は起動していますが、クラスタには所属していない状態です。&lt;/p&gt;

&lt;p&gt;理由は &lt;a href=&#34;http://clusterlabs.org/doc/en-US/Pacemaker/1.1-pcs/html/Clusters_from_Scratch/ch04.html#_start_the_cluster&#34;&gt;Chapter 4. Start and Verify Cluster&lt;/a&gt; に説明があります。 &lt;code&gt;pcsd&lt;/code&gt; は &lt;code&gt;systemctl enable&lt;/code&gt; でOS起動時の自動起動を有効にしていますが &lt;code&gt;corosync&lt;/code&gt; と &lt;code&gt;pacemaker&lt;/code&gt; はしていないからです。&lt;/p&gt;

&lt;p&gt;実運用時に物理的な障害などで &lt;code&gt;pcmk-1&lt;/code&gt; がクラスタから外れた場合、その後電源をいれて起動できたとしても、障害の原因を調査して、正常にサービスを稼働できるかを確認してからクラスタに復帰させたいので、この設定で良いと思います。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-1 ~]# pcs status
Error: cluster is not currently running on this node
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;pcmk-2&lt;/code&gt; で &lt;code&gt;pcs status&lt;/code&gt; は今度はブロックせずに完了します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-2 ~]# pcs status
Cluster name: mycluster
Last updated: Fri Aug 12 13:18:22 2016          Last change: Fri Aug 12 09:34:39 2016 by root via cibadmin on pcmk-1
Stack: corosync
Current DC: pcmk-2 (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum
2 nodes and 2 resources configured

Online: [ pcmk-2 ]
OFFLINE: [ pcmk-1 ]

Full list of resources:

 ClusterIP      (ocf::heartbeat:IPaddr2):       Started pcmk-2
 WebSite        (ocf::heartbeat:apache):        Started pcmk-2

PCSD Status:
  pcmk-1: Online
  pcmk-2: Online

Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下のコマンドを実行して &lt;code&gt;pcmk-1&lt;/code&gt; をクラスタに復帰させます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pcmk-1 ~]# pcs cluster start pcmk-1
pcmk-1: Starting Cluster...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;しばらくして &lt;code&gt;pcs status&lt;/code&gt; を確認すると &lt;code&gt;pcmk-1&lt;/code&gt; がオンラインになり、 ClusterIP と WebSite リソースが &lt;code&gt;pcmk-1&lt;/code&gt; に移動することが確認できました。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LXDのdnsmasqの固定IP設定をSIGHUPで更新する</title>
      <link>https://hnakamur.github.io/blog/2016/08/12/update-lxd-dnsmasq-dhcp-hosts-config-with-sighup/</link>
      <pubDate>Fri, 12 Aug 2016 06:38:18 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2016/08/12/update-lxd-dnsmasq-dhcp-hosts-config-with-sighup/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://hnakamur.github.io/blog/blog/2016/05/07/how-to-use-fixed-ip-address-for-a-lxd-container/&#34;&gt;LXDコンテナで固定IPアドレスを使うための設定 · hnakamur&amp;rsquo;s blog at github&lt;/a&gt; では &lt;code&gt;/etc/dnsmasq.conf&lt;/code&gt; に直接 &lt;code&gt;dhcp-host&lt;/code&gt; で設定を書いていましたが、変更するためには &lt;code&gt;lxd-bridge&lt;/code&gt; の再起動が必要でした。&lt;/p&gt;

&lt;p&gt;その後 &lt;a href=&#34;http://manpages.ubuntu.com/manpages/xenial/en/man8/dnsmasq.8.html&#34;&gt;Ubuntu Manpage: dnsmasq - A lightweight DHCP and caching DNS server.&lt;/a&gt; を見て &lt;code&gt;--dhcp-hostsfile=&amp;lt;path&amp;gt;&lt;/code&gt; または &lt;code&gt;--dhcp-hostsdir=&amp;lt;path&amp;gt;&lt;/code&gt; を使っておけば &lt;code&gt;lxd-bridge&lt;/code&gt; を再起動しなくても &lt;code&gt;dnsmasq&lt;/code&gt; に &lt;code&gt;SIGHUP&lt;/code&gt; を送れば更新できることを知りました。 &lt;code&gt;--dhcp-hostsdir=&amp;lt;path&amp;gt;&lt;/code&gt; の場合は、指定したディレクトリ以下のファイルを追加・更新する場合は SIGHUP すら不要で、ファイルを削除した後に反映するときだけ SIGHUP が必要です。&lt;/p&gt;

&lt;p&gt;ですが、実際に試してみると &lt;code&gt;--dhcp-hostsdir&lt;/code&gt; のほうは SIGHUP を送ると &lt;code&gt;duplicate dhcp-host IP address&lt;/code&gt; というエラーになってしまったので (下記のハマりメモ参照)、 &lt;code&gt;--dhcp-hostsfile&lt;/code&gt; のほうを使うことにしました。&lt;/p&gt;

&lt;h2 id=&#34;lxd-bridgeのdnsmasqで-dhcp-hostsfile-を使う設定&#34;&gt;lxd-bridgeのdnsmasqで&amp;ndash;dhcp-hostsfile を使う設定&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;/var/lib/lxd-bridge/dhcp-hosts&lt;/code&gt; というファイルを作って、そこを見るように切り替えてみます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo touch /var/lib/lxd-bridge/dhcp-hosts
echo &#39;dhcp-hostsfile=/var/lib/lxd-bridge/dhcp-hosts&#39; | sudo tee /etc/dnsmasq.conf &amp;gt; /dev/null
sudo systemctl restart lxd-bridge
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;ipアドレスを指定して新規コンテナを作成する&#34;&gt;IPアドレスを指定して新規コンテナを作成する&lt;/h2&gt;

&lt;p&gt;例えば &lt;code&gt;web01&lt;/code&gt; というコンテナを &lt;code&gt;10.155.92.201&lt;/code&gt; というアドレスで作成したい場合は以下のようにします。 &lt;a href=&#34;http://manpages.ubuntu.com/manpages/xenial/en/man8/dnsmasq.8.html&#34;&gt;Ubuntu Manpage: dnsmasq - A lightweight DHCP and caching DNS server.&lt;/a&gt; によると &lt;code&gt;--dhcp-range&lt;/code&gt; で指定した範囲の外でも良いが &lt;code&gt;--dhcp-range&lt;/code&gt; と同じサブネットである必要があるとのことです。 &lt;code&gt;ps auxww | grep dnsmasq&lt;/code&gt; で見たところ &lt;code&gt;/etc/default/lxd-bridge&lt;/code&gt; の &lt;code&gt;LXD_IPV4_DHCP_RANGE&lt;/code&gt; の値が &lt;code&gt;--dhcp-range&lt;/code&gt; に使われています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo web01,10.155.92.201 | sudo tee /var/lib/lxd-bridge/dhcp-hosts &amp;gt; /dev/null
sudo kill -HUP `cat /var/run/lxd-bridge/dnsmasq.pid`
lxc launch images:centos/7/amd64 web01
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;数秒してから &lt;code&gt;lxc list&lt;/code&gt; を実行すると指定したアドレスになっていることが確認できます。&lt;/p&gt;

&lt;p&gt;なお、この例では dhcp-hosts 内のエントリが web01 の1つだけなので echo と tee で作成・更新していますが、実際の利用時には複数エントリがあるので既存のエントリを残しつつエントリを追加・更新する必要がありますのでご注意ください。&lt;/p&gt;

&lt;h2 id=&#34;既存のコンテナのipアドレスを変更する&#34;&gt;既存のコンテナのIPアドレスを変更する&lt;/h2&gt;

&lt;p&gt;上記で作成した &lt;code&gt;web01&lt;/code&gt; というコンテナのアドレスを &lt;code&gt;10.155.92.202&lt;/code&gt; に変更してみます。変更にはコンテナの再起動が必要になります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo web01,10.155.92.202 | sudo tee /var/lib/lxd-bridge/dhcp-hosts &amp;gt; /dev/null
sudo kill -HUP `cat /var/run/lxd-bridge/dnsmasq.pid`
lxc restart -f web01
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;数秒してから &lt;code&gt;lxc list&lt;/code&gt; を実行すると指定したアドレスになっていることが確認できます。&lt;/p&gt;

&lt;p&gt;この方法でIPアドレスを変更すると &lt;code&gt;/var/lib/lxd-bridge/dnsmasq.lxdbr0.leases&lt;/code&gt; に変更前のアドレスが残らないので、そのアドレスをすぐに他で再利用することが出来ます。&lt;/p&gt;

&lt;h2 id=&#34;コンテナを削除後-同じipアドレスを他のコンテナで使う&#34;&gt;コンテナを削除後、同じIPアドレスを他のコンテナで使う&lt;/h2&gt;

&lt;p&gt;一方、コンテナを削除しても使っていたIPアドレスはまだ貸出中になっています。&lt;/p&gt;

&lt;p&gt;上記の状態の後&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lxc delete -f web01
: | sudo tee /var/lib/lxd-bridge/dhcp-hosts
sudo kill -HUP `cat /var/run/lxd-bridge/dnsmasq.pid`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;としても &lt;code&gt;/var/lib/lxd-bridge/dnsmasq.lxdbr0.leases&lt;/code&gt; には&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1470963716 00:16:3e:45:a6:d1 10.155.92.202 web01 *
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;のようなエントリが残っています。
このアドレスを他のコンテナで使うためには一旦解放する必要があります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo dhcp_release lxdbr0 10.155.92.202 00:16:3e:45:a6:d1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;のように実行するか、あるいは &lt;a href=&#34;https://hnakamur.github.io/blog/blog/2016/08/11/release-all-unused-addresses-of-lxd-bridge/&#34;&gt;LXDのDHCPで使っていないIPアドレスを一括で解放するスクリプトを書いた · hnakamur&amp;rsquo;s blog at github&lt;/a&gt; で書いたスクリプトを実行して解放します。以下では後者のスクリプトを &lt;code&gt;~/bin/lxd-bridge-release-all-unused-addresses.sh&lt;/code&gt; に保存してあるものとして説明します。&lt;/p&gt;

&lt;p&gt;IPアドレスを解放した後で&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo web02,10.155.92.202 | sudo tee /var/lib/lxd-bridge/dhcp-hosts &amp;gt; /dev/null
sudo kill -HUP `cat /var/run/lxd-bridge/dnsmasq.pid`
lxc launch images:centos/7/amd64 web02
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;のように実行すれば、IPアドレスを &lt;code&gt;10.155.92.202&lt;/code&gt; にして &lt;code&gt;web02&lt;/code&gt; というコンテナを作成・起動できました。&lt;/p&gt;

&lt;h2 id=&#34;dhcp-hostsdirのハマりメモ&#34;&gt;&amp;ndash;dhcp-hostsdirのハマりメモ&lt;/h2&gt;

&lt;h3 id=&#34;lxd-bridgeのdnsmasqで-dhcp-hostsdir-を使う設定&#34;&gt;lxd-bridgeのdnsmasqで&amp;ndash;dhcp-hostsdir を使う設定&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;/var/lib/lxd-bridge/dhcp-hosts&lt;/code&gt; というディレクトリを作って、そこを見るように切り替えてみます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[ -f /var/lib/lxd-bridge/dhcp-hosts ] &amp;amp;&amp;amp; sudo rm /var/lib/lxd-bridge/dhcp-hosts
sudo mkdir -p /var/lib/lxd-bridge/dhcp-hosts
echo &#39;dhcp-hostsdir=/var/lib/lxd-bridge/dhcp-hosts&#39; | sudo tee /etc/dnsmasq.conf &amp;gt; /dev/null
sudo systemctl restart lxd-bridge
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;ipアドレスを指定して新規コンテナを作成する-1&#34;&gt;IPアドレスを指定して新規コンテナを作成する&lt;/h3&gt;

&lt;p&gt;例えば &lt;code&gt;web01&lt;/code&gt; というコンテナを &lt;code&gt;10.155.92.201&lt;/code&gt; というアドレスで作成したい場合は以下のようにします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo web01,10.155.92.201 | sudo tee /var/lib/lxd-bridge/dhcp-hosts/web01 &amp;gt; /dev/null
lxc launch images:centos/7/amd64 web01
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;数秒してから &lt;code&gt;lxc list&lt;/code&gt; を実行すると指定したアドレスになっていることが確認できます。&lt;/p&gt;

&lt;p&gt;と、ここまでは良かったのですが、&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;journalctl -f
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;でログを見ておいて、別端末で&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo kill -HUP `cat /var/run/lxd-bridge/dnsmasq.pid`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;と実行すると&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Aug 12 08:39:56 lxdhostname dnsmasq-dhcp[2455]: read /var/lib/lxd-bridge/dhcp-hosts/web01
Aug 12 08:39:56 lxdhostname dnsmasq[2455]: duplicate dhcp-host IP address 10.155.92.201 at line 1 of /var/lib/lxd-bridge/dhcp-hosts/web01
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;のようなエラーが出てしまいました。 &lt;code&gt;duplicate dhcp-host IP address&lt;/code&gt; から後ろは赤字で表示されました。&lt;/p&gt;

&lt;h3 id=&#34;コンテナを削除後-同じipアドレスを他のコンテナで使いたいが失敗&#34;&gt;コンテナを削除後、同じIPアドレスを他のコンテナで使いたいが失敗&lt;/h3&gt;

&lt;p&gt;以下では &lt;a href=&#34;https://hnakamur.github.io/blog/blog/2016/08/11/release-all-unused-addresses-of-lxd-bridge/&#34;&gt;LXDのDHCPで使っていないIPアドレスを一括で解放するスクリプトを書いた · hnakamur&amp;rsquo;s blog at github&lt;/a&gt; のスクリプトを &lt;code&gt;~/bin/lxd-bridge-release-all-unused-addresses.sh&lt;/code&gt; に保存してあるものとして説明します。&lt;/p&gt;

&lt;p&gt;上記の状態の後、 &lt;code&gt;journalctl -f&lt;/code&gt; を引き続き別端末で実行しておいて&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lxc delete -f web01
sudo rm /var/lib/lxd-bridge/dhcp-hosts/web01
sudo kill -HUP `cat /var/run/lxd-bridge/dnsmasq.pid`
~/bin/lxd-bridge-release-all-unused-addresses.sh
echo web02,10.155.92.201 | sudo tee /var/lib/lxd-bridge/dhcp-hosts/web02 &amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;と実行すると&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Aug 12 08:44:13 lxdhostname dnsmasq-dhcp[2455]: read /var/lib/lxd-bridge/dhcp-hosts/web02
Aug 12 08:44:13 lxdhostname dnsmasq[2455]: duplicate dhcp-host IP address 10.155.92.201 at line 1 of /var/lib/lxd-bridge/dhcp-hosts/web02
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;と先程と同様のエラーが出ました。ここから&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lxc launch images:centos/7/amd64 web02
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;と実行しても、指定した &lt;code&gt;10.155.92.201&lt;/code&gt; とは異なるアドレスになってしまいました。&lt;/p&gt;

&lt;p&gt;ということで &lt;code&gt;--dhcp-hostsdir=&amp;lt;path&amp;gt;&lt;/code&gt; は正しい使い方がわからなかったので、諦めて &lt;code&gt;--dhcp-hostsfile=&amp;lt;path&amp;gt;&lt;/code&gt; のほうを使うことにしました。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LXDのDHCPで使っていないIPアドレスを一括で解放するスクリプトを書いた</title>
      <link>https://hnakamur.github.io/blog/2016/08/11/release-all-unused-addresses-of-lxd-bridge/</link>
      <pubDate>Thu, 11 Aug 2016 22:58:21 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2016/08/11/release-all-unused-addresses-of-lxd-bridge/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://hnakamur.github.io/blog/blog/2016/05/07/how-to-use-fixed-ip-address-for-a-lxd-container/&#34;&gt;LXDコンテナで固定IPアドレスを使うための設定 · hnakamur&amp;rsquo;s blog at github&lt;/a&gt; の設定を行ってもIPアドレスが指定通りにならないことがありました。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;journal -xe&lt;/code&gt; で見てみると&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Aug 11 22:46:55 bai1b7faf04 dnsmasq-dhcp[11082]: not using configured address 10.155.92.102 because it is leased to 00:16:3e:1e:08:8a
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;というメッセージが出ていて、他のMACアドレスに貸出中になっています。&lt;/p&gt;

&lt;p&gt;ググってみると &lt;a href=&#34;http://www.linuxquestions.org/questions/linux-newbie-8/dnsmasq-force-release-renew-of-dhcp-clients-how-933535/&#34;&gt;[SOLVED] dnsmasq force release/renew of dhcp clients, how?&lt;/a&gt; に回答がありました。&lt;/p&gt;

&lt;h2 id=&#34;使っていないipアドレスを手動で消す&#34;&gt;使っていないIPアドレスを手動で消す&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl stop lxd-bridge
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;で止めて&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo vi /var/lib/lxd-bridge/dnsmasq.lxdbr0.leases
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;で使っていないIPアドレスの行を全て削除します。&lt;/p&gt;

&lt;p&gt;その後&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl start lxd-bridge
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;で再起動します。&lt;/p&gt;

&lt;h2 id=&#34;自動で消すスクリプトも書きました&#34;&gt;自動で消すスクリプトも書きました&lt;/h2&gt;

&lt;p&gt;これでよいかと思ったら、
&lt;a href=&#34;http://lists.thekelleys.org.uk/pipermail/dnsmasq-discuss/2013q3/007356.html&#34;&gt;http://lists.thekelleys.org.uk/pipermail/dnsmasq-discuss/2013q3/007356.html&lt;/a&gt;
を見て &lt;code&gt;dhcp_release&lt;/code&gt; というコマンドを使えば &lt;code&gt;lxd-bridge&lt;/code&gt; の再起動が不要なことを知りました。&lt;/p&gt;

&lt;p&gt;ということでスクリプトを書いてみました。
&lt;a href=&#34;https://gist.github.com/hnakamur/7ed3f7c6175817b633586a1b468bd5c1&#34;&gt;https://gist.github.com/hnakamur/7ed3f7c6175817b633586a1b468bd5c1&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/sh
set -eu

# Set value of LXD_BRIDGE
. /etc/default/lxd-bridge

addr_list_file=/tmp/lxd-addr-list.`date +%Y-%m-%dT%H:%M:%S`
lxc list | awk &#39;$4==&amp;quot;RUNNING&amp;quot;{print $6}&#39; &amp;gt; $addr_list_file
cleanup() {
  rm $addr_list_file
}
trap cleanup EXIT

awk -v addr_list_file=$addr_list_file -v interface=$LXD_BRIDGE &#39;{
  mac_addr = $2
  addr = $3
  ret = system(sprintf(&amp;quot;awk -v addr=%s &#39;\&#39;&#39;BEGIN{rc=1} $1==addr{rc=0} END{exit rc}&#39;\&#39;&#39; %s&amp;quot;, addr,  addr_list_file))
  if (ret == 1) {
    system(sprintf(&amp;quot;sudo dhcp_release %s %s %s&amp;quot;, interface, addr, mac_addr))
  }
}&#39; /var/lib/lxd-bridge/dnsmasq.$LXD_BRIDGE.leases
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ubuntu 16.04 の場合 &lt;code&gt;dhcp_release&lt;/code&gt; コマンドを使うには以下のように &lt;code&gt;dnsmasq-utils&lt;/code&gt; パッケージをインストールする必要があります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt -y install dnsmasq-utils
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>ブログ記事「Go言語(Golang) はまりどころと解決策」についてのコメント</title>
      <link>https://hnakamur.github.io/blog/2016/08/02/about-go-pitfalls/</link>
      <pubDate>Tue, 02 Aug 2016 05:57:52 +0900</pubDate>
      
      <guid>https://hnakamur.github.io/blog/2016/08/02/about-go-pitfalls/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://www.yunabe.jp/docs/golang_pitfall.html&#34;&gt;Go言語(Golang) はまりどころと解決策&lt;/a&gt;の記事についてのコメント記事を誰かが書くだろうと思ってスルーしてましたが、見かけないので書いてみます。&lt;/p&gt;

&lt;p&gt;ただし私はGo言語を使って開発していますが、言語自体を詳細に知るエキスパートでは無いです。Go言語にかぎらず個人的にはややこしいところにはなるべく近づかないスタンスなので、詳しい方から見ると物足りないかもしれません。そう感じた方は是非ブログ記事なりを書いていただけると嬉しいです。&lt;/p&gt;

&lt;h2 id=&#34;interface-とnil-goのinterfaceは単なる参照ではない&#34;&gt;interface とnil (Goのinterfaceは単なる参照ではない)&lt;/h2&gt;

&lt;p&gt;特にコメントはなくてそのとおりだと思います。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://golang.org/doc/faq&#34;&gt;Frequently Asked Questions (FAQ)&lt;/a&gt;に加えて &lt;a href=&#34;https://golang.org/doc/effective_go.html&#34;&gt;Effective Go&lt;/a&gt;も早めに読んでおいたほうが良いと思います。&lt;/p&gt;

&lt;p&gt;またnilに関する文献としては &lt;a href=&#34;https://speakerdeck.com/campoy/understanding-nil&#34;&gt;Understanding Nil // Speaker Deck&lt;/a&gt; もおすすめです。&lt;/p&gt;

&lt;h2 id=&#34;メソッド内でレシーバ-this-self-がnilでないことをチェックすることに意味がある&#34;&gt;メソッド内でレシーバ(this, self)がnilでないことをチェックすることに意味がある&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://golang.org/ref/spec#Method_declarations&#34;&gt;Method declarations&lt;/a&gt; に&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The type of a method is the type of a function with the receiver as first argument.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;とあります。メソッドの型はメソッドの引数の前にレシーバを第一引数として入れた関数の型になるとのことです。&lt;/p&gt;

&lt;p&gt;大雑把に言えば、メソッドは第一引数にレシーバを追加した関数と実質同じです。と考えればメソッド内でポインタ型のレシーバのnilチェックをすることは特に違和感ないと思います。&lt;/p&gt;

&lt;h2 id=&#34;errorしか返り値がない関数でerrorを処理し忘れる&#34;&gt;errorしか返り値がない関数でerrorを処理し忘れる&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/alecthomas/gometalinter&#34;&gt;alecthomas/gometalinter&lt;/a&gt;でチェックできました。&lt;/p&gt;

&lt;p&gt;実行例を示します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ gometalinter 
main.go:8:6:warning: exported type Data should have comment or be unexported (golint)
main.go:4:2:error: could not import encoding/json (reading export data: /usr/local/go1.7rc3/pkg/linux_amd64/encoding/json.a: unknown version: v1json    E$GOROOT/src/encoding/json/decode.go?Un) (gotype)
-$GOROOT/src/fmt/scan.go not impStatr) (gotype)g export data: /usr/local/go1.7rc3/pkg/linux_amd64/fmt.a: unknown version: v1fmt
main.go:14:2:error: undeclared name: json (gotype)
main.go:15:2:error: undeclared name: fmt (gotype)
main.go:14:16:warning: error return value not checked (json.Unmarshal([]byte(&amp;quot;not json&amp;quot;), d)) (errcheck)
main.go:9:2:warning: unused struct field github.com/hnakamur/forgotten-error-experiment.Data.a (structcheck)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;jsonやfmt関連のエラーは何言ってるのかよくわからないで無視するとして&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;main.go:14:16:warning: error return value not checked (json.Unmarshal([]byte(&amp;quot;not json&amp;quot;), d)) (errcheck)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;で error の戻り値がチェックされていないことを指摘されています。&lt;/p&gt;

&lt;p&gt;gometalinterのセットアップと使い方は&lt;a href=&#34;http://qiita.com/spiegel-im-spiegel/items/238f6f0ee27bdf1de2a0&#34;&gt;gometalinter で楽々 lint - Qiita&lt;/a&gt;にわかりやすい記事がありました。&lt;/p&gt;

&lt;h2 id=&#34;基本型がメソッドを持たない&#34;&gt;基本型がメソッドを持たない&lt;/h2&gt;

&lt;p&gt;FAQの&lt;a href=&#34;https://golang.org/doc/faq#methods_on_basics&#34;&gt;Why is len a function and not a method?&lt;/a&gt;によると &lt;code&gt;len&lt;/code&gt; などをメソッドにすることも検討したけど、 &lt;code&gt;len&lt;/code&gt; がメソッドではなく関数でも実用上困らないし、そのほうが基本型の (Go言語の型の意味での) インタフェースについての質問を複雑にしないので、 &lt;code&gt;len&lt;/code&gt; などは関数として実装することにしたそうです。&lt;/p&gt;

&lt;p&gt;「インタフェースについての質問」あたりはとりあえずそう訳しましたが、意味はよくわかりません。詳しい方のコメントを期待したいところです。&lt;/p&gt;

&lt;h2 id=&#34;stringが単なるバイト列&#34;&gt;stringが単なるバイト列&lt;/h2&gt;

&lt;p&gt;「正直本当に正しいのかはよく分かりません」については私は正しいかどうかという話というよりは、Go言語ではそう決めたというだけの話かと思っています。&lt;/p&gt;

&lt;p&gt;言語の利用者がハマりにくい決定をするほうが望ましいという意味で「正しいか」と言われているのだとは思いますが、私自身はほぼ常にUTF-8の文字列しか使ってないので特にハマったことはないです。&lt;/p&gt;

&lt;p&gt;文字コード変換には&lt;a href=&#34;https://github.com/golang/text&#34;&gt;golang/text: [mirror] Go text processing support&lt;/a&gt;というパッケージがあります。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;io.Reader&lt;/code&gt; からEUC-JP, Shift_JIS, ISO-2022-JPの文字列を読み込んで UTF-8に変換するのは&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/hnakamur/goqueryja/blob/01aead01dd3ac586c6256140a26a50fb30451971/lib.go#L27-L40&#34;&gt;https://github.com/hnakamur/goqueryja/blob/01aead01dd3ac586c6256140a26a50fb30451971/lib.go#L27-L40&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;というコードで実現できます。&lt;/p&gt;

&lt;h2 id=&#34;継承がない&#34;&gt;継承がない&lt;/h2&gt;

&lt;p&gt;継承を敢えて排除したのはGoの好きな点の1つです。&lt;/p&gt;

&lt;h2 id=&#34;genericsがない&#34;&gt;Genericsがない&lt;/h2&gt;

&lt;p&gt;私が他の言語で知ってるのはJavaのGenericsとHaskellの型クラスです。Haskellは軽く勉強した程度ですが、型クラスはシンプルで汎用的で美しさを感じました。&lt;/p&gt;

&lt;p&gt;一方Javaは10年近く仕事で書いてましたが、 &lt;code&gt;? extends&lt;/code&gt; とか &lt;code&gt;? super&lt;/code&gt; のあたりはよくわからなくて避けてました。当時はそれでも困らなかったです。&lt;/p&gt;

&lt;p&gt;複雑なものが苦手な私としては、Javaのような複雑さになるぐらいならGenericsは無いほうが良いと思うので、Goの決断は私は賛成です。&lt;/p&gt;

&lt;p&gt;Genericsが無いとMap, Each, Selectのような関数を []interface{} に対して書いてみたくなると思います。 &lt;a href=&#34;http://qiita.com/hnakamur/items/76b06603013279b14aeb&#34;&gt;goでEach, Map, Selectのサンプル - Qiita&lt;/a&gt;で私も昔書いてみました。でも&lt;a href=&#34;http://qiita.com/hnakamur/items/76b06603013279b14aeb#comment-3d16d66e68bad9626f56&#34;&gt;コメント&lt;/a&gt;に書いたように、Goの開発者のRob Pikeさんもこういう関数は使わずに &lt;code&gt;for&lt;/code&gt; ループを使うべきと書かれています。&lt;/p&gt;

&lt;p&gt;Goに入ってはGoに従え (When in Go, do as the gophers do) ということで &lt;code&gt;for&lt;/code&gt; で書くのが良いと思います。&lt;/p&gt;

&lt;h2 id=&#34;goroutine-はgcされない&#34;&gt;goroutine はGCされない&lt;/h2&gt;

&lt;p&gt;同意です。&lt;/p&gt;

&lt;p&gt;Dave Cheneyさんのツイート
&lt;a href=&#34;https://twitter.com/davecheney/status/714053897841577985&#34;&gt;https://twitter.com/davecheney/status/714053897841577985&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;とスライド
&lt;a href=&#34;https://github.com/davecheney/high-performance-go-workshop/blob/ee2e7a82092a72d742b12b00308b0145f124d593/high-performance-go-workshop.slide#L648-L658&#34;&gt;https://github.com/davecheney/high-performance-go-workshop/blob/ee2e7a82092a72d742b12b00308b0145f124d593/high-performance-go-workshop.slide#L648-L658&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;にある&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Never start a goroutine without knowing how it will stop.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;というルールを守るのが良い習慣だと思います。&lt;/p&gt;

&lt;h2 id=&#34;goroutineはgenerator-yield-の実装には使えない&#34;&gt;goroutineはgenerator (yield) の実装には使えない&lt;/h2&gt;

&lt;p&gt;内容自体は同意です。&lt;/p&gt;

&lt;p&gt;ちょっと脱線になりますが、こういう他の言語の仕組みを同じようなものを作ろうとするのは、そもそもGoの文化になじまないです。Goは他の言語では常識とされている仕組みも一から吟味して取捨選択して最低限のものだけを残して、それ以外は敢えて含めていないと感じていて、ミニマリストな私には非常に魅力的です。&lt;/p&gt;

&lt;p&gt;less is moreの精神を感じます。言語の仕組みが最低限で、同じようなことは同じように書くことになるので、サードパーティのライブラリなど人のコードを読むときに非常に読みやすいというメリットがあります。&lt;/p&gt;

&lt;p&gt;また、自分でコードを書くときにも、似たようなことを実現するために複数の仕組みがあるとこのケースではどれを選ぶべきかと考える必要がありますが、決まったパターンがあれば悩む時間がありません。&lt;/p&gt;

&lt;p&gt;この結果Go言語だと言語でどう書くかよりもアプリケーションやライブラリの問題領域の方に注力しやすいと感じています。&lt;/p&gt;

&lt;p&gt;yieldみたいなことはせずに、goroutineを複数動かしてchannelでデータをやり取りするか、変数を sync.Mutex などで排他制御してデータをやり取りするのがGo流だと思います。あるいは簡単なイテレータなら関数を返すような関数で実現可能だと思います。&lt;/p&gt;

&lt;h2 id=&#34;例外が-推奨され-ない&#34;&gt;例外が(推奨され)ない&lt;/h2&gt;

&lt;p&gt;Java, Python, Rubyなどを書いていた私としても例外がないのは不便なのではと最初は思いましたが、今では err が戻り値で毎回&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if err != nil {
   return err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;と書くほうが、エラーの処理漏れが無いことが明確で安心感を感じます。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://blog.golang.org/errors-are-values&#34;&gt;Errors are values - The Go Blog&lt;/a&gt;のbufioのScannerのようにエラーがチェックする関数が別になっている例もあります。が、個人的には、記事中にある、もしもの例で &lt;code&gt;Scan()&lt;/code&gt; がエラーも返す例のほうがわかりやすいと思います。&lt;/p&gt;

&lt;p&gt;というのも、初めて &lt;code&gt;bufio.Scanner&lt;/code&gt; のドキュメントを見た時は &lt;code&gt;Err()&lt;/code&gt; の存在に気づいて無かったです。ただし、 &lt;a href=&#34;https://golang.org/pkg/bufio/#Scanner&#34;&gt;https://golang.org/pkg/bufio/#Scanner&lt;/a&gt; の Example (Lines) とかを見れば &lt;code&gt;Err()&lt;/code&gt; を使ったサンプルコードが書いてあるんですけどね。&lt;/p&gt;

&lt;p&gt;余談ですけど、APIドキュメントに Example でサンプルコードがついているときは必ず見たほうが良いです。関数のシグネチャ見ただけでは気づかない使い方が説明されていることが多いので。&lt;/p&gt;

&lt;p&gt;エラー処理は&lt;a href=&#34;https://blog.golang.org/error-handling-and-go&#34;&gt;Error handling and Go - The Go Blog&lt;/a&gt;のブログ記事も読みましょう。&lt;/p&gt;

&lt;p&gt;あと &lt;code&gt;panic&lt;/code&gt; と &lt;code&gt;recover&lt;/code&gt; で例外もどきを実現しようとするのも止めましょう。私は &lt;code&gt;recover&lt;/code&gt; は一度足りとも使ったことが無いです。&lt;/p&gt;

&lt;p&gt;panic はエラーがほぼ起きないケースでerrorをreturnして呼び出し側で処理したくないケースは使うこともあります。panicすると標準エラー出力にエラーメッセージとスタックトレースが出力されて異常終了します。&lt;/p&gt;

&lt;p&gt;Goのアプリケーションをsystemdから起動する場合は、panicするとjournalctlでログが見られてそちらで発生日時もわかるので、それでチェックしています。&lt;/p&gt;

&lt;h2 id=&#34;繰り返す-if-err-nil-return-err&#34;&gt;繰り返す if err != nil {return err}&lt;/h2&gt;

&lt;p&gt;ひとつ前の「例外が(推奨され)ない」にまとめて書きました。
個人的には同じパターンで繰り返すほうが、ケースバイケースで書き方が違うより、読みやすいです。&lt;/p&gt;

&lt;h2 id=&#34;return-nil-err-このerrorどこで発生したの&#34;&gt;return nil, err → このerrorどこで発生したの？&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;if err != nil {
  return nil, fmt.Errorf(&amp;quot;Some context: %v&amp;quot;, err)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;でコンテキストを追加するのがGo流らしいです。&lt;/p&gt;

&lt;p&gt;でも個人的にはスタックトレースのほうが楽だと感じます。あと個人的にはエラーが起きた地点での関連する変数もログ出力したいので、自作のログライブラリでは
&lt;a href=&#34;https://godoc.org/github.com/hnakamur/ltsvlog#LTSVLogger.ErrorWithStack&#34;&gt;func (l *LTSVLogger) ErrorWithStack(lv &amp;hellip;LV)&lt;/a&gt; というメソッドを用意して、エラーが起きた箇所でメッセージと変数の値とスタックトレースを出力するようにしています。&lt;/p&gt;

&lt;h2 id=&#34;関数より狭いスコープで-defer&#34;&gt;関数より狭いスコープで defer&lt;/h2&gt;

&lt;p&gt;わかりやすい名前がつけられるケースならprivateの関数に切り出してそちらでdeferするようにします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func myFuncHelper(filename string) (*dataType, error) {
  r, err := os.Open(filename)
  if err != nil {
    return err
  }
  defer r.Close()
  data, err := readDataFromReader(r)  // 実際にはもう少し複雑な処理
  if err != nil {
    return nil, err
  }
  return data, nil
}

func myFunc() error {
  data, err := myFunHelper(filename)
  if err != nil {
    return err
  }
  // その後の他の処理
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;あとエラーで抜けるケースが少なければdeferを使わずに &lt;code&gt;Close()&lt;/code&gt; を呼べば良いと思います。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func myFunc() error {
  // ...
  r, err := os.Open(filename)
  if err != nil {
    return err
  }
  data, err := readDataFromReader(r)  // 実際にはもう少し複雑な処理
  if err != nil {
    r.Close()
    return err
  }
  r.Close()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;「実際にはもう少し複雑な処理」と書いているので、&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  if err != nil {
    r.Close()
    return err
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;が何回も出てくるのでしょうが、多すぎと感じたら別の方法を考える感じで。&lt;/p&gt;

&lt;h2 id=&#34;structとc-javaのクラスとの違い&#34;&gt;structとC++/Javaのクラスとの違い&lt;/h2&gt;

&lt;h3 id=&#34;コンストラクタがない&#34;&gt;コンストラクタがない&lt;/h3&gt;

&lt;p&gt;コンストラクタは無いので &lt;code&gt;NewSomething&lt;/code&gt; とか &lt;code&gt;somepackage.New&lt;/code&gt; のような関数を定義する習慣というのはその通りです。&lt;/p&gt;

&lt;h3 id=&#34;ゼロ初期化が避けられない&#34;&gt;ゼロ初期化が避けられない&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;structが外部に公開されるのならばstructは全てがゼロ初期化された場合にも正しく動くように常に設計しなくてはならないのです。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;これは現実には無理だと思います。例えばファイル名のフィールドのstringが空文字だった時にはどのファイルを処理すれば良いかはわかりっこないです。zero valueでも構わないフィールドについては、zero valueだとどう解釈されるかをAPIドキュメントに書いておけば良い話です。それ以外は呼び出し側が設定する責任があるということで。&lt;/p&gt;

&lt;h3 id=&#34;コピーされるのが避けられない&#34;&gt;コピーされるのが避けられない&lt;/h3&gt;

&lt;p&gt;Go言語自体にコピー防止の仕組みを入れる議論はあったようです。&lt;a href=&#34;https://github.com/golang/go/issues/8005&#34;&gt;runtime: add NoCopy documentation struct type? · Issue #8005 · golang/go&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;このスレッドの&lt;a href=&#34;https://github.com/golang/go/issues/8005#issuecomment-190753527&#34;&gt;コメント&lt;/a&gt;で実現する方法が紹介されています。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/valyala/fasthttp&#34;&gt;valyala/fasthttp&lt;/a&gt;ではこの技を使っていて
&lt;a href=&#34;https://github.com/valyala/fasthttp/blob/master/nocopy.go&#34;&gt;fasthttp/nocopy.go&lt;/a&gt;に &lt;code&gt;noCopy&lt;/code&gt; の定義があり、 &lt;a href=&#34;https://github.com/valyala/fasthttp/blob/45697fe30a130ec6a54426a069c82f3abe76b63d/http.go#L16-L45&#34;&gt;https://github.com/valyala/fasthttp/blob/45697fe30a130ec6a54426a069c82f3abe76b63d/http.go#L16-L45&lt;/a&gt; に使用例があります。&lt;/p&gt;

&lt;h2 id=&#34;型が後置&#34;&gt;型が後置&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://blog.golang.org/gos-declaration-syntax&#34;&gt;Go&amp;rsquo;s Declaration Syntax - The Go Blog&lt;/a&gt; で理由が説明されています。&lt;/p&gt;

&lt;h2 id=&#34;1-0-が浮動小数点型にならない-時がある&#34;&gt;1.0 が浮動小数点型にならない(時がある)&lt;/h2&gt;

&lt;p&gt;これは知りませんでした。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;e := float64(a / 3.0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;と書けば回避できました。 &lt;a href=&#34;https://play.golang.org/p/Y7_LUdQeeq&#34;&gt;https://play.golang.org/p/Y7_LUdQeeq&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;名前が&#34;&gt;名前が…&lt;/h2&gt;

&lt;p&gt;golang で検索すればOKです。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>